{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general information on the `Model()` syntax can be found [here](https://keras.io/getting-started/functional-api-guide/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implementing networks which are discussed in the literature is a vital skill. Here you re-build the architecture from [arXiv:1505.04597](https://arxiv.org/abs/1505.04597). A figure of the network is shown in Figure 1 of this paper.  \n",
    "You can check your results again via `model.compile()` and `model.summary()`.\n",
    "<img src=\"net.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare imput size\n",
    "inputs = Input((572,572,1))\n",
    "\n",
    "# define weight initializations according to what is suggested in the paper below eq. (2)\n",
    "init = tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None)\n",
    "\n",
    "# first box of convolutional 3x3 layers with cropping to copy the layer later on and max pooling\n",
    "# later layers basically the same\n",
    "Contract1conv1 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init) (inputs)\n",
    "Contract1conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init) (Contract1conv1)\n",
    "Contract1crop = Cropping2D(cropping=((88, 88), (88, 88)))(Contract1conv2)\n",
    "Contract1pool = MaxPooling2D((2, 2)) (Contract1conv2)\n",
    "\n",
    "Contract2conv1 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init) (Contract1pool)\n",
    "Contract2conv2 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init) (Contract2conv1)\n",
    "Contract2crop = Cropping2D(cropping=((40, 40), (40, 40)))(Contract2conv2)\n",
    "Contract2pool = MaxPooling2D((2, 2)) (Contract2conv2)\n",
    "\n",
    "Contract3conv1 = Conv2D(256, (3, 3), activation='relu', kernel_initializer=init) (Contract2pool)\n",
    "Contract3conv2 = Conv2D(256, (3, 3), activation='relu', kernel_initializer=init) (Contract3conv1)\n",
    "Contract3crop = Cropping2D(cropping=((16, 16), (16, 16)))(Contract3conv2)\n",
    "Contract3pool = MaxPooling2D((2, 2)) (Contract3conv2)\n",
    "\n",
    "Contract4conv1 = Conv2D(512, (3, 3), activation='relu', kernel_initializer=init) (Contract3pool)\n",
    "Contract4conv2 = Conv2D(512, (3, 3), activation='relu', kernel_initializer=init) (Contract4conv1)\n",
    "Contract4crop = Cropping2D(cropping=((4, 4), (4, 4)))(Contract4conv2)\n",
    "Contract4pool = MaxPooling2D(pool_size=(2, 2)) (Contract4conv2)\n",
    "\n",
    "Contract5conv1 = Conv2D(1024, (3, 3), activation='relu', kernel_initializer=init) (Contract4pool)\n",
    "Contract5conv2 = Conv2D(1024, (3, 3), activation='relu', kernel_initializer=init) (Contract5conv1)\n",
    "\n",
    "# transpose convolution and concatenate with contracting path\n",
    "# followed by convolutions\n",
    "Expand4convtrans = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same', kernel_initializer=init) (Contract5conv2)\n",
    "Expand4concat = concatenate([Expand4convtrans, Contract4crop])\n",
    "Expand4conv1 = Conv2D(512, (3, 3), activation='relu', kernel_initializer=init) (Expand4concat)\n",
    "Expand4conv2 = Conv2D(512, (3, 3), activation='relu', kernel_initializer=init) (Expand4conv1)\n",
    "\n",
    "Expand3convtrans = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same', kernel_initializer=init) (Expand4conv2)\n",
    "Expand3concat = concatenate([Expand3convtrans, Contract3crop])\n",
    "Expand3conv1 = Conv2D(256, (3, 3), activation='relu', kernel_initializer=init) (Expand3concat)\n",
    "Expand3conv2 = Conv2D(256, (3, 3), activation='relu', kernel_initializer=init) (Expand3conv1)\n",
    "\n",
    "Expand2convtrans = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', kernel_initializer=init) (Expand3conv2)\n",
    "Expand2concat = concatenate([Expand2convtrans, Contract2crop])\n",
    "Expand2conv1 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init) (Expand2concat)\n",
    "Expand2conv2 = Conv2D(128, (3, 3), activation='relu', kernel_initializer=init) (Expand2conv1)\n",
    "\n",
    "Expand1convtrans = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', kernel_initializer=init) (Expand2conv2)\n",
    "Expand1concat = concatenate([Expand1convtrans, Contract1crop], axis=3)\n",
    "Expand1conv1 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init) (Expand1concat)\n",
    "Expand1conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer=init) (Expand1conv1)\n",
    "\n",
    "# sigmoid = softmax for two classes\n",
    "outputs = Conv2D(2, (1, 1), activation='sigmoid', kernel_initializer=init) (Expand1conv2)\n",
    "\n",
    "unet = Model(inputs=inputs, outputs=outputs)\n",
    "unet.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a neat visualization, we use tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get session graph\n",
    "graph = K.get_session().graph\n",
    "\n",
    "# write to file\n",
    "tb_path = \"logs_unet/\"\n",
    "writer = tf.summary.FileWriter(logdir=tb_path, graph=graph)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tensorboard in shell -> interrupt kernel to stop\n",
    "# you can click the link to see the graph of the network we built\n",
    "!tensorboard --logdir=logs_unet --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above link does not work for some reason, you should find tensorboard at [localhost:6006](http://localhost:6006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the aim of this exercise is to build a network. In this exercise you should implement the network which is discussed in: [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (Krizhevsky, Sutskever, Hinton). The network architecture is summarised in Figure 2 of that paper and more detailed descriptions are found in the text.  \n",
    "You only need to implement the architecture and check that your network is consistent. Note, that you can check your results by  \n",
    "a) checking your model is compiling in Keras and  \n",
    "b) by comparing your `model.summary()` with the desired dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallel structure of AlexNet is due the fact that it was designed to run on two GPUs. If you don't have access to a GPU, you can use [Google Colab](https://colab.research.google.com). There you can choose to run on a backend with a GPU or TPU. In the Menu, click \"Runtime\" and then \"Change runtime type\".  \n",
    "Unfortunately we only have one GPU available there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simplified version with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = Input(shape=(224,224,3))\n",
    "\n",
    "# 1st convolution block\n",
    "conv1_up = Conv2D(48, kernel_size=11, strides=4, padding='same')(inputs)\n",
    "batchnorm1_up = BatchNormalization(axis=-1)(conv1_up)\n",
    "act1_up = Activation('relu')(batchnorm1_up)\n",
    "\n",
    "conv1_down = Conv2D(48, kernel_size=11, strides=4, padding='same')(inputs)\n",
    "batchnorm1_down = BatchNormalization(axis=-1)(conv1_down)\n",
    "act1_down = Activation('relu')(batchnorm1_down)\n",
    "\n",
    "maxpool1_up = MaxPooling2D(pool_size=3, strides=2)(act1_up)\n",
    "maxpool1_down = MaxPooling2D(pool_size=3, strides=2)(act1_down)\n",
    "\n",
    "# 2nd convolution block\n",
    "conv2_up = Conv2D(128, kernel_size=5, strides=1, padding='same')(maxpool1_up)\n",
    "batchnorm2_up = BatchNormalization(axis=-1)(conv2_up)\n",
    "act2_up = Activation('relu')(batchnorm2_up)\n",
    "\n",
    "conv2_down = Conv2D(128, kernel_size=5, strides=1, padding='same')(maxpool1_down)\n",
    "batchnorm2_down = BatchNormalization(axis=-1)(conv2_down)\n",
    "act2_down = Activation('relu')(batchnorm2_down)\n",
    "\n",
    "maxpool2_up = MaxPooling2D(pool_size=3, strides=2)(act2_up)\n",
    "maxpool2_down = MaxPooling2D(pool_size=3, strides=2)(act2_down)\n",
    "\n",
    "# 3rd convolution block\n",
    "merge3 = concatenate([maxpool2_up, maxpool2_down])\n",
    "conv3_up = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(merge3)\n",
    "conv3_down = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(merge3)\n",
    "\n",
    "# 4th convolution block\n",
    "conv4_up = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(conv3_up)\n",
    "conv4_down = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(conv3_down)\n",
    "\n",
    "# 5th convolution block\n",
    "conv5_up = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv4_up)\n",
    "conv5_down = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv4_down)\n",
    "maxpool5_up = MaxPooling2D(pool_size=3, strides=2)(conv5_up)\n",
    "maxpool5_down = MaxPooling2D(pool_size=3, strides=2)(conv5_down)\n",
    "\n",
    "# Dense Layers 1st block (use dropout)\n",
    "merge_dense1 = concatenate([maxpool5_up, maxpool5_down])\n",
    "flatten_dense1 = Flatten()(merge_dense1)\n",
    "dense1_up = Dense(2048, activation='relu')(flatten_dense1)\n",
    "dense1_dropout_up = Dropout(rate=0.5)(dense1_up)\n",
    "dense1_down = Dense(2048, activation='relu')(flatten_dense1)\n",
    "dense1_dropout_down = Dropout(rate=0.5)(dense1_down)\n",
    "\n",
    "# Dense Layers 2nd block (use dropout)\n",
    "merge_dense2 = concatenate([dense1_dropout_up, dense1_dropout_down])\n",
    "flatten_dense2 = Flatten()(merge_dense2)\n",
    "dense2_up = Dense(2048, activation='relu')(flatten_dense2)\n",
    "dense2_dropout_up = Dropout(rate=0.5)(dense2_up)\n",
    "dense2_down = Dense(2048, activation='relu')(flatten_dense2)\n",
    "dense2_dropout_down = Dropout(rate=0.5)(dense2_down)\n",
    "\n",
    "# Softmax\n",
    "merge_dense3 = concatenate([dense2_dropout_up, dense2_dropout_down])\n",
    "flatten_dense3 = Flatten()(merge_dense3)\n",
    "output = Dense(1000, activation='softmax')(flatten_dense3)\n",
    "\n",
    "# Model\n",
    "alex_net = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "alex_net.compile(loss=\"binary_crossentropy\", optimizer='adam')\n",
    "print(alex_net.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output dimension after the first convolution does not match with the one from the paper. I have found several references that state that the $224$ is a typo in the paper and should be $227$. I am not sure if this is true, as the authors quote the number at so many places in the paper. A quick fix that gives the right output dimensions is to change the padding to valid in the first layer and add a symmetric padding of 2. We cannot know for sure what the authors did without their pre-processed data or code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = Input(shape=(224,224,3))\n",
    "\n",
    "# 1st convolution block\n",
    "pad1_up = ZeroPadding2D(2)(inputs) # fix for output dimensions\n",
    "conv1_up = Conv2D(48, kernel_size=11, strides=4, padding='valid')(pad1_up)\n",
    "batchnorm1_up = BatchNormalization(axis=-1)(conv1_up)\n",
    "act1_up = Activation('relu')(batchnorm1_up)\n",
    "\n",
    "pad1_down = ZeroPadding2D(2)(inputs) # fix for output dimensions\n",
    "conv1_down = Conv2D(48, kernel_size=11, strides=4, padding='valid')(pad1_down)\n",
    "batchnorm1_down = BatchNormalization(axis=-1)(conv1_down)\n",
    "act1_down = Activation('relu')(batchnorm1_down)\n",
    "\n",
    "maxpool1_up = MaxPooling2D(pool_size=3, strides=2)(act1_up)\n",
    "maxpool1_down = MaxPooling2D(pool_size=3, strides=2)(act1_down)\n",
    "\n",
    "# 2nd convolution block\n",
    "conv2_up = Conv2D(128, kernel_size=5, strides=1, padding='same')(maxpool1_up)\n",
    "batchnorm2_up = BatchNormalization(axis=-1)(conv2_up)\n",
    "act2_up = Activation('relu')(batchnorm2_up)\n",
    "\n",
    "conv2_down = Conv2D(128, kernel_size=5, strides=1, padding='same')(maxpool1_down)\n",
    "batchnorm2_down = BatchNormalization(axis=-1)(conv2_down)\n",
    "act2_down = Activation('relu')(batchnorm2_down)\n",
    "\n",
    "maxpool2_up = MaxPooling2D(pool_size=3, strides=2)(act2_up)\n",
    "maxpool2_down = MaxPooling2D(pool_size=3, strides=2)(act2_down)\n",
    "\n",
    "# 3rd convolution block\n",
    "merge3 = concatenate([maxpool2_up, maxpool2_down])\n",
    "conv3_up = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(merge3)\n",
    "conv3_down = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(merge3)\n",
    "\n",
    "# 4th convolution block\n",
    "conv4_up = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(conv3_up)\n",
    "conv4_down = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(conv3_down)\n",
    "\n",
    "# 5th convolution block\n",
    "conv5_up = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv4_up)\n",
    "conv5_down = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv4_down)\n",
    "maxpool5_up = MaxPooling2D(pool_size=3, strides=2)(conv5_up)\n",
    "maxpool5_down = MaxPooling2D(pool_size=3, strides=2)(conv5_down)\n",
    "\n",
    "# Dense Layers 1st block (use dropout)\n",
    "merge_dense1 = concatenate([maxpool5_up, maxpool5_down])\n",
    "flatten_dense1 = Flatten()(merge_dense1)\n",
    "dense1_up = Dense(2048, activation='relu')(flatten_dense1)\n",
    "dense1_dropout_up = Dropout(rate=0.5)(dense1_up)\n",
    "dense1_down = Dense(2048, activation='relu')(flatten_dense1)\n",
    "dense1_dropout_down = Dropout(rate=0.5)(dense1_down)\n",
    "\n",
    "# Dense Layers 2nd block (use dropout)\n",
    "merge_dense2 = concatenate([dense1_dropout_up, dense1_dropout_down])\n",
    "flatten_dense2 = Flatten()(merge_dense2)\n",
    "dense2_up = Dense(2048, activation='relu')(flatten_dense2)\n",
    "dense2_dropout_up = Dropout(rate=0.5)(dense2_up)\n",
    "dense2_down = Dense(2048, activation='relu')(flatten_dense2)\n",
    "dense2_dropout_down = Dropout(rate=0.5)(dense2_down)\n",
    "\n",
    "# Softmax\n",
    "merge_dense3 = concatenate([dense2_dropout_up, dense2_dropout_down])\n",
    "flatten_dense3 = Flatten()(merge_dense3)\n",
    "output = Dense(1000, activation='softmax')(flatten_dense3)\n",
    "\n",
    "# Model\n",
    "alex_net = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "alex_net.compile(loss=\"binary_crossentropy\", optimizer='adam')\n",
    "print(alex_net.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version with Local Response Normalization (LRN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to implement the LRN (see e.g. [here](https://resources.oreilly.com/examples/9781787128422/blob/0e1be827d0179cc535da74957866ed87a4ea0224/DeepLearningwithKeras_Code/Chapter07/tf-keras-func.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Layer, InputSpec\n",
    "\n",
    "class LocalResponseNormalization(Layer):\n",
    "\n",
    "    def __init__(self, n=5, alpha=0.0001, beta=0.75, k=2, **kwargs):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        super(LocalResponseNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(LocalResponseNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if K.image_data_format() == \"th\":\n",
    "            _, f, r, c = self.shape\n",
    "        else:\n",
    "            _, r, c, f = self.shape\n",
    "        squared = K.square(x)\n",
    "        pooled = K.pool2d(squared, (self.n, self.n), strides=(1, 1),\n",
    "        padding=\"same\", pool_mode=\"avg\")\n",
    "        if K.image_data_format() == \"th\":\n",
    "            summed = K.sum(pooled, axis=1, keepdims=True)\n",
    "            averaged = self.alpha * K.repeat_elements(summed, f, axis=1)\n",
    "        else:\n",
    "            summed = K.sum(pooled, axis=3, keepdims=True)\n",
    "            averaged = self.alpha * K.repeat_elements(summed, f, axis=3)\n",
    "        denom = K.pow(self.k + averaged, self.beta)\n",
    "        return x / denom\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = Input(shape=(224,224,3))\n",
    "\n",
    "# 1st convolution block\n",
    "pad1_up = ZeroPadding2D(2)(inputs)\n",
    "conv1_up = Conv2D(48, kernel_size=11, strides=4, padding='valid', activation='relu')(pad1_up)\n",
    "LRN1_up = LocalResponseNormalization()(conv1_up)\n",
    "\n",
    "pad1_down = ZeroPadding2D(2)(inputs)\n",
    "conv1_down = Conv2D(48, kernel_size=11, strides=4, padding='valid', activation='relu')(pad1_down)\n",
    "LRN1_down = LocalResponseNormalization()(conv1_down)\n",
    "\n",
    "maxpool1_up = MaxPooling2D(pool_size=3, strides=2)(LRN1_up)\n",
    "maxpool1_down = MaxPooling2D(pool_size=3, strides=2)(LRN1_down)\n",
    "\n",
    "# 2nd convolution block\n",
    "conv2_up = Conv2D(128, kernel_size=5, strides=1, padding='same', activation='relu')(maxpool1_up)\n",
    "LRN2_up = LocalResponseNormalization()(conv2_up)\n",
    "\n",
    "conv2_down = Conv2D(128, kernel_size=5, strides=1, padding='same', activation='relu')(maxpool1_down)\n",
    "LRN2_down = LocalResponseNormalization()(conv2_down)\n",
    "\n",
    "maxpool2_up = MaxPooling2D(pool_size=3, strides=2)(LRN2_up)\n",
    "maxpool2_down = MaxPooling2D(pool_size=3, strides=2)(LRN2_down)\n",
    "\n",
    "# 3rd convolution block\n",
    "merge3 = concatenate([maxpool2_up, maxpool2_down])\n",
    "conv3_up = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(merge3)\n",
    "conv3_down = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(merge3)\n",
    "\n",
    "# 4th convolution block\n",
    "conv4_up = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(conv3_up)\n",
    "conv4_down = Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(conv3_down)\n",
    "\n",
    "# 5th convolution block\n",
    "conv5_up = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv4_up)\n",
    "conv5_down = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv4_down)\n",
    "maxpool5_up = MaxPooling2D(pool_size=3, strides=2)(conv5_up)\n",
    "maxpool5_down = MaxPooling2D(pool_size=3, strides=2)(conv5_down)\n",
    "\n",
    "# Dense Layers 1st block (use dropout)\n",
    "merge_dense1 = concatenate([maxpool5_up, maxpool5_down])\n",
    "flatten_dense1 = Flatten()(merge_dense1)\n",
    "dense1_up = Dense(2048, activation='relu')(flatten_dense1)\n",
    "dense1_dropout_up = Dropout(rate=0.5)(dense1_up)\n",
    "dense1_down = Dense(2048, activation='relu')(flatten_dense1)\n",
    "dense1_dropout_down = Dropout(rate=0.5)(dense1_down)\n",
    "\n",
    "# Dense Layers 2nd block (use dropout)\n",
    "merge_dense2 = concatenate([dense1_dropout_up, dense1_dropout_down])\n",
    "flatten_dense2 = Flatten()(merge_dense2)\n",
    "dense2_up = Dense(2048, activation='relu')(flatten_dense2)\n",
    "dense2_dropout_up = Dropout(rate=0.5)(dense2_up)\n",
    "dense2_down = Dense(2048, activation='relu')(flatten_dense2)\n",
    "dense2_dropout_down = Dropout(rate=0.5)(dense2_down)\n",
    "\n",
    "# Softmax\n",
    "merge_dense3 = concatenate([dense2_dropout_up, dense2_dropout_down])\n",
    "flatten_dense3 = Flatten()(merge_dense3)\n",
    "output = Dense(1000, activation='softmax')(flatten_dense3)\n",
    "\n",
    "# Model\n",
    "alex_net = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "alex_net.compile(loss=\"binary_crossentropy\", optimizer='adam')\n",
    "print(alex_net.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graph\n",
    "graph = K.get_session().graph\n",
    "\n",
    "# write to files\n",
    "tb_path = \"logs_alexnet/\"\n",
    "writer = tf.summary.FileWriter(logdir=tb_path, graph=graph)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tensorboard in shell -> interrupt kernel to stop\n",
    "# you can click the link to see the graph of the network we built\n",
    "!tensorboard --logdir=logs_alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above link does not work for some reason, you should find tensorboard at [localhost:6006](http://localhost:6006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the lecture we have discussed how to obtain the first principal component $d$ as the eigenvector corresponding to the largest eigenvalue of $X^T X$. Show, using induction, that in general the matrix $D$ is given by the $l$ eigenvectors corresponding to the largest eigenvalues.  \n",
    "Generate a numerical example to compare your data transformation with the transformation given by the implementation of PCA in sklearn. For instance, you can use an example based on a 2D-Gaussian as presented in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proof can be found for example [here](https://people.eecs.berkeley.edu/~satishr/cs270/sp11/rough-notes/PCA.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some 2D data:\n",
    "mean = [0,0]\n",
    "cov = [[2,3],[3,5]]\n",
    "x, y = np.random.multivariate_normal(mean, cov, 500).T  \n",
    "\n",
    "X=np.array([x, y]).T\n",
    "print(X.shape)\n",
    "\n",
    "plt.scatter(x, y, s=2, color='gray')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigendecomposition\n",
    "Sigma = np.dot(X.T,X)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(Sigma)\n",
    "print(eigenvalues)\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "sqrteigenvalues = np.sqrt(eigenvalues)\n",
    "eigenvectors = eigenvectors.T[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the eigenvectors\n",
    "t = np.linspace(-.1,.1,100)\n",
    "EV1X = t*eigenvectors[0,0]*sqrteigenvalues[0]\n",
    "EV1Y = t*eigenvectors[0,1]*sqrteigenvalues[0]\n",
    "EV2X = t*eigenvectors[1,0]*sqrteigenvalues[1]\n",
    "EV2Y = t*eigenvectors[1,1]*sqrteigenvalues[1]\n",
    "\n",
    "plt.scatter(x, y, s=1, color='gray')\n",
    "plt.axis('equal')\n",
    "plt.plot(EV1X,EV1Y,color='r')\n",
    "plt.plot(EV2X,EV2Y,color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print('Sklearn eigenvectors:')\n",
    "print(pca.components_)\n",
    "print('Our eigenvectors:')\n",
    "print(eigenvectors)\n",
    "print('Sklearn singular values:')\n",
    "print(pca.singular_values_)\n",
    "print('Our singular values:')\n",
    "print(sqrteigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the result is only approximately the same. Note that the relative sign in the eigenvectors does not matter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
