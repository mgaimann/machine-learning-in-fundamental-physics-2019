{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to familiarise yourself with the analysis of clustering hyperparameters. For this purpose, let us consider the AgglomerativeClustering algorithm in sklearn. Using `make circles`, `make moons`, `make blobs` visualise the different performance of different linkages (ward, complete, average, single). Perform a little bit of hyperparameter tuning and show for the different linkage models the best model you have found.  \n",
    "In the second part of this exercise, compare these results with the performance of t-SNE and k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[t-SNE paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)  \n",
    "[evaluate clustering performance](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data using blobs\n",
    "X, y = make_blobs(n_samples=100, centers=3, n_features=2, random_state=4)\n",
    "plotcolors = {0: 'orange', 1: 'blue', 2: 'green'}\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,10))\n",
    "for blob in range(3):\n",
    "    axs[0,0].scatter(X[y == blob][:,0],X[y == blob][:,1], color=plotcolors[blob])\n",
    "    axs[0,0].set_title('True classes')\n",
    "    \n",
    "# assign different linkages to different plots\n",
    "pltno={'ward': np.s_[0,1], 'complete': np.s_[0,2], 'average': np.s_[1,0], 'single': np.s_[1,1]}    \n",
    "\n",
    "# loop over different linkages\n",
    "for linkage in ('ward', 'complete', 'average', 'single'):\n",
    "    model = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None,\n",
    "                                    linkage=linkage, n_clusters=3, pooling_func='deprecated')\n",
    "    model.fit(X)\n",
    "    y_predicted = model.fit_predict(X)\n",
    "\n",
    "    # take the label permutation with maximal overlap\n",
    "    perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "    accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "    \n",
    "    for blob in range(3):\n",
    "        axs[pltno[linkage]].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "        axs[pltno[linkage]].set_title(str(linkage)+': accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that `linking='average'` gives by far the best result. Let's see if we can do better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(20,10))\n",
    "pltno={'euclidean': np.s_[0,0], 'l1': np.s_[0,1], 'l2': np.s_[0,2], 'manhattan': np.s_[1,0], 'cosine': np.s_[1,1]}\n",
    "\n",
    "# now we loop over different distance functions\n",
    "for affinity in ('euclidean', 'l1', 'l2', 'manhattan', 'cosine'):\n",
    "    # euclidean & l2, manhattan &l1 should be the same\n",
    "    model = AgglomerativeClustering(affinity=affinity, compute_full_tree='auto', connectivity=None,\n",
    "                                    linkage='average', n_clusters=3, pooling_func='deprecated')\n",
    "    model.fit(X)\n",
    "    y_predicted = model.fit_predict(X)\n",
    "\n",
    "\n",
    "    perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "    accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "    for blob in range(3):\n",
    "        axs[pltno[affinity]].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "        axs[pltno[affinity]].set_title('linkage: average, affinity:'+str(affinity)+', accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE & KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first transform data using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30,early_exaggeration=20)\n",
    "Xtrf = tsne.fit_transform(X)\n",
    "plotcolors = {0: 'orange', 1: 'blue', 2: 'green'}\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "for blob in range(3):\n",
    "    axs[0].scatter(Xtrf[y == blob][:,0],Xtrf[y == blob][:,1], color=plotcolors[blob])\n",
    "    axs[0].set_title('t-SNE')\n",
    "\n",
    "# cluster in latent space using KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(Xtrf)\n",
    "y_predicted = model.fit_predict(Xtrf)\n",
    "perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "\n",
    "for blob in range(3):\n",
    "    axs[1].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "    axs[1].set_title('t-SNE+KMEANS:, accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data using moons\n",
    "X, y = make_moons(n_samples=100, noise=0.1, random_state=4)\n",
    "plotcolors = {0: 'orange', 1: 'blue', 2: 'green'}\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,10))\n",
    "for blob in range(3):\n",
    "    axs[0,0].scatter(X[y == blob][:,0],X[y == blob][:,1], color=plotcolors[blob])\n",
    "    axs[0,0].set_title('True classes')\n",
    "    \n",
    "pltno={'ward': np.s_[0,1], 'complete': np.s_[0,2], 'average': np.s_[1,0], 'single': np.s_[1,1]}    \n",
    "\n",
    "for linkage in ('ward', 'complete', 'average', 'single'):\n",
    "    model = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None,\n",
    "                                    linkage=linkage, n_clusters=3, pooling_func='deprecated')\n",
    "    model.fit(X)\n",
    "    y_predicted = model.fit_predict(X)\n",
    "\n",
    "\n",
    "    perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "    accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "    \n",
    "    for blob in range(3):\n",
    "        axs[pltno[linkage]].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "        axs[pltno[linkage]].set_title(str(linkage)+': accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `linking='ward'` and `linking='complete'` give best result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE & KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10,early_exaggeration=10)\n",
    "Xtrf = tsne.fit_transform(X)\n",
    "plotcolors = {0: 'orange', 1: 'blue', 2: 'green'}\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "for blob in range(3):\n",
    "    axs[0].scatter(Xtrf[y == blob][:,0],Xtrf[y == blob][:,1], color=plotcolors[blob])\n",
    "    axs[0].set_title('t-SNE')\n",
    "    \n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(Xtrf)\n",
    "y_predicted = model.fit_predict(Xtrf)\n",
    "perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "\n",
    "for blob in range(3):\n",
    "    axs[1].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "    axs[1].set_title('t-SNE+KMEANS:, accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data using circles\n",
    "X, y = make_circles(n_samples=100, noise=0.05, factor=0.5, random_state=4)\n",
    "plotcolors = {0: 'orange', 1: 'blue', 2: 'green'}\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,10))\n",
    "for blob in range(3):\n",
    "    axs[0,0].scatter(X[y == blob][:,0],X[y == blob][:,1], color=plotcolors[blob])\n",
    "    axs[0,0].set_title('True classes')\n",
    "    \n",
    "pltno={'ward': np.s_[0,1], 'complete': np.s_[0,2], 'average': np.s_[1,0], 'single': np.s_[1,1]}    \n",
    "\n",
    "for linkage in ('ward', 'complete', 'average', 'single'):\n",
    "    model = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None,\n",
    "                                    linkage=linkage, n_clusters=3, pooling_func='deprecated')\n",
    "    model.fit(X)\n",
    "    y_predicted = model.fit_predict(X)\n",
    "\n",
    "\n",
    "    perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "    accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "    \n",
    "    for blob in range(3):\n",
    "        axs[pltno[linkage]].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "        axs[pltno[linkage]].set_title(str(linkage)+': accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case `linkage='single'` gives the best result. Can we improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(20,10))\n",
    "pltno={'euclidean': np.s_[0,0], 'l1': np.s_[0,1], 'l2': np.s_[0,2], 'manhattan': np.s_[1,0], 'cosine': np.s_[1,1]}\n",
    "\n",
    "for affinity in ('euclidean', 'l1', 'l2', 'manhattan', 'cosine'):\n",
    "    model = AgglomerativeClustering(affinity=affinity, compute_full_tree='auto', connectivity=None,\n",
    "                                    linkage='single', n_clusters=3, pooling_func='deprecated')\n",
    "    model.fit(X)\n",
    "    y_predicted = model.fit_predict(X)\n",
    "\n",
    "\n",
    "    perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "    accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "    for blob in range(3):\n",
    "        axs[pltno[affinity]].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "        axs[pltno[affinity]].set_title('linkage: average, affinity:'+str(affinity)+', accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy does not improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE & KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=6, early_exaggeration=3)\n",
    "Xtrf = tsne.fit_transform(X)\n",
    "plotcolors = {0: 'orange', 1: 'blue', 2: 'green'}\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "for blob in range(3):\n",
    "    axs[0].scatter(Xtrf[y == blob][:,0],Xtrf[y == blob][:,1], color=plotcolors[blob])\n",
    "    axs[0].set_title('t-SNE')\n",
    "    \n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(Xtrf)\n",
    "y_predicted = model.fit_predict(Xtrf)\n",
    "perm = np.array(list((itertools.permutations([0, 1, 2]))))\n",
    "accuracy = max([X[y_predicted == np.array([perm[i,x] for x in y])].shape[0]/X.shape[0] for i in range(6)])\n",
    "\n",
    "for blob in range(3):\n",
    "    axs[1].scatter(X[y_predicted == blob][:,0],X[y_predicted == blob][:,1], color=plotcolors[blob])\n",
    "    axs[1].set_title('t-SNE+KMEANS:, accuracy of '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this result requires a finely tuned value of the perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to implement simple autoencoders.  \n",
    "* Generate 2-d images (1-channel) showing polynomials up to a maximum degree (e.g. 40x40) in two variables.\n",
    "* Build two autoencoder architectures (e.g. similar to the ones presented in the lecture) which involve a single hidden dense layer and several hidden dense layers.\n",
    "* For quadratic polynomials and two latent dimensions, visualise the results of your latent dimensions. Is an interpretation of your latent parameters easily visible?\n",
    "* $\\star$ Add a custom loss function which de-correlates the latent parameters. Is it possible to find an interpretation for the latent parameters in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 40\n",
    "\n",
    "# evaluate polynomial over grid of size 40x40\n",
    "def polynomial(degree):\n",
    "    coeff = np.random.normal(0,1,(degree+1, degree+1))\n",
    "    #coeff = np.random.uniform(-1,1,(degree+1, degree+1))\n",
    "    return [[sum([coeff[i,j]*((x/size)**i)*((y/size)**j)\n",
    "            for i in range(degree+1) for j in range(degree+1) if (i+j)<=degree]) \n",
    "            for x in range(size)] for y in range(size)]\n",
    "\n",
    "# training set of polynomial images of degree <=5\n",
    "maxdegree = 5\n",
    "size = 40\n",
    "num_polys = 3000\n",
    "polydata = np.array([polynomial(np.random.randint(0,maxdegree)) for i in range(num_polys)])\n",
    "polydata = tf.keras.utils.normalize(polydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(polydata[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = size**2\n",
    "hidden_size = 2\n",
    "output_size = size**2\n",
    "\n",
    "polydata = polydata.reshape(num_polys, size**2)\n",
    "\n",
    "inputs = Input(shape=(input_size,))\n",
    "\n",
    "# autoencoder with a single hidden layer\n",
    "\n",
    "# hidden layer\n",
    "AE1encoded = Dense(2, activation='linear')(inputs)\n",
    "# decoding\n",
    "AE1decoded = Dense(size**2, activation='linear')(AE1encoded)\n",
    "# we use linear activations for bottleneck and output because our data can be negative\n",
    "\n",
    "AE1 = Model(inputs, AE1decoded)\n",
    "AE1.compile(optimizer='adam', loss='mse')\n",
    "AE1_initial_weights = AE1.get_weights()\n",
    "\n",
    "# autoencoder with multiple hidden layers\n",
    "\n",
    "AE2compress1 = Dense(512, activation='relu')(inputs)\n",
    "AE2compress2 = Dense(64, activation='relu')(AE2compress1)\n",
    "\n",
    "AE2encoded = Dense(2, activation='linear')(AE2compress2)\n",
    "\n",
    "\n",
    "AE2decompress1 = Dense(64, activation='relu')(AE2encoded)\n",
    "AE2decompress2 = Dense(512, activation='relu')(AE2decompress1)\n",
    "AE2decoded = Dense(size**2, activation='linear')(AE2decompress2)\n",
    "\n",
    "AE2 = Model(inputs, AE2decoded)\n",
    "AE2.compile(optimizer='adam', loss='mse')\n",
    "AE2_initial_weights = AE2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train one layer encoder\n",
    "hist = AE1.fit(polydata, polydata, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and train the larger autoencoder\n",
    "hist_large = AE2.fit(polydata, polydata, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one layer autoencoder\n",
    "\n",
    "# model which computes the output of the hidden layer\n",
    "AE1_latent = Model(inputs, AE1encoded)\n",
    "\n",
    "# plot latent dimensions\n",
    "AE1_encoded = AE1_latent.predict(polydata)\n",
    "AE1_encoded[:,0].shape\n",
    "_ = plt.scatter(AE1_encoded[:,0], AE1_encoded[:,1],s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with PCA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model = PCA(2)\n",
    "PCA_polydata = PCA_model.fit_transform(polydata)\n",
    "plt.scatter(PCA_polydata[:,0],PCA_polydata[:,1], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more complex autoencoder\n",
    "\n",
    "# model which computes the output of the hidden layer\n",
    "AE2_latent = Model(inputs, AE2encoded)\n",
    "# plot latent dimensions\n",
    "AE2_encoded = AE2_latent.predict(polydata)\n",
    "AE2_encoded[:,0].shape\n",
    "plt.scatter(AE2_encoded[:,0], AE2_encoded[:,1],s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree 2 polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree two polynomials\n",
    "Npoly = 3000\n",
    "deg2polydata = np.array([polynomial(2) for i in range(Npoly)])\n",
    "deg2polydata = deg2polydata.reshape(Npoly, size*size)\n",
    "deg2mean = np.mean(deg2polydata)\n",
    "deg2sdev = np.std(deg2polydata)\n",
    "deg2polydata = tf.keras.utils.normalize(deg2polydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on degree 2 polynomials\n",
    "AE1.set_weights(AE1_initial_weights)\n",
    "# AE1.compile(optimizer='adam', loss='mse')\n",
    "hist = AE1.fit(deg2polydata, deg2polydata, epochs=10, batch_size=100)\n",
    "\n",
    "# plot the latent dimensions of the small encoder for polynomials of degree 2\n",
    "AE1_encoded = AE1_latent.predict(deg2polydata)\n",
    "_ = plt.scatter(AE1_encoded[:,0], AE1_encoded[:,1], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on degree 2 polynomials\n",
    "AE2.set_weights(AE2_initial_weights)\n",
    "# AE2.compile(optimizer='adam', loss='mse')\n",
    "hist = AE2.fit(deg2polydata, deg2polydata, epochs=10, batch_size=100)\n",
    "\n",
    "# plot the latent dimensions of the small encoder for polynomials of degree 2\n",
    "AE2_encoded = AE2_latent.predict(deg2polydata)\n",
    "_ = plt.scatter(AE2_encoded[:,0], AE2_encoded[:,1], s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how certain classes of polynomials are mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly=a*x^2+y^2\n",
    "def poly_evaluate(curv_x, curv_y, slope_x, slope_y):\n",
    "    return [[curv_x*((x/size)**2)+curv_y*((y/size)**2)+slope_x*(x/size)+slope_y*(y/size)\n",
    "             for x in range(size)] for y in range(size)]\n",
    "\n",
    "# run these polynomials through AE2\n",
    "def predict(data):\n",
    "    data = data.reshape(100, size**2)\n",
    "    data = tf.keras.utils.normalize(data)\n",
    "    return AE2_latent.predict(data)\n",
    "    \n",
    "fig, axs = plt.subplots(1,2, figsize=(15,7))\n",
    "\n",
    "# quadratic\n",
    "data = np.array([poly_evaluate(a,1,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(1,a,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(-1,a,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(a,-1,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "axs[0].legend(['$p=a\\cdot x^2+y^2$', '$p=x^2+a\\cdot y^2$', '$p=-x^2+a\\cdot y^2$', '$p=a\\cdot x^2-y^2$'])\n",
    "axs[0].set_title('Pure quadratic polynomials')\n",
    "\n",
    "# linear\n",
    "data = np.array([poly_evaluate(0,0,a,1) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(0,0,1,a) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(0,0,-1,a) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(0,0,a,-1) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "axs[1].legend(['$p=a\\cdot x+y$', '$p=x+a\\cdot y$', '$p=-x+a\\cdot y$', '$p=a\\cdot x-y$'])\n",
    "axs[1].set_title('Pure linear polynomials')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent dimensions are related to the coefficients of the polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few inputs with varying average gradient\n",
    "fig, axs = plt.subplots(1,4, figsize=(20,5))\n",
    "for i, a in enumerate(np.linspace(-1,1,4)):\n",
    "    axs[i].imshow(poly_evaluate(a,1,0,0))\n",
    "    axs[i].set_title('$p=('+str(round(a,2))+')\\cdot x^2+y^2$')\n",
    "    \n",
    "fig, axs = plt.subplots(1,4, figsize=(20,5))\n",
    "for i, a in enumerate(np.linspace(-1,1,4)):\n",
    "    axs[i].imshow(poly_evaluate(0,0,a,1))\n",
    "    axs[i].set_title('$p=('+str(round(a,2))+')\\cdot x+y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it is no coincidence that the latent representation for the linear and quadratic polynomials looks almost the same. The auto-encoder seems to primarily learn to represent the average gradient of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decorrelate latent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wrap it by a function that can take arbitrary arguments and spits out the actual loss function\n",
    "def custom_loss(layer):\n",
    "    def loss(y_true, y_pred):\n",
    "        X = layer[:,0]\n",
    "        X = tf.expand_dims(X, axis=1)\n",
    "        Y = layer[:,1]\n",
    "        Y = tf.expand_dims(Y, axis=1)\n",
    "        latent_loss = tfp.stats.covariance(X,Y)[0,0]**2\n",
    "        return (tf.losses.mean_squared_error(y_true, y_pred)+(10**-1)*latent_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on degree 2 polynomials\n",
    "AE2.set_weights(AE2_initial_weights)\n",
    "AE2.compile(optimizer='adam', loss=custom_loss(AE2encoded))\n",
    "hist = AE2.fit(deg2polydata, deg2polydata, epochs=10, batch_size=100)\n",
    "\n",
    "# plot the latent dimensions of the small encoder for polynomials of degree 2\n",
    "AE2_encoded = AE2_latent.predict(deg2polydata)\n",
    "plt.scatter(AE2_encoded[:,0], AE2_encoded[:,1], s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let this run long enough, we can observe that two clusters form in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15,7))\n",
    "    \n",
    "data = np.array([poly_evaluate(a,1,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(1,a,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(-1,a,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(a,-1,0,0) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[0].scatter(pred[:,0], pred[:,1], s=1)\n",
    "axs[0].legend(['$p=a\\cdot x^2+y^2$', '$p=x^2+a\\cdot y^2$', '$p=-x^2+a\\cdot y^2$', '$p=a\\cdot x^2-y^2$'])\n",
    "axs[0].set_title('Pure quadratic polynomials')\n",
    "\n",
    "data = np.array([poly_evaluate(0,0,a,1) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(0,0,1,a) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(0,0,-1,a) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "\n",
    "data = np.array([poly_evaluate(0,0,a,-1) for a in np.linspace(-1,1,100)])\n",
    "pred = predict(data)\n",
    "axs[1].scatter(pred[:,0], pred[:,1], s=1)\n",
    "axs[1].legend(['$p=a\\cdot x+y$', '$p=x+a\\cdot y$', '$p=-x+a\\cdot y$', '$p=a\\cdot x-y$'])\n",
    "axs[1].set_title('Pure linear polynomials')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 KL-Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show by example that $D_\\text{KL}(Q||P)\\neq D_\\text{KL}(P||Q)$ in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple example is $P=\\{\\frac12,\\frac12\\}$ and $Q=\\{\\frac1N,\\frac{N-1}{N}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "P = np.array([1/2,1/2])\n",
    "Q = np.array([1/N,(N-1)/N])\n",
    "\n",
    "print(np.sum(P*np.log(P/Q)))\n",
    "print(np.sum(Q*np.log(Q/P)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
