{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_116 (Conv2D)          (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPaddi (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_16 (Reshape)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_32 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_33 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.999919] [G loss: 1.000155]\n",
      "1 [D loss: 0.999921] [G loss: 1.000161]\n",
      "2 [D loss: 0.999924] [G loss: 1.000155]\n",
      "3 [D loss: 0.999919] [G loss: 1.000152]\n",
      "4 [D loss: 0.999924] [G loss: 1.000156]\n",
      "5 [D loss: 0.999922] [G loss: 1.000152]\n",
      "6 [D loss: 0.999925] [G loss: 1.000159]\n",
      "7 [D loss: 0.999927] [G loss: 1.000154]\n",
      "8 [D loss: 0.999928] [G loss: 1.000152]\n",
      "9 [D loss: 0.999927] [G loss: 1.000147]\n",
      "10 [D loss: 0.999925] [G loss: 1.000148]\n",
      "11 [D loss: 0.999929] [G loss: 1.000147]\n",
      "12 [D loss: 0.999933] [G loss: 1.000139]\n",
      "13 [D loss: 0.999927] [G loss: 1.000134]\n",
      "14 [D loss: 0.999934] [G loss: 1.000125]\n",
      "15 [D loss: 0.999935] [G loss: 1.000112]\n",
      "16 [D loss: 0.999940] [G loss: 1.000106]\n",
      "17 [D loss: 0.999944] [G loss: 1.000095]\n",
      "18 [D loss: 0.999949] [G loss: 1.000092]\n",
      "19 [D loss: 0.999954] [G loss: 1.000083]\n",
      "20 [D loss: 0.999957] [G loss: 1.000081]\n",
      "21 [D loss: 0.999960] [G loss: 1.000074]\n",
      "22 [D loss: 0.999962] [G loss: 1.000077]\n",
      "23 [D loss: 0.999963] [G loss: 1.000071]\n",
      "24 [D loss: 0.999963] [G loss: 1.000068]\n",
      "25 [D loss: 0.999966] [G loss: 1.000069]\n",
      "26 [D loss: 0.999969] [G loss: 1.000068]\n",
      "27 [D loss: 0.999967] [G loss: 1.000066]\n",
      "28 [D loss: 0.999968] [G loss: 1.000065]\n",
      "29 [D loss: 0.999968] [G loss: 1.000064]\n",
      "30 [D loss: 0.999969] [G loss: 1.000066]\n",
      "31 [D loss: 0.999968] [G loss: 1.000066]\n",
      "32 [D loss: 0.999969] [G loss: 1.000069]\n",
      "33 [D loss: 0.999970] [G loss: 1.000068]\n",
      "34 [D loss: 0.999972] [G loss: 1.000067]\n",
      "35 [D loss: 0.999972] [G loss: 1.000064]\n",
      "36 [D loss: 0.999972] [G loss: 1.000066]\n",
      "37 [D loss: 0.999971] [G loss: 1.000068]\n",
      "38 [D loss: 0.999967] [G loss: 1.000066]\n",
      "39 [D loss: 0.999970] [G loss: 1.000065]\n",
      "40 [D loss: 0.999970] [G loss: 1.000066]\n",
      "41 [D loss: 0.999972] [G loss: 1.000066]\n",
      "42 [D loss: 0.999973] [G loss: 1.000060]\n",
      "43 [D loss: 0.999969] [G loss: 1.000067]\n",
      "44 [D loss: 0.999969] [G loss: 1.000065]\n",
      "45 [D loss: 0.999969] [G loss: 1.000063]\n",
      "46 [D loss: 0.999968] [G loss: 1.000065]\n",
      "47 [D loss: 0.999970] [G loss: 1.000066]\n",
      "48 [D loss: 0.999970] [G loss: 1.000069]\n",
      "49 [D loss: 0.999970] [G loss: 1.000063]\n",
      "50 [D loss: 0.999972] [G loss: 1.000064]\n",
      "51 [D loss: 0.999970] [G loss: 1.000065]\n",
      "52 [D loss: 0.999969] [G loss: 1.000064]\n",
      "53 [D loss: 0.999970] [G loss: 1.000063]\n",
      "54 [D loss: 0.999972] [G loss: 1.000066]\n",
      "55 [D loss: 0.999971] [G loss: 1.000061]\n",
      "56 [D loss: 0.999969] [G loss: 1.000065]\n",
      "57 [D loss: 0.999969] [G loss: 1.000067]\n",
      "58 [D loss: 0.999971] [G loss: 1.000063]\n",
      "59 [D loss: 0.999969] [G loss: 1.000067]\n",
      "60 [D loss: 0.999971] [G loss: 1.000064]\n",
      "61 [D loss: 0.999969] [G loss: 1.000065]\n",
      "62 [D loss: 0.999969] [G loss: 1.000066]\n",
      "63 [D loss: 0.999971] [G loss: 1.000064]\n",
      "64 [D loss: 0.999971] [G loss: 1.000065]\n",
      "65 [D loss: 0.999966] [G loss: 1.000067]\n",
      "66 [D loss: 0.999971] [G loss: 1.000062]\n",
      "67 [D loss: 0.999969] [G loss: 1.000063]\n",
      "68 [D loss: 0.999970] [G loss: 1.000067]\n",
      "69 [D loss: 0.999972] [G loss: 1.000064]\n",
      "70 [D loss: 0.999971] [G loss: 1.000065]\n",
      "71 [D loss: 0.999969] [G loss: 1.000057]\n",
      "72 [D loss: 0.999967] [G loss: 1.000066]\n",
      "73 [D loss: 0.999971] [G loss: 1.000067]\n",
      "74 [D loss: 0.999968] [G loss: 1.000065]\n",
      "75 [D loss: 0.999970] [G loss: 1.000065]\n",
      "76 [D loss: 0.999968] [G loss: 1.000067]\n",
      "77 [D loss: 0.999970] [G loss: 1.000066]\n",
      "78 [D loss: 0.999970] [G loss: 1.000064]\n",
      "79 [D loss: 0.999970] [G loss: 1.000064]\n",
      "80 [D loss: 0.999970] [G loss: 1.000068]\n",
      "81 [D loss: 0.999971] [G loss: 1.000065]\n",
      "82 [D loss: 0.999967] [G loss: 1.000063]\n",
      "83 [D loss: 0.999969] [G loss: 1.000069]\n",
      "84 [D loss: 0.999970] [G loss: 1.000066]\n",
      "85 [D loss: 0.999972] [G loss: 1.000062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 [D loss: 0.999970] [G loss: 1.000065]\n",
      "87 [D loss: 0.999972] [G loss: 1.000068]\n",
      "88 [D loss: 0.999969] [G loss: 1.000063]\n",
      "89 [D loss: 0.999970] [G loss: 1.000067]\n",
      "90 [D loss: 0.999970] [G loss: 1.000064]\n",
      "91 [D loss: 0.999969] [G loss: 1.000066]\n",
      "92 [D loss: 0.999970] [G loss: 1.000067]\n",
      "93 [D loss: 0.999971] [G loss: 1.000067]\n",
      "94 [D loss: 0.999969] [G loss: 1.000066]\n",
      "95 [D loss: 0.999971] [G loss: 1.000066]\n",
      "96 [D loss: 0.999970] [G loss: 1.000066]\n",
      "97 [D loss: 0.999969] [G loss: 1.000065]\n",
      "98 [D loss: 0.999971] [G loss: 1.000065]\n",
      "99 [D loss: 0.999969] [G loss: 1.000063]\n",
      "100 [D loss: 0.999970] [G loss: 1.000064]\n",
      "101 [D loss: 0.999972] [G loss: 1.000067]\n",
      "102 [D loss: 0.999972] [G loss: 1.000065]\n",
      "103 [D loss: 0.999972] [G loss: 1.000064]\n",
      "104 [D loss: 0.999971] [G loss: 1.000062]\n",
      "105 [D loss: 0.999969] [G loss: 1.000063]\n",
      "106 [D loss: 0.999972] [G loss: 1.000069]\n",
      "107 [D loss: 0.999970] [G loss: 1.000062]\n",
      "108 [D loss: 0.999971] [G loss: 1.000067]\n",
      "109 [D loss: 0.999971] [G loss: 1.000065]\n",
      "110 [D loss: 0.999970] [G loss: 1.000064]\n",
      "111 [D loss: 0.999968] [G loss: 1.000064]\n",
      "112 [D loss: 0.999971] [G loss: 1.000061]\n",
      "113 [D loss: 0.999970] [G loss: 1.000063]\n",
      "114 [D loss: 0.999969] [G loss: 1.000062]\n",
      "115 [D loss: 0.999971] [G loss: 1.000063]\n",
      "116 [D loss: 0.999971] [G loss: 1.000065]\n",
      "117 [D loss: 0.999969] [G loss: 1.000064]\n",
      "118 [D loss: 0.999968] [G loss: 1.000066]\n",
      "119 [D loss: 0.999970] [G loss: 1.000067]\n",
      "120 [D loss: 0.999970] [G loss: 1.000067]\n",
      "121 [D loss: 0.999970] [G loss: 1.000065]\n",
      "122 [D loss: 0.999969] [G loss: 1.000065]\n",
      "123 [D loss: 0.999972] [G loss: 1.000065]\n",
      "124 [D loss: 0.999968] [G loss: 1.000064]\n",
      "125 [D loss: 0.999969] [G loss: 1.000065]\n",
      "126 [D loss: 0.999968] [G loss: 1.000066]\n",
      "127 [D loss: 0.999971] [G loss: 1.000064]\n",
      "128 [D loss: 0.999971] [G loss: 1.000064]\n",
      "129 [D loss: 0.999969] [G loss: 1.000067]\n",
      "130 [D loss: 0.999972] [G loss: 1.000062]\n",
      "131 [D loss: 0.999970] [G loss: 1.000063]\n",
      "132 [D loss: 0.999969] [G loss: 1.000064]\n",
      "133 [D loss: 0.999973] [G loss: 1.000068]\n",
      "134 [D loss: 0.999971] [G loss: 1.000064]\n",
      "135 [D loss: 0.999971] [G loss: 1.000068]\n",
      "136 [D loss: 0.999967] [G loss: 1.000067]\n",
      "137 [D loss: 0.999968] [G loss: 1.000066]\n",
      "138 [D loss: 0.999971] [G loss: 1.000062]\n",
      "139 [D loss: 0.999969] [G loss: 1.000065]\n",
      "140 [D loss: 0.999969] [G loss: 1.000065]\n",
      "141 [D loss: 0.999968] [G loss: 1.000066]\n",
      "142 [D loss: 0.999971] [G loss: 1.000061]\n",
      "143 [D loss: 0.999971] [G loss: 1.000062]\n",
      "144 [D loss: 0.999971] [G loss: 1.000067]\n",
      "145 [D loss: 0.999971] [G loss: 1.000065]\n",
      "146 [D loss: 0.999969] [G loss: 1.000069]\n",
      "147 [D loss: 0.999970] [G loss: 1.000065]\n",
      "148 [D loss: 0.999971] [G loss: 1.000064]\n",
      "149 [D loss: 0.999972] [G loss: 1.000068]\n",
      "150 [D loss: 0.999970] [G loss: 1.000066]\n",
      "151 [D loss: 0.999970] [G loss: 1.000068]\n",
      "152 [D loss: 0.999970] [G loss: 1.000068]\n",
      "153 [D loss: 0.999972] [G loss: 1.000064]\n",
      "154 [D loss: 0.999970] [G loss: 1.000067]\n",
      "155 [D loss: 0.999969] [G loss: 1.000064]\n",
      "156 [D loss: 0.999971] [G loss: 1.000065]\n",
      "157 [D loss: 0.999970] [G loss: 1.000065]\n",
      "158 [D loss: 0.999969] [G loss: 1.000064]\n",
      "159 [D loss: 0.999971] [G loss: 1.000067]\n",
      "160 [D loss: 0.999971] [G loss: 1.000061]\n",
      "161 [D loss: 0.999973] [G loss: 1.000065]\n",
      "162 [D loss: 0.999969] [G loss: 1.000066]\n",
      "163 [D loss: 0.999968] [G loss: 1.000067]\n",
      "164 [D loss: 0.999968] [G loss: 1.000062]\n",
      "165 [D loss: 0.999969] [G loss: 1.000065]\n",
      "166 [D loss: 0.999969] [G loss: 1.000065]\n",
      "167 [D loss: 0.999970] [G loss: 1.000063]\n",
      "168 [D loss: 0.999972] [G loss: 1.000066]\n",
      "169 [D loss: 0.999971] [G loss: 1.000065]\n",
      "170 [D loss: 0.999967] [G loss: 1.000064]\n",
      "171 [D loss: 0.999971] [G loss: 1.000065]\n",
      "172 [D loss: 0.999971] [G loss: 1.000064]\n",
      "173 [D loss: 0.999969] [G loss: 1.000066]\n",
      "174 [D loss: 0.999970] [G loss: 1.000062]\n",
      "175 [D loss: 0.999973] [G loss: 1.000063]\n",
      "176 [D loss: 0.999971] [G loss: 1.000063]\n",
      "177 [D loss: 0.999973] [G loss: 1.000065]\n",
      "178 [D loss: 0.999968] [G loss: 1.000059]\n",
      "179 [D loss: 0.999970] [G loss: 1.000066]\n",
      "180 [D loss: 0.999970] [G loss: 1.000066]\n",
      "181 [D loss: 0.999967] [G loss: 1.000065]\n",
      "182 [D loss: 0.999972] [G loss: 1.000061]\n",
      "183 [D loss: 0.999973] [G loss: 1.000063]\n",
      "184 [D loss: 0.999972] [G loss: 1.000066]\n",
      "185 [D loss: 0.999972] [G loss: 1.000061]\n",
      "186 [D loss: 0.999971] [G loss: 1.000067]\n",
      "187 [D loss: 0.999971] [G loss: 1.000069]\n",
      "188 [D loss: 0.999970] [G loss: 1.000063]\n",
      "189 [D loss: 0.999971] [G loss: 1.000065]\n",
      "190 [D loss: 0.999971] [G loss: 1.000066]\n",
      "191 [D loss: 0.999968] [G loss: 1.000066]\n",
      "192 [D loss: 0.999971] [G loss: 1.000065]\n",
      "193 [D loss: 0.999973] [G loss: 1.000068]\n",
      "194 [D loss: 0.999972] [G loss: 1.000064]\n",
      "195 [D loss: 0.999973] [G loss: 1.000059]\n",
      "196 [D loss: 0.999968] [G loss: 1.000067]\n",
      "197 [D loss: 0.999972] [G loss: 1.000062]\n",
      "198 [D loss: 0.999969] [G loss: 1.000068]\n",
      "199 [D loss: 0.999971] [G loss: 1.000069]\n",
      "200 [D loss: 0.999970] [G loss: 1.000065]\n",
      "201 [D loss: 0.999970] [G loss: 1.000067]\n",
      "202 [D loss: 0.999972] [G loss: 1.000066]\n",
      "203 [D loss: 0.999972] [G loss: 1.000064]\n",
      "204 [D loss: 0.999973] [G loss: 1.000064]\n",
      "205 [D loss: 0.999970] [G loss: 1.000069]\n",
      "206 [D loss: 0.999971] [G loss: 1.000066]\n",
      "207 [D loss: 0.999970] [G loss: 1.000068]\n",
      "208 [D loss: 0.999969] [G loss: 1.000062]\n",
      "209 [D loss: 0.999970] [G loss: 1.000066]\n",
      "210 [D loss: 0.999969] [G loss: 1.000065]\n",
      "211 [D loss: 0.999969] [G loss: 1.000064]\n",
      "212 [D loss: 0.999971] [G loss: 1.000063]\n",
      "213 [D loss: 0.999969] [G loss: 1.000069]\n",
      "214 [D loss: 0.999972] [G loss: 1.000066]\n",
      "215 [D loss: 0.999969] [G loss: 1.000065]\n",
      "216 [D loss: 0.999972] [G loss: 1.000059]\n",
      "217 [D loss: 0.999970] [G loss: 1.000065]\n",
      "218 [D loss: 0.999970] [G loss: 1.000065]\n",
      "219 [D loss: 0.999971] [G loss: 1.000065]\n",
      "220 [D loss: 0.999970] [G loss: 1.000064]\n",
      "221 [D loss: 0.999971] [G loss: 1.000065]\n",
      "222 [D loss: 0.999970] [G loss: 1.000064]\n",
      "223 [D loss: 0.999973] [G loss: 1.000066]\n",
      "224 [D loss: 0.999971] [G loss: 1.000061]\n",
      "225 [D loss: 0.999970] [G loss: 1.000064]\n",
      "226 [D loss: 0.999971] [G loss: 1.000067]\n",
      "227 [D loss: 0.999970] [G loss: 1.000065]\n",
      "228 [D loss: 0.999971] [G loss: 1.000062]\n",
      "229 [D loss: 0.999971] [G loss: 1.000062]\n",
      "230 [D loss: 0.999972] [G loss: 1.000065]\n",
      "231 [D loss: 0.999972] [G loss: 1.000064]\n",
      "232 [D loss: 0.999974] [G loss: 1.000062]\n",
      "233 [D loss: 0.999968] [G loss: 1.000069]\n",
      "234 [D loss: 0.999968] [G loss: 1.000065]\n",
      "235 [D loss: 0.999971] [G loss: 1.000064]\n",
      "236 [D loss: 0.999969] [G loss: 1.000063]\n",
      "237 [D loss: 0.999972] [G loss: 1.000065]\n",
      "238 [D loss: 0.999973] [G loss: 1.000068]\n",
      "239 [D loss: 0.999973] [G loss: 1.000065]\n",
      "240 [D loss: 0.999972] [G loss: 1.000064]\n",
      "241 [D loss: 0.999973] [G loss: 1.000064]\n",
      "242 [D loss: 0.999971] [G loss: 1.000061]\n",
      "243 [D loss: 0.999972] [G loss: 1.000061]\n",
      "244 [D loss: 0.999971] [G loss: 1.000062]\n",
      "245 [D loss: 0.999965] [G loss: 1.000064]\n",
      "246 [D loss: 0.999971] [G loss: 1.000064]\n",
      "247 [D loss: 0.999971] [G loss: 1.000064]\n",
      "248 [D loss: 0.999970] [G loss: 1.000066]\n",
      "249 [D loss: 0.999968] [G loss: 1.000064]\n",
      "250 [D loss: 0.999970] [G loss: 1.000067]\n",
      "251 [D loss: 0.999970] [G loss: 1.000063]\n",
      "252 [D loss: 0.999970] [G loss: 1.000065]\n",
      "253 [D loss: 0.999972] [G loss: 1.000064]\n",
      "254 [D loss: 0.999970] [G loss: 1.000066]\n",
      "255 [D loss: 0.999971] [G loss: 1.000065]\n",
      "256 [D loss: 0.999971] [G loss: 1.000065]\n",
      "257 [D loss: 0.999970] [G loss: 1.000066]\n",
      "258 [D loss: 0.999972] [G loss: 1.000067]\n",
      "259 [D loss: 0.999970] [G loss: 1.000062]\n",
      "260 [D loss: 0.999970] [G loss: 1.000066]\n",
      "261 [D loss: 0.999970] [G loss: 1.000070]\n",
      "262 [D loss: 0.999972] [G loss: 1.000063]\n",
      "263 [D loss: 0.999968] [G loss: 1.000065]\n",
      "264 [D loss: 0.999973] [G loss: 1.000065]\n",
      "265 [D loss: 0.999971] [G loss: 1.000064]\n",
      "266 [D loss: 0.999972] [G loss: 1.000062]\n",
      "267 [D loss: 0.999971] [G loss: 1.000064]\n",
      "268 [D loss: 0.999969] [G loss: 1.000069]\n",
      "269 [D loss: 0.999973] [G loss: 1.000066]\n",
      "270 [D loss: 0.999973] [G loss: 1.000067]\n",
      "271 [D loss: 0.999970] [G loss: 1.000064]\n",
      "272 [D loss: 0.999970] [G loss: 1.000065]\n",
      "273 [D loss: 0.999969] [G loss: 1.000062]\n",
      "274 [D loss: 0.999971] [G loss: 1.000066]\n",
      "275 [D loss: 0.999967] [G loss: 1.000067]\n",
      "276 [D loss: 0.999969] [G loss: 1.000067]\n",
      "277 [D loss: 0.999970] [G loss: 1.000067]\n",
      "278 [D loss: 0.999971] [G loss: 1.000064]\n",
      "279 [D loss: 0.999971] [G loss: 1.000063]\n",
      "280 [D loss: 0.999971] [G loss: 1.000066]\n",
      "281 [D loss: 0.999970] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 [D loss: 0.999970] [G loss: 1.000065]\n",
      "283 [D loss: 0.999970] [G loss: 1.000065]\n",
      "284 [D loss: 0.999969] [G loss: 1.000065]\n",
      "285 [D loss: 0.999967] [G loss: 1.000063]\n",
      "286 [D loss: 0.999971] [G loss: 1.000066]\n",
      "287 [D loss: 0.999971] [G loss: 1.000067]\n",
      "288 [D loss: 0.999971] [G loss: 1.000067]\n",
      "289 [D loss: 0.999971] [G loss: 1.000064]\n",
      "290 [D loss: 0.999969] [G loss: 1.000063]\n",
      "291 [D loss: 0.999970] [G loss: 1.000062]\n",
      "292 [D loss: 0.999970] [G loss: 1.000065]\n",
      "293 [D loss: 0.999972] [G loss: 1.000065]\n",
      "294 [D loss: 0.999973] [G loss: 1.000066]\n",
      "295 [D loss: 0.999972] [G loss: 1.000065]\n",
      "296 [D loss: 0.999970] [G loss: 1.000065]\n",
      "297 [D loss: 0.999971] [G loss: 1.000063]\n",
      "298 [D loss: 0.999970] [G loss: 1.000065]\n",
      "299 [D loss: 0.999967] [G loss: 1.000064]\n",
      "300 [D loss: 0.999970] [G loss: 1.000064]\n",
      "301 [D loss: 0.999972] [G loss: 1.000066]\n",
      "302 [D loss: 0.999970] [G loss: 1.000064]\n",
      "303 [D loss: 0.999971] [G loss: 1.000065]\n",
      "304 [D loss: 0.999971] [G loss: 1.000063]\n",
      "305 [D loss: 0.999971] [G loss: 1.000062]\n",
      "306 [D loss: 0.999967] [G loss: 1.000066]\n",
      "307 [D loss: 0.999970] [G loss: 1.000064]\n",
      "308 [D loss: 0.999970] [G loss: 1.000064]\n",
      "309 [D loss: 0.999970] [G loss: 1.000066]\n",
      "310 [D loss: 0.999971] [G loss: 1.000068]\n",
      "311 [D loss: 0.999972] [G loss: 1.000065]\n",
      "312 [D loss: 0.999970] [G loss: 1.000065]\n",
      "313 [D loss: 0.999969] [G loss: 1.000069]\n",
      "314 [D loss: 0.999973] [G loss: 1.000064]\n",
      "315 [D loss: 0.999968] [G loss: 1.000066]\n",
      "316 [D loss: 0.999972] [G loss: 1.000066]\n",
      "317 [D loss: 0.999969] [G loss: 1.000069]\n",
      "318 [D loss: 0.999969] [G loss: 1.000064]\n",
      "319 [D loss: 0.999971] [G loss: 1.000065]\n",
      "320 [D loss: 0.999972] [G loss: 1.000063]\n",
      "321 [D loss: 0.999969] [G loss: 1.000063]\n",
      "322 [D loss: 0.999971] [G loss: 1.000064]\n",
      "323 [D loss: 0.999970] [G loss: 1.000064]\n",
      "324 [D loss: 0.999969] [G loss: 1.000064]\n",
      "325 [D loss: 0.999969] [G loss: 1.000061]\n",
      "326 [D loss: 0.999970] [G loss: 1.000063]\n",
      "327 [D loss: 0.999969] [G loss: 1.000069]\n",
      "328 [D loss: 0.999969] [G loss: 1.000064]\n",
      "329 [D loss: 0.999969] [G loss: 1.000064]\n",
      "330 [D loss: 0.999970] [G loss: 1.000067]\n",
      "331 [D loss: 0.999971] [G loss: 1.000063]\n",
      "332 [D loss: 0.999973] [G loss: 1.000066]\n",
      "333 [D loss: 0.999970] [G loss: 1.000063]\n",
      "334 [D loss: 0.999972] [G loss: 1.000067]\n",
      "335 [D loss: 0.999970] [G loss: 1.000064]\n",
      "336 [D loss: 0.999972] [G loss: 1.000063]\n",
      "337 [D loss: 0.999970] [G loss: 1.000068]\n",
      "338 [D loss: 0.999970] [G loss: 1.000064]\n",
      "339 [D loss: 0.999972] [G loss: 1.000065]\n",
      "340 [D loss: 0.999969] [G loss: 1.000066]\n",
      "341 [D loss: 0.999972] [G loss: 1.000062]\n",
      "342 [D loss: 0.999970] [G loss: 1.000063]\n",
      "343 [D loss: 0.999970] [G loss: 1.000068]\n",
      "344 [D loss: 0.999969] [G loss: 1.000064]\n",
      "345 [D loss: 0.999969] [G loss: 1.000062]\n",
      "346 [D loss: 0.999969] [G loss: 1.000063]\n",
      "347 [D loss: 0.999969] [G loss: 1.000066]\n",
      "348 [D loss: 0.999970] [G loss: 1.000065]\n",
      "349 [D loss: 0.999968] [G loss: 1.000066]\n",
      "350 [D loss: 0.999972] [G loss: 1.000066]\n",
      "351 [D loss: 0.999971] [G loss: 1.000064]\n",
      "352 [D loss: 0.999970] [G loss: 1.000062]\n",
      "353 [D loss: 0.999971] [G loss: 1.000062]\n",
      "354 [D loss: 0.999971] [G loss: 1.000066]\n",
      "355 [D loss: 0.999972] [G loss: 1.000066]\n",
      "356 [D loss: 0.999969] [G loss: 1.000063]\n",
      "357 [D loss: 0.999969] [G loss: 1.000069]\n",
      "358 [D loss: 0.999971] [G loss: 1.000067]\n",
      "359 [D loss: 0.999971] [G loss: 1.000064]\n",
      "360 [D loss: 0.999969] [G loss: 1.000067]\n",
      "361 [D loss: 0.999971] [G loss: 1.000065]\n",
      "362 [D loss: 0.999970] [G loss: 1.000066]\n",
      "363 [D loss: 0.999972] [G loss: 1.000065]\n",
      "364 [D loss: 0.999974] [G loss: 1.000064]\n",
      "365 [D loss: 0.999971] [G loss: 1.000065]\n",
      "366 [D loss: 0.999969] [G loss: 1.000066]\n",
      "367 [D loss: 0.999971] [G loss: 1.000064]\n",
      "368 [D loss: 0.999970] [G loss: 1.000064]\n",
      "369 [D loss: 0.999969] [G loss: 1.000064]\n",
      "370 [D loss: 0.999972] [G loss: 1.000064]\n",
      "371 [D loss: 0.999972] [G loss: 1.000065]\n",
      "372 [D loss: 0.999971] [G loss: 1.000067]\n",
      "373 [D loss: 0.999967] [G loss: 1.000065]\n",
      "374 [D loss: 0.999969] [G loss: 1.000060]\n",
      "375 [D loss: 0.999968] [G loss: 1.000066]\n",
      "376 [D loss: 0.999970] [G loss: 1.000066]\n",
      "377 [D loss: 0.999968] [G loss: 1.000063]\n",
      "378 [D loss: 0.999971] [G loss: 1.000066]\n",
      "379 [D loss: 0.999969] [G loss: 1.000063]\n",
      "380 [D loss: 0.999972] [G loss: 1.000064]\n",
      "381 [D loss: 0.999968] [G loss: 1.000063]\n",
      "382 [D loss: 0.999968] [G loss: 1.000065]\n",
      "383 [D loss: 0.999972] [G loss: 1.000063]\n",
      "384 [D loss: 0.999971] [G loss: 1.000066]\n",
      "385 [D loss: 0.999971] [G loss: 1.000066]\n",
      "386 [D loss: 0.999971] [G loss: 1.000064]\n",
      "387 [D loss: 0.999970] [G loss: 1.000066]\n",
      "388 [D loss: 0.999970] [G loss: 1.000065]\n",
      "389 [D loss: 0.999968] [G loss: 1.000067]\n",
      "390 [D loss: 0.999970] [G loss: 1.000067]\n",
      "391 [D loss: 0.999968] [G loss: 1.000063]\n",
      "392 [D loss: 0.999970] [G loss: 1.000066]\n",
      "393 [D loss: 0.999970] [G loss: 1.000066]\n",
      "394 [D loss: 0.999971] [G loss: 1.000063]\n",
      "395 [D loss: 0.999971] [G loss: 1.000066]\n",
      "396 [D loss: 0.999970] [G loss: 1.000066]\n",
      "397 [D loss: 0.999971] [G loss: 1.000067]\n",
      "398 [D loss: 0.999970] [G loss: 1.000067]\n",
      "399 [D loss: 0.999971] [G loss: 1.000069]\n",
      "400 [D loss: 0.999969] [G loss: 1.000065]\n",
      "401 [D loss: 0.999971] [G loss: 1.000068]\n",
      "402 [D loss: 0.999972] [G loss: 1.000066]\n",
      "403 [D loss: 0.999970] [G loss: 1.000066]\n",
      "404 [D loss: 0.999972] [G loss: 1.000066]\n",
      "405 [D loss: 0.999971] [G loss: 1.000065]\n",
      "406 [D loss: 0.999970] [G loss: 1.000068]\n",
      "407 [D loss: 0.999971] [G loss: 1.000062]\n",
      "408 [D loss: 0.999971] [G loss: 1.000063]\n",
      "409 [D loss: 0.999971] [G loss: 1.000065]\n",
      "410 [D loss: 0.999970] [G loss: 1.000061]\n",
      "411 [D loss: 0.999971] [G loss: 1.000066]\n",
      "412 [D loss: 0.999968] [G loss: 1.000063]\n",
      "413 [D loss: 0.999970] [G loss: 1.000063]\n",
      "414 [D loss: 0.999969] [G loss: 1.000066]\n",
      "415 [D loss: 0.999972] [G loss: 1.000068]\n",
      "416 [D loss: 0.999969] [G loss: 1.000063]\n",
      "417 [D loss: 0.999971] [G loss: 1.000063]\n",
      "418 [D loss: 0.999970] [G loss: 1.000062]\n",
      "419 [D loss: 0.999972] [G loss: 1.000066]\n",
      "420 [D loss: 0.999970] [G loss: 1.000065]\n",
      "421 [D loss: 0.999970] [G loss: 1.000062]\n",
      "422 [D loss: 0.999971] [G loss: 1.000064]\n",
      "423 [D loss: 0.999973] [G loss: 1.000062]\n",
      "424 [D loss: 0.999970] [G loss: 1.000064]\n",
      "425 [D loss: 0.999972] [G loss: 1.000064]\n",
      "426 [D loss: 0.999970] [G loss: 1.000063]\n",
      "427 [D loss: 0.999972] [G loss: 1.000062]\n",
      "428 [D loss: 0.999968] [G loss: 1.000065]\n",
      "429 [D loss: 0.999971] [G loss: 1.000062]\n",
      "430 [D loss: 0.999969] [G loss: 1.000061]\n",
      "431 [D loss: 0.999970] [G loss: 1.000066]\n",
      "432 [D loss: 0.999971] [G loss: 1.000065]\n",
      "433 [D loss: 0.999970] [G loss: 1.000065]\n",
      "434 [D loss: 0.999971] [G loss: 1.000065]\n",
      "435 [D loss: 0.999970] [G loss: 1.000064]\n",
      "436 [D loss: 0.999973] [G loss: 1.000064]\n",
      "437 [D loss: 0.999971] [G loss: 1.000064]\n",
      "438 [D loss: 0.999968] [G loss: 1.000067]\n",
      "439 [D loss: 0.999970] [G loss: 1.000067]\n",
      "440 [D loss: 0.999970] [G loss: 1.000065]\n",
      "441 [D loss: 0.999969] [G loss: 1.000065]\n",
      "442 [D loss: 0.999971] [G loss: 1.000064]\n",
      "443 [D loss: 0.999971] [G loss: 1.000066]\n",
      "444 [D loss: 0.999969] [G loss: 1.000065]\n",
      "445 [D loss: 0.999971] [G loss: 1.000066]\n",
      "446 [D loss: 0.999971] [G loss: 1.000064]\n",
      "447 [D loss: 0.999971] [G loss: 1.000065]\n",
      "448 [D loss: 0.999972] [G loss: 1.000064]\n",
      "449 [D loss: 0.999968] [G loss: 1.000068]\n",
      "450 [D loss: 0.999969] [G loss: 1.000069]\n",
      "451 [D loss: 0.999971] [G loss: 1.000065]\n",
      "452 [D loss: 0.999972] [G loss: 1.000065]\n",
      "453 [D loss: 0.999969] [G loss: 1.000062]\n",
      "454 [D loss: 0.999970] [G loss: 1.000065]\n",
      "455 [D loss: 0.999972] [G loss: 1.000067]\n",
      "456 [D loss: 0.999971] [G loss: 1.000065]\n",
      "457 [D loss: 0.999969] [G loss: 1.000067]\n",
      "458 [D loss: 0.999970] [G loss: 1.000063]\n",
      "459 [D loss: 0.999969] [G loss: 1.000068]\n",
      "460 [D loss: 0.999972] [G loss: 1.000067]\n",
      "461 [D loss: 0.999971] [G loss: 1.000062]\n",
      "462 [D loss: 0.999972] [G loss: 1.000067]\n",
      "463 [D loss: 0.999973] [G loss: 1.000061]\n",
      "464 [D loss: 0.999969] [G loss: 1.000064]\n",
      "465 [D loss: 0.999970] [G loss: 1.000063]\n",
      "466 [D loss: 0.999969] [G loss: 1.000066]\n",
      "467 [D loss: 0.999972] [G loss: 1.000065]\n",
      "468 [D loss: 0.999972] [G loss: 1.000063]\n",
      "469 [D loss: 0.999969] [G loss: 1.000066]\n",
      "470 [D loss: 0.999972] [G loss: 1.000068]\n",
      "471 [D loss: 0.999970] [G loss: 1.000063]\n",
      "472 [D loss: 0.999971] [G loss: 1.000062]\n",
      "473 [D loss: 0.999971] [G loss: 1.000064]\n",
      "474 [D loss: 0.999970] [G loss: 1.000064]\n",
      "475 [D loss: 0.999970] [G loss: 1.000066]\n",
      "476 [D loss: 0.999971] [G loss: 1.000069]\n",
      "477 [D loss: 0.999969] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 [D loss: 0.999969] [G loss: 1.000066]\n",
      "479 [D loss: 0.999968] [G loss: 1.000067]\n",
      "480 [D loss: 0.999970] [G loss: 1.000063]\n",
      "481 [D loss: 0.999970] [G loss: 1.000067]\n",
      "482 [D loss: 0.999970] [G loss: 1.000067]\n",
      "483 [D loss: 0.999971] [G loss: 1.000067]\n",
      "484 [D loss: 0.999971] [G loss: 1.000068]\n",
      "485 [D loss: 0.999970] [G loss: 1.000064]\n",
      "486 [D loss: 0.999971] [G loss: 1.000066]\n",
      "487 [D loss: 0.999971] [G loss: 1.000065]\n",
      "488 [D loss: 0.999972] [G loss: 1.000066]\n",
      "489 [D loss: 0.999969] [G loss: 1.000065]\n",
      "490 [D loss: 0.999968] [G loss: 1.000066]\n",
      "491 [D loss: 0.999972] [G loss: 1.000065]\n",
      "492 [D loss: 0.999971] [G loss: 1.000066]\n",
      "493 [D loss: 0.999971] [G loss: 1.000066]\n",
      "494 [D loss: 0.999969] [G loss: 1.000061]\n",
      "495 [D loss: 0.999967] [G loss: 1.000066]\n",
      "496 [D loss: 0.999973] [G loss: 1.000067]\n",
      "497 [D loss: 0.999969] [G loss: 1.000069]\n",
      "498 [D loss: 0.999971] [G loss: 1.000065]\n",
      "499 [D loss: 0.999969] [G loss: 1.000063]\n",
      "500 [D loss: 0.999971] [G loss: 1.000068]\n",
      "501 [D loss: 0.999972] [G loss: 1.000065]\n",
      "502 [D loss: 0.999971] [G loss: 1.000067]\n",
      "503 [D loss: 0.999970] [G loss: 1.000064]\n",
      "504 [D loss: 0.999972] [G loss: 1.000060]\n",
      "505 [D loss: 0.999971] [G loss: 1.000067]\n",
      "506 [D loss: 0.999972] [G loss: 1.000066]\n",
      "507 [D loss: 0.999970] [G loss: 1.000065]\n",
      "508 [D loss: 0.999971] [G loss: 1.000066]\n",
      "509 [D loss: 0.999968] [G loss: 1.000065]\n",
      "510 [D loss: 0.999974] [G loss: 1.000064]\n",
      "511 [D loss: 0.999971] [G loss: 1.000064]\n",
      "512 [D loss: 0.999970] [G loss: 1.000064]\n",
      "513 [D loss: 0.999973] [G loss: 1.000064]\n",
      "514 [D loss: 0.999973] [G loss: 1.000067]\n",
      "515 [D loss: 0.999972] [G loss: 1.000061]\n",
      "516 [D loss: 0.999970] [G loss: 1.000063]\n",
      "517 [D loss: 0.999970] [G loss: 1.000062]\n",
      "518 [D loss: 0.999974] [G loss: 1.000062]\n",
      "519 [D loss: 0.999970] [G loss: 1.000062]\n",
      "520 [D loss: 0.999968] [G loss: 1.000064]\n",
      "521 [D loss: 0.999971] [G loss: 1.000064]\n",
      "522 [D loss: 0.999972] [G loss: 1.000064]\n",
      "523 [D loss: 0.999971] [G loss: 1.000067]\n",
      "524 [D loss: 0.999969] [G loss: 1.000064]\n",
      "525 [D loss: 0.999969] [G loss: 1.000067]\n",
      "526 [D loss: 0.999970] [G loss: 1.000063]\n",
      "527 [D loss: 0.999970] [G loss: 1.000063]\n",
      "528 [D loss: 0.999972] [G loss: 1.000065]\n",
      "529 [D loss: 0.999969] [G loss: 1.000069]\n",
      "530 [D loss: 0.999972] [G loss: 1.000065]\n",
      "531 [D loss: 0.999969] [G loss: 1.000064]\n",
      "532 [D loss: 0.999971] [G loss: 1.000065]\n",
      "533 [D loss: 0.999970] [G loss: 1.000064]\n",
      "534 [D loss: 0.999971] [G loss: 1.000064]\n",
      "535 [D loss: 0.999969] [G loss: 1.000065]\n",
      "536 [D loss: 0.999971] [G loss: 1.000069]\n",
      "537 [D loss: 0.999969] [G loss: 1.000069]\n",
      "538 [D loss: 0.999966] [G loss: 1.000066]\n",
      "539 [D loss: 0.999969] [G loss: 1.000066]\n",
      "540 [D loss: 0.999971] [G loss: 1.000062]\n",
      "541 [D loss: 0.999969] [G loss: 1.000066]\n",
      "542 [D loss: 0.999970] [G loss: 1.000067]\n",
      "543 [D loss: 0.999973] [G loss: 1.000065]\n",
      "544 [D loss: 0.999971] [G loss: 1.000066]\n",
      "545 [D loss: 0.999968] [G loss: 1.000066]\n",
      "546 [D loss: 0.999967] [G loss: 1.000065]\n",
      "547 [D loss: 0.999971] [G loss: 1.000065]\n",
      "548 [D loss: 0.999971] [G loss: 1.000063]\n",
      "549 [D loss: 0.999970] [G loss: 1.000065]\n",
      "550 [D loss: 0.999969] [G loss: 1.000065]\n",
      "551 [D loss: 0.999971] [G loss: 1.000065]\n",
      "552 [D loss: 0.999971] [G loss: 1.000068]\n",
      "553 [D loss: 0.999971] [G loss: 1.000062]\n",
      "554 [D loss: 0.999970] [G loss: 1.000066]\n",
      "555 [D loss: 0.999968] [G loss: 1.000064]\n",
      "556 [D loss: 0.999971] [G loss: 1.000065]\n",
      "557 [D loss: 0.999973] [G loss: 1.000067]\n",
      "558 [D loss: 0.999970] [G loss: 1.000064]\n",
      "559 [D loss: 0.999970] [G loss: 1.000066]\n",
      "560 [D loss: 0.999968] [G loss: 1.000065]\n",
      "561 [D loss: 0.999971] [G loss: 1.000066]\n",
      "562 [D loss: 0.999971] [G loss: 1.000066]\n",
      "563 [D loss: 0.999970] [G loss: 1.000066]\n",
      "564 [D loss: 0.999970] [G loss: 1.000065]\n",
      "565 [D loss: 0.999970] [G loss: 1.000065]\n",
      "566 [D loss: 0.999969] [G loss: 1.000066]\n",
      "567 [D loss: 0.999971] [G loss: 1.000061]\n",
      "568 [D loss: 0.999971] [G loss: 1.000066]\n",
      "569 [D loss: 0.999972] [G loss: 1.000063]\n",
      "570 [D loss: 0.999971] [G loss: 1.000066]\n",
      "571 [D loss: 0.999970] [G loss: 1.000063]\n",
      "572 [D loss: 0.999967] [G loss: 1.000066]\n",
      "573 [D loss: 0.999975] [G loss: 1.000066]\n",
      "574 [D loss: 0.999971] [G loss: 1.000071]\n",
      "575 [D loss: 0.999971] [G loss: 1.000066]\n",
      "576 [D loss: 0.999970] [G loss: 1.000067]\n",
      "577 [D loss: 0.999972] [G loss: 1.000065]\n",
      "578 [D loss: 0.999970] [G loss: 1.000067]\n",
      "579 [D loss: 0.999970] [G loss: 1.000065]\n",
      "580 [D loss: 0.999972] [G loss: 1.000067]\n",
      "581 [D loss: 0.999972] [G loss: 1.000065]\n",
      "582 [D loss: 0.999969] [G loss: 1.000067]\n",
      "583 [D loss: 0.999971] [G loss: 1.000067]\n",
      "584 [D loss: 0.999971] [G loss: 1.000067]\n",
      "585 [D loss: 0.999972] [G loss: 1.000064]\n",
      "586 [D loss: 0.999970] [G loss: 1.000065]\n",
      "587 [D loss: 0.999967] [G loss: 1.000069]\n",
      "588 [D loss: 0.999968] [G loss: 1.000065]\n",
      "589 [D loss: 0.999969] [G loss: 1.000066]\n",
      "590 [D loss: 0.999971] [G loss: 1.000064]\n",
      "591 [D loss: 0.999973] [G loss: 1.000066]\n",
      "592 [D loss: 0.999969] [G loss: 1.000064]\n",
      "593 [D loss: 0.999970] [G loss: 1.000064]\n",
      "594 [D loss: 0.999970] [G loss: 1.000065]\n",
      "595 [D loss: 0.999970] [G loss: 1.000065]\n",
      "596 [D loss: 0.999971] [G loss: 1.000068]\n",
      "597 [D loss: 0.999971] [G loss: 1.000063]\n",
      "598 [D loss: 0.999972] [G loss: 1.000067]\n",
      "599 [D loss: 0.999968] [G loss: 1.000064]\n",
      "600 [D loss: 0.999970] [G loss: 1.000064]\n",
      "601 [D loss: 0.999970] [G loss: 1.000061]\n",
      "602 [D loss: 0.999970] [G loss: 1.000068]\n",
      "603 [D loss: 0.999970] [G loss: 1.000069]\n",
      "604 [D loss: 0.999971] [G loss: 1.000061]\n",
      "605 [D loss: 0.999971] [G loss: 1.000061]\n",
      "606 [D loss: 0.999969] [G loss: 1.000063]\n",
      "607 [D loss: 0.999970] [G loss: 1.000065]\n",
      "608 [D loss: 0.999971] [G loss: 1.000062]\n",
      "609 [D loss: 0.999970] [G loss: 1.000065]\n",
      "610 [D loss: 0.999969] [G loss: 1.000068]\n",
      "611 [D loss: 0.999972] [G loss: 1.000065]\n",
      "612 [D loss: 0.999970] [G loss: 1.000064]\n",
      "613 [D loss: 0.999969] [G loss: 1.000063]\n",
      "614 [D loss: 0.999969] [G loss: 1.000066]\n",
      "615 [D loss: 0.999971] [G loss: 1.000065]\n",
      "616 [D loss: 0.999970] [G loss: 1.000066]\n",
      "617 [D loss: 0.999972] [G loss: 1.000065]\n",
      "618 [D loss: 0.999970] [G loss: 1.000067]\n",
      "619 [D loss: 0.999972] [G loss: 1.000068]\n",
      "620 [D loss: 0.999968] [G loss: 1.000066]\n",
      "621 [D loss: 0.999971] [G loss: 1.000062]\n",
      "622 [D loss: 0.999969] [G loss: 1.000065]\n",
      "623 [D loss: 0.999971] [G loss: 1.000065]\n",
      "624 [D loss: 0.999970] [G loss: 1.000063]\n",
      "625 [D loss: 0.999971] [G loss: 1.000066]\n",
      "626 [D loss: 0.999973] [G loss: 1.000063]\n",
      "627 [D loss: 0.999971] [G loss: 1.000064]\n",
      "628 [D loss: 0.999970] [G loss: 1.000066]\n",
      "629 [D loss: 0.999970] [G loss: 1.000066]\n",
      "630 [D loss: 0.999971] [G loss: 1.000066]\n",
      "631 [D loss: 0.999974] [G loss: 1.000064]\n",
      "632 [D loss: 0.999968] [G loss: 1.000066]\n",
      "633 [D loss: 0.999969] [G loss: 1.000068]\n",
      "634 [D loss: 0.999972] [G loss: 1.000068]\n",
      "635 [D loss: 0.999969] [G loss: 1.000063]\n",
      "636 [D loss: 0.999969] [G loss: 1.000069]\n",
      "637 [D loss: 0.999971] [G loss: 1.000064]\n",
      "638 [D loss: 0.999970] [G loss: 1.000065]\n",
      "639 [D loss: 0.999969] [G loss: 1.000063]\n",
      "640 [D loss: 0.999969] [G loss: 1.000061]\n",
      "641 [D loss: 0.999970] [G loss: 1.000067]\n",
      "642 [D loss: 0.999970] [G loss: 1.000066]\n",
      "643 [D loss: 0.999970] [G loss: 1.000066]\n",
      "644 [D loss: 0.999971] [G loss: 1.000064]\n",
      "645 [D loss: 0.999970] [G loss: 1.000063]\n",
      "646 [D loss: 0.999971] [G loss: 1.000065]\n",
      "647 [D loss: 0.999971] [G loss: 1.000069]\n",
      "648 [D loss: 0.999971] [G loss: 1.000067]\n",
      "649 [D loss: 0.999969] [G loss: 1.000066]\n",
      "650 [D loss: 0.999971] [G loss: 1.000066]\n",
      "651 [D loss: 0.999969] [G loss: 1.000070]\n",
      "652 [D loss: 0.999971] [G loss: 1.000067]\n",
      "653 [D loss: 0.999971] [G loss: 1.000065]\n",
      "654 [D loss: 0.999970] [G loss: 1.000068]\n",
      "655 [D loss: 0.999969] [G loss: 1.000064]\n",
      "656 [D loss: 0.999970] [G loss: 1.000064]\n",
      "657 [D loss: 0.999970] [G loss: 1.000067]\n",
      "658 [D loss: 0.999972] [G loss: 1.000064]\n",
      "659 [D loss: 0.999969] [G loss: 1.000059]\n",
      "660 [D loss: 0.999971] [G loss: 1.000068]\n",
      "661 [D loss: 0.999970] [G loss: 1.000065]\n",
      "662 [D loss: 0.999971] [G loss: 1.000065]\n",
      "663 [D loss: 0.999972] [G loss: 1.000066]\n",
      "664 [D loss: 0.999970] [G loss: 1.000064]\n",
      "665 [D loss: 0.999971] [G loss: 1.000066]\n",
      "666 [D loss: 0.999972] [G loss: 1.000067]\n",
      "667 [D loss: 0.999970] [G loss: 1.000068]\n",
      "668 [D loss: 0.999971] [G loss: 1.000067]\n",
      "669 [D loss: 0.999971] [G loss: 1.000065]\n",
      "670 [D loss: 0.999970] [G loss: 1.000065]\n",
      "671 [D loss: 0.999972] [G loss: 1.000065]\n",
      "672 [D loss: 0.999970] [G loss: 1.000066]\n",
      "673 [D loss: 0.999969] [G loss: 1.000063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 [D loss: 0.999973] [G loss: 1.000065]\n",
      "675 [D loss: 0.999971] [G loss: 1.000062]\n",
      "676 [D loss: 0.999971] [G loss: 1.000067]\n",
      "677 [D loss: 0.999973] [G loss: 1.000067]\n",
      "678 [D loss: 0.999971] [G loss: 1.000068]\n",
      "679 [D loss: 0.999970] [G loss: 1.000066]\n",
      "680 [D loss: 0.999970] [G loss: 1.000067]\n",
      "681 [D loss: 0.999971] [G loss: 1.000065]\n",
      "682 [D loss: 0.999972] [G loss: 1.000066]\n",
      "683 [D loss: 0.999968] [G loss: 1.000064]\n",
      "684 [D loss: 0.999970] [G loss: 1.000066]\n",
      "685 [D loss: 0.999970] [G loss: 1.000067]\n",
      "686 [D loss: 0.999970] [G loss: 1.000065]\n",
      "687 [D loss: 0.999970] [G loss: 1.000065]\n",
      "688 [D loss: 0.999969] [G loss: 1.000065]\n",
      "689 [D loss: 0.999970] [G loss: 1.000065]\n",
      "690 [D loss: 0.999971] [G loss: 1.000065]\n",
      "691 [D loss: 0.999969] [G loss: 1.000067]\n",
      "692 [D loss: 0.999969] [G loss: 1.000066]\n",
      "693 [D loss: 0.999970] [G loss: 1.000065]\n",
      "694 [D loss: 0.999972] [G loss: 1.000066]\n",
      "695 [D loss: 0.999970] [G loss: 1.000068]\n",
      "696 [D loss: 0.999970] [G loss: 1.000070]\n",
      "697 [D loss: 0.999968] [G loss: 1.000067]\n",
      "698 [D loss: 0.999969] [G loss: 1.000067]\n",
      "699 [D loss: 0.999970] [G loss: 1.000064]\n",
      "700 [D loss: 0.999970] [G loss: 1.000064]\n",
      "701 [D loss: 0.999972] [G loss: 1.000066]\n",
      "702 [D loss: 0.999970] [G loss: 1.000068]\n",
      "703 [D loss: 0.999972] [G loss: 1.000067]\n",
      "704 [D loss: 0.999972] [G loss: 1.000064]\n",
      "705 [D loss: 0.999971] [G loss: 1.000065]\n",
      "706 [D loss: 0.999970] [G loss: 1.000066]\n",
      "707 [D loss: 0.999972] [G loss: 1.000066]\n",
      "708 [D loss: 0.999971] [G loss: 1.000066]\n",
      "709 [D loss: 0.999972] [G loss: 1.000067]\n",
      "710 [D loss: 0.999970] [G loss: 1.000069]\n",
      "711 [D loss: 0.999971] [G loss: 1.000067]\n",
      "712 [D loss: 0.999971] [G loss: 1.000067]\n",
      "713 [D loss: 0.999970] [G loss: 1.000066]\n",
      "714 [D loss: 0.999971] [G loss: 1.000065]\n",
      "715 [D loss: 0.999972] [G loss: 1.000068]\n",
      "716 [D loss: 0.999969] [G loss: 1.000065]\n",
      "717 [D loss: 0.999972] [G loss: 1.000065]\n",
      "718 [D loss: 0.999972] [G loss: 1.000064]\n",
      "719 [D loss: 0.999973] [G loss: 1.000063]\n",
      "720 [D loss: 0.999969] [G loss: 1.000064]\n",
      "721 [D loss: 0.999967] [G loss: 1.000066]\n",
      "722 [D loss: 0.999972] [G loss: 1.000070]\n",
      "723 [D loss: 0.999970] [G loss: 1.000064]\n",
      "724 [D loss: 0.999969] [G loss: 1.000063]\n",
      "725 [D loss: 0.999970] [G loss: 1.000064]\n",
      "726 [D loss: 0.999972] [G loss: 1.000063]\n",
      "727 [D loss: 0.999969] [G loss: 1.000061]\n",
      "728 [D loss: 0.999966] [G loss: 1.000065]\n",
      "729 [D loss: 0.999972] [G loss: 1.000067]\n",
      "730 [D loss: 0.999966] [G loss: 1.000066]\n",
      "731 [D loss: 0.999968] [G loss: 1.000061]\n",
      "732 [D loss: 0.999969] [G loss: 1.000067]\n",
      "733 [D loss: 0.999967] [G loss: 1.000066]\n",
      "734 [D loss: 0.999971] [G loss: 1.000068]\n",
      "735 [D loss: 0.999969] [G loss: 1.000066]\n",
      "736 [D loss: 0.999970] [G loss: 1.000065]\n",
      "737 [D loss: 0.999970] [G loss: 1.000066]\n",
      "738 [D loss: 0.999970] [G loss: 1.000066]\n",
      "739 [D loss: 0.999971] [G loss: 1.000064]\n",
      "740 [D loss: 0.999969] [G loss: 1.000065]\n",
      "741 [D loss: 0.999973] [G loss: 1.000064]\n",
      "742 [D loss: 0.999971] [G loss: 1.000062]\n",
      "743 [D loss: 0.999971] [G loss: 1.000065]\n",
      "744 [D loss: 0.999970] [G loss: 1.000065]\n",
      "745 [D loss: 0.999973] [G loss: 1.000065]\n",
      "746 [D loss: 0.999972] [G loss: 1.000066]\n",
      "747 [D loss: 0.999972] [G loss: 1.000065]\n",
      "748 [D loss: 0.999970] [G loss: 1.000067]\n",
      "749 [D loss: 0.999970] [G loss: 1.000066]\n",
      "750 [D loss: 0.999968] [G loss: 1.000064]\n",
      "751 [D loss: 0.999971] [G loss: 1.000065]\n",
      "752 [D loss: 0.999969] [G loss: 1.000064]\n",
      "753 [D loss: 0.999972] [G loss: 1.000067]\n",
      "754 [D loss: 0.999968] [G loss: 1.000066]\n",
      "755 [D loss: 0.999971] [G loss: 1.000065]\n",
      "756 [D loss: 0.999969] [G loss: 1.000069]\n",
      "757 [D loss: 0.999972] [G loss: 1.000066]\n",
      "758 [D loss: 0.999971] [G loss: 1.000069]\n",
      "759 [D loss: 0.999972] [G loss: 1.000066]\n",
      "760 [D loss: 0.999969] [G loss: 1.000065]\n",
      "761 [D loss: 0.999969] [G loss: 1.000068]\n",
      "762 [D loss: 0.999971] [G loss: 1.000068]\n",
      "763 [D loss: 0.999971] [G loss: 1.000067]\n",
      "764 [D loss: 0.999972] [G loss: 1.000066]\n",
      "765 [D loss: 0.999972] [G loss: 1.000063]\n",
      "766 [D loss: 0.999973] [G loss: 1.000065]\n",
      "767 [D loss: 0.999970] [G loss: 1.000062]\n",
      "768 [D loss: 0.999971] [G loss: 1.000065]\n",
      "769 [D loss: 0.999970] [G loss: 1.000064]\n",
      "770 [D loss: 0.999970] [G loss: 1.000065]\n",
      "771 [D loss: 0.999973] [G loss: 1.000067]\n",
      "772 [D loss: 0.999973] [G loss: 1.000061]\n",
      "773 [D loss: 0.999971] [G loss: 1.000064]\n",
      "774 [D loss: 0.999969] [G loss: 1.000067]\n",
      "775 [D loss: 0.999970] [G loss: 1.000064]\n",
      "776 [D loss: 0.999971] [G loss: 1.000062]\n",
      "777 [D loss: 0.999970] [G loss: 1.000064]\n",
      "778 [D loss: 0.999972] [G loss: 1.000065]\n",
      "779 [D loss: 0.999969] [G loss: 1.000063]\n",
      "780 [D loss: 0.999973] [G loss: 1.000064]\n",
      "781 [D loss: 0.999970] [G loss: 1.000063]\n",
      "782 [D loss: 0.999971] [G loss: 1.000063]\n",
      "783 [D loss: 0.999969] [G loss: 1.000061]\n",
      "784 [D loss: 0.999970] [G loss: 1.000060]\n",
      "785 [D loss: 0.999970] [G loss: 1.000065]\n",
      "786 [D loss: 0.999969] [G loss: 1.000066]\n",
      "787 [D loss: 0.999970] [G loss: 1.000064]\n",
      "788 [D loss: 0.999971] [G loss: 1.000063]\n",
      "789 [D loss: 0.999971] [G loss: 1.000062]\n",
      "790 [D loss: 0.999969] [G loss: 1.000065]\n",
      "791 [D loss: 0.999969] [G loss: 1.000064]\n",
      "792 [D loss: 0.999971] [G loss: 1.000065]\n",
      "793 [D loss: 0.999971] [G loss: 1.000063]\n",
      "794 [D loss: 0.999970] [G loss: 1.000063]\n",
      "795 [D loss: 0.999971] [G loss: 1.000063]\n",
      "796 [D loss: 0.999969] [G loss: 1.000068]\n",
      "797 [D loss: 0.999971] [G loss: 1.000066]\n",
      "798 [D loss: 0.999969] [G loss: 1.000066]\n",
      "799 [D loss: 0.999972] [G loss: 1.000066]\n",
      "800 [D loss: 0.999970] [G loss: 1.000066]\n",
      "801 [D loss: 0.999972] [G loss: 1.000065]\n",
      "802 [D loss: 0.999971] [G loss: 1.000067]\n",
      "803 [D loss: 0.999969] [G loss: 1.000065]\n",
      "804 [D loss: 0.999971] [G loss: 1.000066]\n",
      "805 [D loss: 0.999969] [G loss: 1.000064]\n",
      "806 [D loss: 0.999972] [G loss: 1.000064]\n",
      "807 [D loss: 0.999970] [G loss: 1.000066]\n",
      "808 [D loss: 0.999969] [G loss: 1.000066]\n",
      "809 [D loss: 0.999971] [G loss: 1.000063]\n",
      "810 [D loss: 0.999971] [G loss: 1.000065]\n",
      "811 [D loss: 0.999972] [G loss: 1.000064]\n",
      "812 [D loss: 0.999971] [G loss: 1.000066]\n",
      "813 [D loss: 0.999969] [G loss: 1.000066]\n",
      "814 [D loss: 0.999971] [G loss: 1.000066]\n",
      "815 [D loss: 0.999970] [G loss: 1.000067]\n",
      "816 [D loss: 0.999970] [G loss: 1.000066]\n",
      "817 [D loss: 0.999970] [G loss: 1.000066]\n",
      "818 [D loss: 0.999970] [G loss: 1.000066]\n",
      "819 [D loss: 0.999970] [G loss: 1.000065]\n",
      "820 [D loss: 0.999971] [G loss: 1.000065]\n",
      "821 [D loss: 0.999970] [G loss: 1.000065]\n",
      "822 [D loss: 0.999972] [G loss: 1.000065]\n",
      "823 [D loss: 0.999970] [G loss: 1.000068]\n",
      "824 [D loss: 0.999968] [G loss: 1.000066]\n",
      "825 [D loss: 0.999971] [G loss: 1.000064]\n",
      "826 [D loss: 0.999973] [G loss: 1.000065]\n",
      "827 [D loss: 0.999969] [G loss: 1.000063]\n",
      "828 [D loss: 0.999969] [G loss: 1.000064]\n",
      "829 [D loss: 0.999970] [G loss: 1.000063]\n",
      "830 [D loss: 0.999970] [G loss: 1.000062]\n",
      "831 [D loss: 0.999969] [G loss: 1.000066]\n",
      "832 [D loss: 0.999969] [G loss: 1.000064]\n",
      "833 [D loss: 0.999969] [G loss: 1.000066]\n",
      "834 [D loss: 0.999973] [G loss: 1.000065]\n",
      "835 [D loss: 0.999971] [G loss: 1.000065]\n",
      "836 [D loss: 0.999970] [G loss: 1.000064]\n",
      "837 [D loss: 0.999969] [G loss: 1.000067]\n",
      "838 [D loss: 0.999969] [G loss: 1.000063]\n",
      "839 [D loss: 0.999969] [G loss: 1.000064]\n",
      "840 [D loss: 0.999973] [G loss: 1.000063]\n",
      "841 [D loss: 0.999971] [G loss: 1.000065]\n",
      "842 [D loss: 0.999970] [G loss: 1.000065]\n",
      "843 [D loss: 0.999970] [G loss: 1.000066]\n",
      "844 [D loss: 0.999971] [G loss: 1.000063]\n",
      "845 [D loss: 0.999972] [G loss: 1.000066]\n",
      "846 [D loss: 0.999969] [G loss: 1.000066]\n",
      "847 [D loss: 0.999970] [G loss: 1.000068]\n",
      "848 [D loss: 0.999970] [G loss: 1.000067]\n",
      "849 [D loss: 0.999970] [G loss: 1.000067]\n",
      "850 [D loss: 0.999972] [G loss: 1.000066]\n",
      "851 [D loss: 0.999971] [G loss: 1.000067]\n",
      "852 [D loss: 0.999970] [G loss: 1.000065]\n",
      "853 [D loss: 0.999971] [G loss: 1.000066]\n",
      "854 [D loss: 0.999971] [G loss: 1.000064]\n",
      "855 [D loss: 0.999971] [G loss: 1.000063]\n",
      "856 [D loss: 0.999971] [G loss: 1.000068]\n",
      "857 [D loss: 0.999969] [G loss: 1.000064]\n",
      "858 [D loss: 0.999971] [G loss: 1.000065]\n",
      "859 [D loss: 0.999970] [G loss: 1.000065]\n",
      "860 [D loss: 0.999972] [G loss: 1.000062]\n",
      "861 [D loss: 0.999970] [G loss: 1.000068]\n",
      "862 [D loss: 0.999970] [G loss: 1.000068]\n",
      "863 [D loss: 0.999971] [G loss: 1.000062]\n",
      "864 [D loss: 0.999971] [G loss: 1.000066]\n",
      "865 [D loss: 0.999970] [G loss: 1.000068]\n",
      "866 [D loss: 0.999970] [G loss: 1.000065]\n",
      "867 [D loss: 0.999971] [G loss: 1.000064]\n",
      "868 [D loss: 0.999971] [G loss: 1.000066]\n",
      "869 [D loss: 0.999970] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870 [D loss: 0.999971] [G loss: 1.000067]\n",
      "871 [D loss: 0.999972] [G loss: 1.000065]\n",
      "872 [D loss: 0.999969] [G loss: 1.000066]\n",
      "873 [D loss: 0.999970] [G loss: 1.000064]\n",
      "874 [D loss: 0.999971] [G loss: 1.000064]\n",
      "875 [D loss: 0.999969] [G loss: 1.000064]\n",
      "876 [D loss: 0.999970] [G loss: 1.000064]\n",
      "877 [D loss: 0.999969] [G loss: 1.000066]\n",
      "878 [D loss: 0.999969] [G loss: 1.000065]\n",
      "879 [D loss: 0.999969] [G loss: 1.000065]\n",
      "880 [D loss: 0.999970] [G loss: 1.000065]\n",
      "881 [D loss: 0.999971] [G loss: 1.000064]\n",
      "882 [D loss: 0.999973] [G loss: 1.000064]\n",
      "883 [D loss: 0.999968] [G loss: 1.000068]\n",
      "884 [D loss: 0.999971] [G loss: 1.000068]\n",
      "885 [D loss: 0.999969] [G loss: 1.000064]\n",
      "886 [D loss: 0.999969] [G loss: 1.000068]\n",
      "887 [D loss: 0.999970] [G loss: 1.000068]\n",
      "888 [D loss: 0.999971] [G loss: 1.000066]\n",
      "889 [D loss: 0.999972] [G loss: 1.000067]\n",
      "890 [D loss: 0.999970] [G loss: 1.000067]\n",
      "891 [D loss: 0.999971] [G loss: 1.000068]\n",
      "892 [D loss: 0.999968] [G loss: 1.000066]\n",
      "893 [D loss: 0.999970] [G loss: 1.000065]\n",
      "894 [D loss: 0.999969] [G loss: 1.000066]\n",
      "895 [D loss: 0.999972] [G loss: 1.000065]\n",
      "896 [D loss: 0.999972] [G loss: 1.000066]\n",
      "897 [D loss: 0.999971] [G loss: 1.000063]\n",
      "898 [D loss: 0.999973] [G loss: 1.000066]\n",
      "899 [D loss: 0.999971] [G loss: 1.000065]\n",
      "900 [D loss: 0.999968] [G loss: 1.000067]\n",
      "901 [D loss: 0.999970] [G loss: 1.000064]\n",
      "902 [D loss: 0.999969] [G loss: 1.000065]\n",
      "903 [D loss: 0.999971] [G loss: 1.000067]\n",
      "904 [D loss: 0.999970] [G loss: 1.000064]\n",
      "905 [D loss: 0.999970] [G loss: 1.000067]\n",
      "906 [D loss: 0.999971] [G loss: 1.000065]\n",
      "907 [D loss: 0.999971] [G loss: 1.000067]\n",
      "908 [D loss: 0.999972] [G loss: 1.000065]\n",
      "909 [D loss: 0.999970] [G loss: 1.000069]\n",
      "910 [D loss: 0.999972] [G loss: 1.000066]\n",
      "911 [D loss: 0.999970] [G loss: 1.000068]\n",
      "912 [D loss: 0.999970] [G loss: 1.000069]\n",
      "913 [D loss: 0.999969] [G loss: 1.000068]\n",
      "914 [D loss: 0.999969] [G loss: 1.000065]\n",
      "915 [D loss: 0.999971] [G loss: 1.000068]\n",
      "916 [D loss: 0.999969] [G loss: 1.000067]\n",
      "917 [D loss: 0.999970] [G loss: 1.000065]\n",
      "918 [D loss: 0.999971] [G loss: 1.000066]\n",
      "919 [D loss: 0.999973] [G loss: 1.000063]\n",
      "920 [D loss: 0.999971] [G loss: 1.000065]\n",
      "921 [D loss: 0.999970] [G loss: 1.000066]\n",
      "922 [D loss: 0.999970] [G loss: 1.000066]\n",
      "923 [D loss: 0.999972] [G loss: 1.000063]\n",
      "924 [D loss: 0.999969] [G loss: 1.000062]\n",
      "925 [D loss: 0.999971] [G loss: 1.000069]\n",
      "926 [D loss: 0.999970] [G loss: 1.000066]\n",
      "927 [D loss: 0.999971] [G loss: 1.000065]\n",
      "928 [D loss: 0.999970] [G loss: 1.000063]\n",
      "929 [D loss: 0.999972] [G loss: 1.000066]\n",
      "930 [D loss: 0.999970] [G loss: 1.000064]\n",
      "931 [D loss: 0.999970] [G loss: 1.000063]\n",
      "932 [D loss: 0.999972] [G loss: 1.000063]\n",
      "933 [D loss: 0.999971] [G loss: 1.000064]\n",
      "934 [D loss: 0.999972] [G loss: 1.000062]\n",
      "935 [D loss: 0.999970] [G loss: 1.000062]\n",
      "936 [D loss: 0.999969] [G loss: 1.000064]\n",
      "937 [D loss: 0.999971] [G loss: 1.000065]\n",
      "938 [D loss: 0.999970] [G loss: 1.000065]\n",
      "939 [D loss: 0.999970] [G loss: 1.000067]\n",
      "940 [D loss: 0.999973] [G loss: 1.000064]\n",
      "941 [D loss: 0.999969] [G loss: 1.000067]\n",
      "942 [D loss: 0.999973] [G loss: 1.000062]\n",
      "943 [D loss: 0.999971] [G loss: 1.000063]\n",
      "944 [D loss: 0.999970] [G loss: 1.000064]\n",
      "945 [D loss: 0.999969] [G loss: 1.000067]\n",
      "946 [D loss: 0.999971] [G loss: 1.000066]\n",
      "947 [D loss: 0.999970] [G loss: 1.000065]\n",
      "948 [D loss: 0.999969] [G loss: 1.000067]\n",
      "949 [D loss: 0.999969] [G loss: 1.000065]\n",
      "950 [D loss: 0.999971] [G loss: 1.000063]\n",
      "951 [D loss: 0.999970] [G loss: 1.000066]\n",
      "952 [D loss: 0.999971] [G loss: 1.000063]\n",
      "953 [D loss: 0.999971] [G loss: 1.000066]\n",
      "954 [D loss: 0.999968] [G loss: 1.000065]\n",
      "955 [D loss: 0.999971] [G loss: 1.000065]\n",
      "956 [D loss: 0.999968] [G loss: 1.000064]\n",
      "957 [D loss: 0.999970] [G loss: 1.000065]\n",
      "958 [D loss: 0.999972] [G loss: 1.000063]\n",
      "959 [D loss: 0.999971] [G loss: 1.000066]\n",
      "960 [D loss: 0.999971] [G loss: 1.000064]\n",
      "961 [D loss: 0.999969] [G loss: 1.000066]\n",
      "962 [D loss: 0.999971] [G loss: 1.000067]\n",
      "963 [D loss: 0.999972] [G loss: 1.000066]\n",
      "964 [D loss: 0.999971] [G loss: 1.000064]\n",
      "965 [D loss: 0.999970] [G loss: 1.000064]\n",
      "966 [D loss: 0.999971] [G loss: 1.000064]\n",
      "967 [D loss: 0.999971] [G loss: 1.000063]\n",
      "968 [D loss: 0.999970] [G loss: 1.000068]\n",
      "969 [D loss: 0.999971] [G loss: 1.000065]\n",
      "970 [D loss: 0.999971] [G loss: 1.000064]\n",
      "971 [D loss: 0.999970] [G loss: 1.000066]\n",
      "972 [D loss: 0.999971] [G loss: 1.000063]\n",
      "973 [D loss: 0.999969] [G loss: 1.000065]\n",
      "974 [D loss: 0.999972] [G loss: 1.000065]\n",
      "975 [D loss: 0.999969] [G loss: 1.000066]\n",
      "976 [D loss: 0.999969] [G loss: 1.000066]\n",
      "977 [D loss: 0.999969] [G loss: 1.000069]\n",
      "978 [D loss: 0.999972] [G loss: 1.000066]\n",
      "979 [D loss: 0.999970] [G loss: 1.000066]\n",
      "980 [D loss: 0.999971] [G loss: 1.000064]\n",
      "981 [D loss: 0.999970] [G loss: 1.000067]\n",
      "982 [D loss: 0.999970] [G loss: 1.000062]\n",
      "983 [D loss: 0.999969] [G loss: 1.000065]\n",
      "984 [D loss: 0.999972] [G loss: 1.000067]\n",
      "985 [D loss: 0.999970] [G loss: 1.000067]\n",
      "986 [D loss: 0.999968] [G loss: 1.000064]\n",
      "987 [D loss: 0.999970] [G loss: 1.000065]\n",
      "988 [D loss: 0.999972] [G loss: 1.000063]\n",
      "989 [D loss: 0.999970] [G loss: 1.000066]\n",
      "990 [D loss: 0.999971] [G loss: 1.000064]\n",
      "991 [D loss: 0.999969] [G loss: 1.000062]\n",
      "992 [D loss: 0.999972] [G loss: 1.000067]\n",
      "993 [D loss: 0.999972] [G loss: 1.000067]\n",
      "994 [D loss: 0.999971] [G loss: 1.000066]\n",
      "995 [D loss: 0.999968] [G loss: 1.000066]\n",
      "996 [D loss: 0.999969] [G loss: 1.000065]\n",
      "997 [D loss: 0.999970] [G loss: 1.000066]\n",
      "998 [D loss: 0.999968] [G loss: 1.000066]\n",
      "999 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1000 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1001 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1002 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1003 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1004 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1005 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1006 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1007 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1008 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1009 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1010 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1011 [D loss: 0.999973] [G loss: 1.000064]\n",
      "1012 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1013 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1014 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1015 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1016 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1017 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1018 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1019 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1020 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1021 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1022 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1023 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1024 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1025 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1026 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1027 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1028 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1029 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1030 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1031 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1032 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1033 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1034 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1035 [D loss: 0.999969] [G loss: 1.000070]\n",
      "1036 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1037 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1038 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1039 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1040 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1041 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1042 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1043 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1044 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1045 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1046 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1047 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1048 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1049 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1050 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1051 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1052 [D loss: 0.999968] [G loss: 1.000062]\n",
      "1053 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1054 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1055 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1056 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1057 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1058 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1059 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1060 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1061 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1062 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1063 [D loss: 0.999973] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1065 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1066 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1067 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1068 [D loss: 0.999968] [G loss: 1.000059]\n",
      "1069 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1070 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1071 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1072 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1073 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1074 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1075 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1076 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1077 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1078 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1079 [D loss: 0.999972] [G loss: 1.000068]\n",
      "1080 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1081 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1082 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1083 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1084 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1085 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1086 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1087 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1088 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1089 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1090 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1091 [D loss: 0.999973] [G loss: 1.000061]\n",
      "1092 [D loss: 0.999972] [G loss: 1.000060]\n",
      "1093 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1094 [D loss: 0.999972] [G loss: 1.000069]\n",
      "1095 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1096 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1097 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1098 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1099 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1100 [D loss: 0.999969] [G loss: 1.000060]\n",
      "1101 [D loss: 0.999975] [G loss: 1.000065]\n",
      "1102 [D loss: 0.999972] [G loss: 1.000070]\n",
      "1103 [D loss: 0.999970] [G loss: 1.000070]\n",
      "1104 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1105 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1106 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1107 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1108 [D loss: 0.999971] [G loss: 1.000071]\n",
      "1109 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1110 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1111 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1112 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1113 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1114 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1115 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1116 [D loss: 0.999971] [G loss: 1.000070]\n",
      "1117 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1118 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1119 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1120 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1121 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1122 [D loss: 0.999968] [G loss: 1.000068]\n",
      "1123 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1124 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1125 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1126 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1127 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1128 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1129 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1130 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1131 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1132 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1133 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1134 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1135 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1136 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1137 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1138 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1139 [D loss: 0.999973] [G loss: 1.000064]\n",
      "1140 [D loss: 0.999968] [G loss: 1.000067]\n",
      "1141 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1142 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1143 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1144 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1145 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1146 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1147 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1148 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1149 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1150 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1151 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1152 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1153 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1154 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1155 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1156 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1157 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1158 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1159 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1160 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1161 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1162 [D loss: 0.999973] [G loss: 1.000061]\n",
      "1163 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1164 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1165 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1166 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1167 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1168 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1169 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1170 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1171 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1172 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1173 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1174 [D loss: 0.999973] [G loss: 1.000063]\n",
      "1175 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1176 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1177 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1178 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1179 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1180 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1181 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1182 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1183 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1184 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1185 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1186 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1187 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1188 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1189 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1190 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1191 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1192 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1193 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1194 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1195 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1196 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1197 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1198 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1199 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1200 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1201 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1202 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1203 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1204 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1205 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1206 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1207 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1208 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1209 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1210 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1211 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1212 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1213 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1214 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1215 [D loss: 0.999967] [G loss: 1.000062]\n",
      "1216 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1217 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1218 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1219 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1220 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1221 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1222 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1223 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1224 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1225 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1226 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1227 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1228 [D loss: 0.999973] [G loss: 1.000063]\n",
      "1229 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1230 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1231 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1232 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1233 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1234 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1235 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1236 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1237 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1238 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1239 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1240 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1241 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1242 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1243 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1244 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1245 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1246 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1247 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1248 [D loss: 0.999968] [G loss: 1.000063]\n",
      "1249 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1250 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1251 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1252 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1253 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1254 [D loss: 0.999970] [G loss: 1.000064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1255 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1256 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1257 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1258 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1259 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1260 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1261 [D loss: 0.999970] [G loss: 1.000060]\n",
      "1262 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1263 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1264 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1265 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1266 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1267 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1268 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1269 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1270 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1271 [D loss: 0.999974] [G loss: 1.000066]\n",
      "1272 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1273 [D loss: 0.999969] [G loss: 1.000061]\n",
      "1274 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1275 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1276 [D loss: 0.999972] [G loss: 1.000069]\n",
      "1277 [D loss: 0.999970] [G loss: 1.000060]\n",
      "1278 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1279 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1280 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1281 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1282 [D loss: 0.999973] [G loss: 1.000060]\n",
      "1283 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1284 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1285 [D loss: 0.999969] [G loss: 1.000070]\n",
      "1286 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1287 [D loss: 0.999969] [G loss: 1.000061]\n",
      "1288 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1289 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1290 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1291 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1292 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1293 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1294 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1295 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1296 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1297 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1298 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1299 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1300 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1301 [D loss: 0.999967] [G loss: 1.000063]\n",
      "1302 [D loss: 0.999968] [G loss: 1.000067]\n",
      "1303 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1304 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1305 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1306 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1307 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1308 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1309 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1310 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1311 [D loss: 0.999967] [G loss: 1.000065]\n",
      "1312 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1313 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1314 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1315 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1316 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1317 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1318 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1319 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1320 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1321 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1322 [D loss: 0.999967] [G loss: 1.000067]\n",
      "1323 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1324 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1325 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1326 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1327 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1328 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1329 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1330 [D loss: 0.999970] [G loss: 1.000060]\n",
      "1331 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1332 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1333 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1334 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1335 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1336 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1337 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1338 [D loss: 0.999974] [G loss: 1.000064]\n",
      "1339 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1340 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1341 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1342 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1343 [D loss: 0.999968] [G loss: 1.000063]\n",
      "1344 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1345 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1346 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1347 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1348 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1349 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1350 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1351 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1352 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1353 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1354 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1355 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1356 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1357 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1358 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1359 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1360 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1361 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1362 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1363 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1364 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1365 [D loss: 0.999972] [G loss: 1.000060]\n",
      "1366 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1367 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1368 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1369 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1370 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1371 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1372 [D loss: 0.999973] [G loss: 1.000069]\n",
      "1373 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1374 [D loss: 0.999968] [G loss: 1.000061]\n",
      "1375 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1376 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1377 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1378 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1379 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1380 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1381 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1382 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1383 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1384 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1385 [D loss: 0.999967] [G loss: 1.000064]\n",
      "1386 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1387 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1388 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1389 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1390 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1391 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1392 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1393 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1394 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1395 [D loss: 0.999972] [G loss: 1.000069]\n",
      "1396 [D loss: 0.999971] [G loss: 1.000060]\n",
      "1397 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1398 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1399 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1400 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1401 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1402 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1403 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1404 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1405 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1406 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1407 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1408 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1409 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1410 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1411 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1412 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1413 [D loss: 0.999967] [G loss: 1.000061]\n",
      "1414 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1415 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1416 [D loss: 0.999974] [G loss: 1.000062]\n",
      "1417 [D loss: 0.999970] [G loss: 1.000060]\n",
      "1418 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1419 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1420 [D loss: 0.999973] [G loss: 1.000060]\n",
      "1421 [D loss: 0.999975] [G loss: 1.000061]\n",
      "1422 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1423 [D loss: 0.999976] [G loss: 1.000061]\n",
      "1424 [D loss: 0.999975] [G loss: 1.000059]\n",
      "1425 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1426 [D loss: 0.999974] [G loss: 1.000062]\n",
      "1427 [D loss: 0.999974] [G loss: 1.000067]\n",
      "1428 [D loss: 0.999973] [G loss: 1.000068]\n",
      "1429 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1430 [D loss: 0.999974] [G loss: 1.000069]\n",
      "1431 [D loss: 0.999974] [G loss: 1.000066]\n",
      "1432 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1433 [D loss: 0.999977] [G loss: 1.000061]\n",
      "1434 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1435 [D loss: 0.999974] [G loss: 1.000073]\n",
      "1436 [D loss: 0.999973] [G loss: 1.000068]\n",
      "1437 [D loss: 0.999973] [G loss: 1.000069]\n",
      "1438 [D loss: 0.999972] [G loss: 1.000075]\n",
      "1439 [D loss: 0.999972] [G loss: 1.000071]\n",
      "1440 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1441 [D loss: 0.999976] [G loss: 1.000071]\n",
      "1442 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1443 [D loss: 0.999975] [G loss: 1.000075]\n",
      "1444 [D loss: 0.999973] [G loss: 1.000073]\n",
      "1445 [D loss: 0.999970] [G loss: 1.000075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446 [D loss: 0.999970] [G loss: 1.000074]\n",
      "1447 [D loss: 0.999970] [G loss: 1.000079]\n",
      "1448 [D loss: 0.999971] [G loss: 1.000078]\n",
      "1449 [D loss: 0.999968] [G loss: 1.000080]\n",
      "1450 [D loss: 0.999968] [G loss: 1.000082]\n",
      "1451 [D loss: 0.999972] [G loss: 1.000077]\n",
      "1452 [D loss: 0.999973] [G loss: 1.000074]\n",
      "1453 [D loss: 0.999967] [G loss: 1.000078]\n",
      "1454 [D loss: 0.999970] [G loss: 1.000079]\n",
      "1455 [D loss: 0.999968] [G loss: 1.000083]\n",
      "1456 [D loss: 0.999969] [G loss: 1.000077]\n",
      "1457 [D loss: 0.999971] [G loss: 1.000081]\n",
      "1458 [D loss: 0.999970] [G loss: 1.000076]\n",
      "1459 [D loss: 0.999968] [G loss: 1.000075]\n",
      "1460 [D loss: 0.999970] [G loss: 1.000081]\n",
      "1461 [D loss: 0.999969] [G loss: 1.000078]\n",
      "1462 [D loss: 0.999971] [G loss: 1.000078]\n",
      "1463 [D loss: 0.999972] [G loss: 1.000079]\n",
      "1464 [D loss: 0.999972] [G loss: 1.000080]\n",
      "1465 [D loss: 0.999974] [G loss: 1.000080]\n",
      "1466 [D loss: 0.999970] [G loss: 1.000077]\n",
      "1467 [D loss: 0.999967] [G loss: 1.000079]\n",
      "1468 [D loss: 0.999969] [G loss: 1.000080]\n",
      "1469 [D loss: 0.999968] [G loss: 1.000078]\n",
      "1470 [D loss: 0.999968] [G loss: 1.000079]\n",
      "1471 [D loss: 0.999970] [G loss: 1.000076]\n",
      "1472 [D loss: 0.999970] [G loss: 1.000079]\n",
      "1473 [D loss: 0.999972] [G loss: 1.000075]\n",
      "1474 [D loss: 0.999972] [G loss: 1.000077]\n",
      "1475 [D loss: 0.999970] [G loss: 1.000076]\n",
      "1476 [D loss: 0.999969] [G loss: 1.000079]\n",
      "1477 [D loss: 0.999972] [G loss: 1.000075]\n",
      "1478 [D loss: 0.999969] [G loss: 1.000072]\n",
      "1479 [D loss: 0.999973] [G loss: 1.000070]\n",
      "1480 [D loss: 0.999971] [G loss: 1.000076]\n",
      "1481 [D loss: 0.999969] [G loss: 1.000077]\n",
      "1482 [D loss: 0.999970] [G loss: 1.000073]\n",
      "1483 [D loss: 0.999966] [G loss: 1.000074]\n",
      "1484 [D loss: 0.999968] [G loss: 1.000078]\n",
      "1485 [D loss: 0.999970] [G loss: 1.000074]\n",
      "1486 [D loss: 0.999970] [G loss: 1.000072]\n",
      "1487 [D loss: 0.999967] [G loss: 1.000075]\n",
      "1488 [D loss: 0.999967] [G loss: 1.000074]\n",
      "1489 [D loss: 0.999970] [G loss: 1.000074]\n",
      "1490 [D loss: 0.999969] [G loss: 1.000071]\n",
      "1491 [D loss: 0.999970] [G loss: 1.000072]\n",
      "1492 [D loss: 0.999968] [G loss: 1.000073]\n",
      "1493 [D loss: 0.999969] [G loss: 1.000074]\n",
      "1494 [D loss: 0.999967] [G loss: 1.000073]\n",
      "1495 [D loss: 0.999967] [G loss: 1.000072]\n",
      "1496 [D loss: 0.999968] [G loss: 1.000072]\n",
      "1497 [D loss: 0.999968] [G loss: 1.000069]\n",
      "1498 [D loss: 0.999969] [G loss: 1.000075]\n",
      "1499 [D loss: 0.999972] [G loss: 1.000071]\n",
      "1500 [D loss: 0.999968] [G loss: 1.000067]\n",
      "1501 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1502 [D loss: 0.999970] [G loss: 1.000070]\n",
      "1503 [D loss: 0.999968] [G loss: 1.000069]\n",
      "1504 [D loss: 0.999968] [G loss: 1.000071]\n",
      "1505 [D loss: 0.999968] [G loss: 1.000070]\n",
      "1506 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1507 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1508 [D loss: 0.999968] [G loss: 1.000069]\n",
      "1509 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1510 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1511 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1512 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1513 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1514 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1515 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1516 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1517 [D loss: 0.999968] [G loss: 1.000069]\n",
      "1518 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1519 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1520 [D loss: 0.999968] [G loss: 1.000067]\n",
      "1521 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1522 [D loss: 0.999970] [G loss: 1.000071]\n",
      "1523 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1524 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1525 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1526 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1527 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1528 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1529 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1530 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1531 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1532 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1533 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1534 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1535 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1536 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1537 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1538 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1539 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1540 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1541 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1542 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1543 [D loss: 0.999967] [G loss: 1.000065]\n",
      "1544 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1545 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1546 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1547 [D loss: 0.999968] [G loss: 1.000068]\n",
      "1548 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1549 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1550 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1551 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1552 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1553 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1554 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1555 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1556 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1557 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1558 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1559 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1560 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1561 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1562 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1563 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1564 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1565 [D loss: 0.999968] [G loss: 1.000067]\n",
      "1566 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1567 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1568 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1569 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1570 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1571 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1572 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1573 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1574 [D loss: 0.999970] [G loss: 1.000070]\n",
      "1575 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1576 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1577 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1578 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1579 [D loss: 0.999972] [G loss: 1.000069]\n",
      "1580 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1581 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1582 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1583 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1584 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1585 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1586 [D loss: 0.999974] [G loss: 1.000061]\n",
      "1587 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1588 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1589 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1590 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1591 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1592 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1593 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1594 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1595 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1596 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1597 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1598 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1599 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1600 [D loss: 0.999967] [G loss: 1.000066]\n",
      "1601 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1602 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1603 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1604 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1605 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1606 [D loss: 0.999967] [G loss: 1.000063]\n",
      "1607 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1608 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1609 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1610 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1611 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1612 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1613 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1614 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1615 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1616 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1617 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1618 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1619 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1620 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1621 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1622 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1623 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1624 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1625 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1626 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1627 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1628 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1629 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1630 [D loss: 0.999974] [G loss: 1.000065]\n",
      "1631 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1632 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1633 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1634 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1635 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1636 [D loss: 0.999971] [G loss: 1.000067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1638 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1639 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1640 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1641 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1642 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1643 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1644 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1645 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1646 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1647 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1648 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1649 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1650 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1651 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1652 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1653 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1654 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1655 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1656 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1657 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1658 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1659 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1660 [D loss: 0.999966] [G loss: 1.000066]\n",
      "1661 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1662 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1663 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1664 [D loss: 0.999974] [G loss: 1.000064]\n",
      "1665 [D loss: 0.999968] [G loss: 1.000063]\n",
      "1666 [D loss: 0.999966] [G loss: 1.000064]\n",
      "1667 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1668 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1669 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1670 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1671 [D loss: 0.999968] [G loss: 1.000068]\n",
      "1672 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1673 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1674 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1675 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1676 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1677 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1678 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1679 [D loss: 0.999968] [G loss: 1.000069]\n",
      "1680 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1681 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1682 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1683 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1684 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1685 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1686 [D loss: 0.999967] [G loss: 1.000065]\n",
      "1687 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1688 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1689 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1690 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1691 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1692 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1693 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1694 [D loss: 0.999973] [G loss: 1.000063]\n",
      "1695 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1696 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1697 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1698 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1699 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1700 [D loss: 0.999972] [G loss: 1.000059]\n",
      "1701 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1702 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1703 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1704 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1705 [D loss: 0.999969] [G loss: 1.000068]\n",
      "1706 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1707 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1708 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1709 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1710 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1711 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1712 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1713 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1714 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1715 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1716 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1717 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1718 [D loss: 0.999968] [G loss: 1.000068]\n",
      "1719 [D loss: 0.999968] [G loss: 1.000068]\n",
      "1720 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1721 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1722 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1723 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1724 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1725 [D loss: 0.999967] [G loss: 1.000070]\n",
      "1726 [D loss: 0.999971] [G loss: 1.000070]\n",
      "1727 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1728 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1729 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1730 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1731 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1732 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1733 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1734 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1735 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1736 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1737 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1738 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1739 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1740 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1741 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1742 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1743 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1744 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1745 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1746 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1747 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1748 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1749 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1750 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1751 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1752 [D loss: 0.999971] [G loss: 1.000060]\n",
      "1753 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1754 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1755 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1756 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1757 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1758 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1759 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1760 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1761 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1762 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1763 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1764 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1765 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1766 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1767 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1768 [D loss: 0.999968] [G loss: 1.000067]\n",
      "1769 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1770 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1771 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1772 [D loss: 0.999968] [G loss: 1.000063]\n",
      "1773 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1774 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1775 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1776 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1777 [D loss: 0.999968] [G loss: 1.000062]\n",
      "1778 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1779 [D loss: 0.999973] [G loss: 1.000064]\n",
      "1780 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1781 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1782 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1783 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1784 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1785 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1786 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1787 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1788 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1789 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1790 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1791 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1792 [D loss: 0.999972] [G loss: 1.000060]\n",
      "1793 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1794 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1795 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1796 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1797 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1798 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1799 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1800 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1801 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1802 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1803 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1804 [D loss: 0.999968] [G loss: 1.000062]\n",
      "1805 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1806 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1807 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1808 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1809 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1810 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1811 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1812 [D loss: 0.999967] [G loss: 1.000063]\n",
      "1813 [D loss: 0.999974] [G loss: 1.000065]\n",
      "1814 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1815 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1816 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1817 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1818 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1819 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1820 [D loss: 0.999968] [G loss: 1.000062]\n",
      "1821 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1822 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1823 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1824 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1825 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1826 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1827 [D loss: 0.999970] [G loss: 1.000063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1829 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1830 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1831 [D loss: 0.999967] [G loss: 1.000066]\n",
      "1832 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1833 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1834 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1835 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1836 [D loss: 0.999974] [G loss: 1.000064]\n",
      "1837 [D loss: 0.999972] [G loss: 1.000068]\n",
      "1838 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1839 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1840 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1841 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1842 [D loss: 0.999966] [G loss: 1.000065]\n",
      "1843 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1844 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1845 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1846 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1847 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1848 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1849 [D loss: 0.999970] [G loss: 1.000059]\n",
      "1850 [D loss: 0.999968] [G loss: 1.000067]\n",
      "1851 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1852 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1853 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1854 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1855 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1856 [D loss: 0.999968] [G loss: 1.000061]\n",
      "1857 [D loss: 0.999968] [G loss: 1.000068]\n",
      "1858 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1859 [D loss: 0.999970] [G loss: 1.000060]\n",
      "1860 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1861 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1862 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1863 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1864 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1865 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1866 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1867 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1868 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1869 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1870 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1871 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1872 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1873 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1874 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1875 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1876 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1877 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1878 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1879 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1880 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1881 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1882 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1883 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1884 [D loss: 0.999967] [G loss: 1.000066]\n",
      "1885 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1886 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1887 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1888 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1889 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1890 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1891 [D loss: 0.999973] [G loss: 1.000064]\n",
      "1892 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1893 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1894 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1895 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1896 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1897 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1898 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1899 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1900 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1901 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1902 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1903 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1904 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1905 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1906 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1907 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1908 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1909 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1910 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1911 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1912 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1913 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1914 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1915 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1916 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1917 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1918 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1919 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1920 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1921 [D loss: 0.999972] [G loss: 1.000068]\n",
      "1922 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1923 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1924 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1925 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1926 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1927 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1928 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1929 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1930 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1931 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1932 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1933 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1934 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1935 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1936 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1937 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1938 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1939 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1940 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1941 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1942 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1943 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1944 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1945 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1946 [D loss: 0.999969] [G loss: 1.000058]\n",
      "1947 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1948 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1949 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1950 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1951 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1952 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1953 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1954 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1955 [D loss: 0.999972] [G loss: 1.000064]\n",
      "1956 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1957 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1958 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1959 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1960 [D loss: 0.999969] [G loss: 1.000065]\n",
      "1961 [D loss: 0.999976] [G loss: 1.000062]\n",
      "1962 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1963 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1964 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1965 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1966 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1967 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1968 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1969 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1970 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1971 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1972 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1973 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1974 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1975 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1976 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1977 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1978 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1979 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1980 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1981 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1982 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1983 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1984 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1985 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1986 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1987 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1988 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1989 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1990 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1991 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1992 [D loss: 0.999970] [G loss: 1.000069]\n",
      "1993 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1994 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1995 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1996 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1997 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1998 [D loss: 0.999967] [G loss: 1.000063]\n",
      "1999 [D loss: 0.999968] [G loss: 1.000068]\n",
      "2000 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2001 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2002 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2003 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2004 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2005 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2006 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2007 [D loss: 0.999968] [G loss: 1.000063]\n",
      "2008 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2009 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2010 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2011 [D loss: 0.999969] [G loss: 1.000062]\n",
      "2012 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2013 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2014 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2015 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2016 [D loss: 0.999967] [G loss: 1.000064]\n",
      "2017 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2018 [D loss: 0.999970] [G loss: 1.000064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2020 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2021 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2022 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2023 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2024 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2025 [D loss: 0.999968] [G loss: 1.000069]\n",
      "2026 [D loss: 0.999970] [G loss: 1.000059]\n",
      "2027 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2028 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2029 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2030 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2031 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2032 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2033 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2034 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2035 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2036 [D loss: 0.999973] [G loss: 1.000060]\n",
      "2037 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2038 [D loss: 0.999967] [G loss: 1.000068]\n",
      "2039 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2040 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2041 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2042 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2043 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2044 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2045 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2046 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2047 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2048 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2049 [D loss: 0.999970] [G loss: 1.000060]\n",
      "2050 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2051 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2052 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2053 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2054 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2055 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2056 [D loss: 0.999971] [G loss: 1.000069]\n",
      "2057 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2058 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2059 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2060 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2061 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2062 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2063 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2064 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2065 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2066 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2067 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2068 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2069 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2070 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2071 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2072 [D loss: 0.999968] [G loss: 1.000062]\n",
      "2073 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2074 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2075 [D loss: 0.999966] [G loss: 1.000066]\n",
      "2076 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2077 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2078 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2079 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2080 [D loss: 0.999975] [G loss: 1.000064]\n",
      "2081 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2082 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2083 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2084 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2085 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2086 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2087 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2088 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2089 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2090 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2091 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2092 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2093 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2094 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2095 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2096 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2097 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2098 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2099 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2100 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2101 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2102 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2103 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2104 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2105 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2106 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2107 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2108 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2109 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2110 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2111 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2112 [D loss: 0.999970] [G loss: 1.000060]\n",
      "2113 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2114 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2115 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2116 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2117 [D loss: 0.999969] [G loss: 1.000062]\n",
      "2118 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2119 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2120 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2121 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2122 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2123 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2124 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2125 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2126 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2127 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2128 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2129 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2130 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2131 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2132 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2133 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2134 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2135 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2136 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2137 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2138 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2139 [D loss: 0.999972] [G loss: 1.000069]\n",
      "2140 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2141 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2142 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2143 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2144 [D loss: 0.999966] [G loss: 1.000063]\n",
      "2145 [D loss: 0.999968] [G loss: 1.000068]\n",
      "2146 [D loss: 0.999967] [G loss: 1.000067]\n",
      "2147 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2148 [D loss: 0.999969] [G loss: 1.000059]\n",
      "2149 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2150 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2151 [D loss: 0.999973] [G loss: 1.000068]\n",
      "2152 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2153 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2154 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2155 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2156 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2157 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2158 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2159 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2160 [D loss: 0.999973] [G loss: 1.000061]\n",
      "2161 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2162 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2163 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2164 [D loss: 0.999967] [G loss: 1.000059]\n",
      "2165 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2166 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2167 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2168 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2169 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2170 [D loss: 0.999973] [G loss: 1.000064]\n",
      "2171 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2172 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2173 [D loss: 0.999969] [G loss: 1.000069]\n",
      "2174 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2175 [D loss: 0.999974] [G loss: 1.000066]\n",
      "2176 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2177 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2178 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2179 [D loss: 0.999970] [G loss: 1.000061]\n",
      "2180 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2181 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2182 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2183 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2184 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2185 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2186 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2187 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2188 [D loss: 0.999973] [G loss: 1.000062]\n",
      "2189 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2190 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2191 [D loss: 0.999973] [G loss: 1.000068]\n",
      "2192 [D loss: 0.999968] [G loss: 1.000061]\n",
      "2193 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2194 [D loss: 0.999973] [G loss: 1.000064]\n",
      "2195 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2196 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2197 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2198 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2199 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2200 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2201 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2202 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2203 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2204 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2205 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2206 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2207 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2208 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2209 [D loss: 0.999970] [G loss: 1.000063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2211 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2212 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2213 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2214 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2215 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2216 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2217 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2218 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2219 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2220 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2221 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2222 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2223 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2224 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2225 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2226 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2227 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2228 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2229 [D loss: 0.999967] [G loss: 1.000068]\n",
      "2230 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2231 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2232 [D loss: 0.999972] [G loss: 1.000069]\n",
      "2233 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2234 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2235 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2236 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2237 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2238 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2239 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2240 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2241 [D loss: 0.999966] [G loss: 1.000064]\n",
      "2242 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2243 [D loss: 0.999972] [G loss: 1.000069]\n",
      "2244 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2245 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2246 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2247 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2248 [D loss: 0.999969] [G loss: 1.000062]\n",
      "2249 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2250 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2251 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2252 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2253 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2254 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2255 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2256 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2257 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2258 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2259 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2260 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2261 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2262 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2263 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2264 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2265 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2266 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2267 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2268 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2269 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2270 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2271 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2272 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2273 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2274 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2275 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2276 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2277 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2278 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2279 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2280 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2281 [D loss: 0.999970] [G loss: 1.000060]\n",
      "2282 [D loss: 0.999969] [G loss: 1.000069]\n",
      "2283 [D loss: 0.999967] [G loss: 1.000061]\n",
      "2284 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2285 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2286 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2287 [D loss: 0.999969] [G loss: 1.000061]\n",
      "2288 [D loss: 0.999970] [G loss: 1.000060]\n",
      "2289 [D loss: 0.999970] [G loss: 1.000061]\n",
      "2290 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2291 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2292 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2293 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2294 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2295 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2296 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2297 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2298 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2299 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2300 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2301 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2302 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2303 [D loss: 0.999968] [G loss: 1.000067]\n",
      "2304 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2305 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2306 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2307 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2308 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2309 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2310 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2311 [D loss: 0.999971] [G loss: 1.000060]\n",
      "2312 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2313 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2314 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2315 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2316 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2317 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2318 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2319 [D loss: 0.999967] [G loss: 1.000069]\n",
      "2320 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2321 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2322 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2323 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2324 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2325 [D loss: 0.999975] [G loss: 1.000066]\n",
      "2326 [D loss: 0.999969] [G loss: 1.000062]\n",
      "2327 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2328 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2329 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2330 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2331 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2332 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2333 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2334 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2335 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2336 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2337 [D loss: 0.999972] [G loss: 1.000070]\n",
      "2338 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2339 [D loss: 0.999968] [G loss: 1.000067]\n",
      "2340 [D loss: 0.999974] [G loss: 1.000065]\n",
      "2341 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2342 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2343 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2344 [D loss: 0.999973] [G loss: 1.000067]\n",
      "2345 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2346 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2347 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2348 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2349 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2350 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2351 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2352 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2353 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2354 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2355 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2356 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2357 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2358 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2359 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2360 [D loss: 0.999971] [G loss: 1.000070]\n",
      "2361 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2362 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2363 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2364 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2365 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2366 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2367 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2368 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2369 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2370 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2371 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2372 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2373 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2374 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2375 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2376 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2377 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2378 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2379 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2380 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2381 [D loss: 0.999967] [G loss: 1.000063]\n",
      "2382 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2383 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2384 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2385 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2386 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2387 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2388 [D loss: 0.999974] [G loss: 1.000064]\n",
      "2389 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2390 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2391 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2392 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2393 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2394 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2395 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2396 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2397 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2398 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2399 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2400 [D loss: 0.999973] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2401 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2402 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2403 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2404 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2405 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2406 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2407 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2408 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2409 [D loss: 0.999973] [G loss: 1.000064]\n",
      "2410 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2411 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2412 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2413 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2414 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2415 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2416 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2417 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2418 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2419 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2420 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2421 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2422 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2423 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2424 [D loss: 0.999969] [G loss: 1.000061]\n",
      "2425 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2426 [D loss: 0.999966] [G loss: 1.000064]\n",
      "2427 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2428 [D loss: 0.999968] [G loss: 1.000062]\n",
      "2429 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2430 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2431 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2432 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2433 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2434 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2435 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2436 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2437 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2438 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2439 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2440 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2441 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2442 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2443 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2444 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2445 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2446 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2447 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2448 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2449 [D loss: 0.999969] [G loss: 1.000069]\n",
      "2450 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2451 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2452 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2453 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2454 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2455 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2456 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2457 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2458 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2459 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2460 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2461 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2462 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2463 [D loss: 0.999969] [G loss: 1.000062]\n",
      "2464 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2465 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2466 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2467 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2468 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2469 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2470 [D loss: 0.999971] [G loss: 1.000060]\n",
      "2471 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2472 [D loss: 0.999967] [G loss: 1.000064]\n",
      "2473 [D loss: 0.999973] [G loss: 1.000061]\n",
      "2474 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2475 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2476 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2477 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2478 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2479 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2480 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2481 [D loss: 0.999971] [G loss: 1.000059]\n",
      "2482 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2483 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2484 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2485 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2486 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2487 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2488 [D loss: 0.999968] [G loss: 1.000067]\n",
      "2489 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2490 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2491 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2492 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2493 [D loss: 0.999972] [G loss: 1.000069]\n",
      "2494 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2495 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2496 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2497 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2498 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2499 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2500 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2501 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2502 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2503 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2504 [D loss: 0.999973] [G loss: 1.000068]\n",
      "2505 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2506 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2507 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2508 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2509 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2510 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2511 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2512 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2513 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2514 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2515 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2516 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2517 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2518 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2519 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2520 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2521 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2522 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2523 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2524 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2525 [D loss: 0.999968] [G loss: 1.000063]\n",
      "2526 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2527 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2528 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2529 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2530 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2531 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2532 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2533 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2534 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2535 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2536 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2537 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2538 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2539 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2540 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2541 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2542 [D loss: 0.999973] [G loss: 1.000062]\n",
      "2543 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2544 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2545 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2546 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2547 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2548 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2549 [D loss: 0.999968] [G loss: 1.000063]\n",
      "2550 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2551 [D loss: 0.999967] [G loss: 1.000062]\n",
      "2552 [D loss: 0.999972] [G loss: 1.000060]\n",
      "2553 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2554 [D loss: 0.999973] [G loss: 1.000067]\n",
      "2555 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2556 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2557 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2558 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2559 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2560 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2561 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2562 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2563 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2564 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2565 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2566 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2567 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2568 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2569 [D loss: 0.999968] [G loss: 1.000068]\n",
      "2570 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2571 [D loss: 0.999967] [G loss: 1.000065]\n",
      "2572 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2573 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2574 [D loss: 0.999973] [G loss: 1.000069]\n",
      "2575 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2576 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2577 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2578 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2579 [D loss: 0.999968] [G loss: 1.000063]\n",
      "2580 [D loss: 0.999973] [G loss: 1.000067]\n",
      "2581 [D loss: 0.999968] [G loss: 1.000067]\n",
      "2582 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2583 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2584 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2585 [D loss: 0.999967] [G loss: 1.000065]\n",
      "2586 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2587 [D loss: 0.999967] [G loss: 1.000065]\n",
      "2588 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2589 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2590 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2591 [D loss: 0.999971] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2593 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2594 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2595 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2596 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2597 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2598 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2599 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2600 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2601 [D loss: 0.999967] [G loss: 1.000064]\n",
      "2602 [D loss: 0.999967] [G loss: 1.000061]\n",
      "2603 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2604 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2605 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2606 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2607 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2608 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2609 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2610 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2611 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2612 [D loss: 0.999973] [G loss: 1.000064]\n",
      "2613 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2614 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2615 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2616 [D loss: 0.999967] [G loss: 1.000065]\n",
      "2617 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2618 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2619 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2620 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2621 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2622 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2623 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2624 [D loss: 0.999973] [G loss: 1.000064]\n",
      "2625 [D loss: 0.999972] [G loss: 1.000061]\n",
      "2626 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2627 [D loss: 0.999967] [G loss: 1.000066]\n",
      "2628 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2629 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2630 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2631 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2632 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2633 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2634 [D loss: 0.999971] [G loss: 1.000069]\n",
      "2635 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2636 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2637 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2638 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2639 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2640 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2641 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2642 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2643 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2644 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2645 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2646 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2647 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2648 [D loss: 0.999970] [G loss: 1.000061]\n",
      "2649 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2650 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2651 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2652 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2653 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2654 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2655 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2656 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2657 [D loss: 0.999970] [G loss: 1.000060]\n",
      "2658 [D loss: 0.999967] [G loss: 1.000066]\n",
      "2659 [D loss: 0.999967] [G loss: 1.000065]\n",
      "2660 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2661 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2662 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2663 [D loss: 0.999967] [G loss: 1.000067]\n",
      "2664 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2665 [D loss: 0.999966] [G loss: 1.000063]\n",
      "2666 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2667 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2668 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2669 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2670 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2671 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2672 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2673 [D loss: 0.999969] [G loss: 1.000069]\n",
      "2674 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2675 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2676 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2677 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2678 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2679 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2680 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2681 [D loss: 0.999973] [G loss: 1.000064]\n",
      "2682 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2683 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2684 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2685 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2686 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2687 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2688 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2689 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2690 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2691 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2692 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2693 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2694 [D loss: 0.999971] [G loss: 1.000070]\n",
      "2695 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2696 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2697 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2698 [D loss: 0.999971] [G loss: 1.000070]\n",
      "2699 [D loss: 0.999973] [G loss: 1.000062]\n",
      "2700 [D loss: 0.999972] [G loss: 1.000070]\n",
      "2701 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2702 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2703 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2704 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2705 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2706 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2707 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2708 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2709 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2710 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2711 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2712 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2713 [D loss: 0.999973] [G loss: 1.000067]\n",
      "2714 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2715 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2716 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2717 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2718 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2719 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2720 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2721 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2722 [D loss: 0.999971] [G loss: 1.000060]\n",
      "2723 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2724 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2725 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2726 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2727 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2728 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2729 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2730 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2731 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2732 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2733 [D loss: 0.999973] [G loss: 1.000067]\n",
      "2734 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2735 [D loss: 0.999969] [G loss: 1.000061]\n",
      "2736 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2737 [D loss: 0.999968] [G loss: 1.000063]\n",
      "2738 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2739 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2740 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2741 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2742 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2743 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2744 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2745 [D loss: 0.999970] [G loss: 1.000061]\n",
      "2746 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2747 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2748 [D loss: 0.999968] [G loss: 1.000068]\n",
      "2749 [D loss: 0.999966] [G loss: 1.000065]\n",
      "2750 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2751 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2752 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2753 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2754 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2755 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2756 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2757 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2758 [D loss: 0.999970] [G loss: 1.000062]\n",
      "2759 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2760 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2761 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2762 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2763 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2764 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2765 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2766 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2767 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2768 [D loss: 0.999969] [G loss: 1.000062]\n",
      "2769 [D loss: 0.999973] [G loss: 1.000064]\n",
      "2770 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2771 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2772 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2773 [D loss: 0.999973] [G loss: 1.000065]\n",
      "2774 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2775 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2776 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2777 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2778 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2779 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2780 [D loss: 0.999971] [G loss: 1.000070]\n",
      "2781 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2782 [D loss: 0.999970] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2783 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2784 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2785 [D loss: 0.999972] [G loss: 1.000063]\n",
      "2786 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2787 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2788 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2789 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2790 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2791 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2792 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2793 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2794 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2795 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2796 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2797 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2798 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2799 [D loss: 0.999969] [G loss: 1.000069]\n",
      "2800 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2801 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2802 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2803 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2804 [D loss: 0.999969] [G loss: 1.000060]\n",
      "2805 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2806 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2807 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2808 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2809 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2810 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2811 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2812 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2813 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2814 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2815 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2816 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2817 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2818 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2819 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2820 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2821 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2822 [D loss: 0.999971] [G loss: 1.000060]\n",
      "2823 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2824 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2825 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2826 [D loss: 0.999973] [G loss: 1.000062]\n",
      "2827 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2828 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2829 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2830 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2831 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2832 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2833 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2834 [D loss: 0.999972] [G loss: 1.000062]\n",
      "2835 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2836 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2837 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2838 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2839 [D loss: 0.999970] [G loss: 1.000070]\n",
      "2840 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2841 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2842 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2843 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2844 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2845 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2846 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2847 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2848 [D loss: 0.999970] [G loss: 1.000069]\n",
      "2849 [D loss: 0.999970] [G loss: 1.000060]\n",
      "2850 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2851 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2852 [D loss: 0.999967] [G loss: 1.000065]\n",
      "2853 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2854 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2855 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2856 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2857 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2858 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2859 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2860 [D loss: 0.999972] [G loss: 1.000060]\n",
      "2861 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2862 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2863 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2864 [D loss: 0.999970] [G loss: 1.000070]\n",
      "2865 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2866 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2867 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2868 [D loss: 0.999971] [G loss: 1.000070]\n",
      "2869 [D loss: 0.999973] [G loss: 1.000066]\n",
      "2870 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2871 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2872 [D loss: 0.999968] [G loss: 1.000063]\n",
      "2873 [D loss: 0.999970] [G loss: 1.000061]\n",
      "2874 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2875 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2876 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2877 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2878 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2879 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2880 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2881 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2882 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2883 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2884 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2885 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2886 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2887 [D loss: 0.999968] [G loss: 1.000068]\n",
      "2888 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2889 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2890 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2891 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2892 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2893 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2894 [D loss: 0.999971] [G loss: 1.000068]\n",
      "2895 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2896 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2897 [D loss: 0.999971] [G loss: 1.000061]\n",
      "2898 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2899 [D loss: 0.999968] [G loss: 1.000067]\n",
      "2900 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2901 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2902 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2903 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2904 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2905 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2906 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2907 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2908 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2909 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2910 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2911 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2912 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2913 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2914 [D loss: 0.999970] [G loss: 1.000071]\n",
      "2915 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2916 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2917 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2918 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2919 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2920 [D loss: 0.999973] [G loss: 1.000067]\n",
      "2921 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2922 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2923 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2924 [D loss: 0.999968] [G loss: 1.000066]\n",
      "2925 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2926 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2927 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2928 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2929 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2930 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2931 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2932 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2933 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2934 [D loss: 0.999969] [G loss: 1.000064]\n",
      "2935 [D loss: 0.999972] [G loss: 1.000068]\n",
      "2936 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2937 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2938 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2939 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2940 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2941 [D loss: 0.999969] [G loss: 1.000069]\n",
      "2942 [D loss: 0.999968] [G loss: 1.000065]\n",
      "2943 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2944 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2945 [D loss: 0.999972] [G loss: 1.000067]\n",
      "2946 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2947 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2948 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2949 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2950 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2951 [D loss: 0.999970] [G loss: 1.000066]\n",
      "2952 [D loss: 0.999971] [G loss: 1.000062]\n",
      "2953 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2954 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2955 [D loss: 0.999971] [G loss: 1.000067]\n",
      "2956 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2957 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2958 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2959 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2960 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2961 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2962 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2963 [D loss: 0.999974] [G loss: 1.000065]\n",
      "2964 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2965 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2966 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2967 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2968 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2969 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2970 [D loss: 0.999973] [G loss: 1.000063]\n",
      "2971 [D loss: 0.999971] [G loss: 1.000063]\n",
      "2972 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2973 [D loss: 0.999972] [G loss: 1.000064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2974 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2975 [D loss: 0.999970] [G loss: 1.000067]\n",
      "2976 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2977 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2978 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2979 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2980 [D loss: 0.999969] [G loss: 1.000068]\n",
      "2981 [D loss: 0.999969] [G loss: 1.000066]\n",
      "2982 [D loss: 0.999969] [G loss: 1.000063]\n",
      "2983 [D loss: 0.999970] [G loss: 1.000065]\n",
      "2984 [D loss: 0.999970] [G loss: 1.000064]\n",
      "2985 [D loss: 0.999972] [G loss: 1.000065]\n",
      "2986 [D loss: 0.999972] [G loss: 1.000066]\n",
      "2987 [D loss: 0.999968] [G loss: 1.000067]\n",
      "2988 [D loss: 0.999972] [G loss: 1.000060]\n",
      "2989 [D loss: 0.999971] [G loss: 1.000065]\n",
      "2990 [D loss: 0.999968] [G loss: 1.000067]\n",
      "2991 [D loss: 0.999970] [G loss: 1.000063]\n",
      "2992 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2993 [D loss: 0.999970] [G loss: 1.000068]\n",
      "2994 [D loss: 0.999971] [G loss: 1.000064]\n",
      "2995 [D loss: 0.999969] [G loss: 1.000067]\n",
      "2996 [D loss: 0.999969] [G loss: 1.000065]\n",
      "2997 [D loss: 0.999971] [G loss: 1.000066]\n",
      "2998 [D loss: 0.999972] [G loss: 1.000064]\n",
      "2999 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3000 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3001 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3002 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3003 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3004 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3005 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3006 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3007 [D loss: 0.999968] [G loss: 1.000066]\n",
      "3008 [D loss: 0.999967] [G loss: 1.000066]\n",
      "3009 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3010 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3011 [D loss: 0.999972] [G loss: 1.000068]\n",
      "3012 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3013 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3014 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3015 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3016 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3017 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3018 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3019 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3020 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3021 [D loss: 0.999967] [G loss: 1.000066]\n",
      "3022 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3023 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3024 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3025 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3026 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3027 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3028 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3029 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3030 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3031 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3032 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3033 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3034 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3035 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3036 [D loss: 0.999973] [G loss: 1.000064]\n",
      "3037 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3038 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3039 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3040 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3041 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3042 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3043 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3044 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3045 [D loss: 0.999974] [G loss: 1.000064]\n",
      "3046 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3047 [D loss: 0.999969] [G loss: 1.000062]\n",
      "3048 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3049 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3050 [D loss: 0.999975] [G loss: 1.000067]\n",
      "3051 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3052 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3053 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3054 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3055 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3056 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3057 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3058 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3059 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3060 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3061 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3062 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3063 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3064 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3065 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3066 [D loss: 0.999973] [G loss: 1.000064]\n",
      "3067 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3068 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3069 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3070 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3071 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3072 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3073 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3074 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3075 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3076 [D loss: 0.999969] [G loss: 1.000061]\n",
      "3077 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3078 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3079 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3080 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3081 [D loss: 0.999973] [G loss: 1.000067]\n",
      "3082 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3083 [D loss: 0.999966] [G loss: 1.000063]\n",
      "3084 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3085 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3086 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3087 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3088 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3089 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3090 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3091 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3092 [D loss: 0.999973] [G loss: 1.000069]\n",
      "3093 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3094 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3095 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3096 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3097 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3098 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3099 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3100 [D loss: 0.999974] [G loss: 1.000062]\n",
      "3101 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3102 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3103 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3104 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3105 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3106 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3107 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3108 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3109 [D loss: 0.999970] [G loss: 1.000072]\n",
      "3110 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3111 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3112 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3113 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3114 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3115 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3116 [D loss: 0.999972] [G loss: 1.000060]\n",
      "3117 [D loss: 0.999967] [G loss: 1.000064]\n",
      "3118 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3119 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3120 [D loss: 0.999973] [G loss: 1.000061]\n",
      "3121 [D loss: 0.999972] [G loss: 1.000059]\n",
      "3122 [D loss: 0.999966] [G loss: 1.000064]\n",
      "3123 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3124 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3125 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3126 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3127 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3128 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3129 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3130 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3131 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3132 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3133 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3134 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3135 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3136 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3137 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3138 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3139 [D loss: 0.999967] [G loss: 1.000067]\n",
      "3140 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3141 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3142 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3143 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3144 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3145 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3146 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3147 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3148 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3149 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3150 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3151 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3152 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3153 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3154 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3155 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3156 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3157 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3158 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3159 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3160 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3161 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3162 [D loss: 0.999971] [G loss: 1.000061]\n",
      "3163 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3164 [D loss: 0.999970] [G loss: 1.000066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3165 [D loss: 0.999968] [G loss: 1.000064]\n",
      "3166 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3167 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3168 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3169 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3170 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3171 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3172 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3173 [D loss: 0.999969] [G loss: 1.000069]\n",
      "3174 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3175 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3176 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3177 [D loss: 0.999968] [G loss: 1.000066]\n",
      "3178 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3179 [D loss: 0.999970] [G loss: 1.000069]\n",
      "3180 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3181 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3182 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3183 [D loss: 0.999974] [G loss: 1.000065]\n",
      "3184 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3185 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3186 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3187 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3188 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3189 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3190 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3191 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3192 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3193 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3194 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3195 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3196 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3197 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3198 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3199 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3200 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3201 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3202 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3203 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3204 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3205 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3206 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3207 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3208 [D loss: 0.999971] [G loss: 1.000070]\n",
      "3209 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3210 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3211 [D loss: 0.999973] [G loss: 1.000065]\n",
      "3212 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3213 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3214 [D loss: 0.999974] [G loss: 1.000062]\n",
      "3215 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3216 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3217 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3218 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3219 [D loss: 0.999968] [G loss: 1.000062]\n",
      "3220 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3221 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3222 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3223 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3224 [D loss: 0.999972] [G loss: 1.000069]\n",
      "3225 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3226 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3227 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3228 [D loss: 0.999971] [G loss: 1.000061]\n",
      "3229 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3230 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3231 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3232 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3233 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3234 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3235 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3236 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3237 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3238 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3239 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3240 [D loss: 0.999973] [G loss: 1.000063]\n",
      "3241 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3242 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3243 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3244 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3245 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3246 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3247 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3248 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3249 [D loss: 0.999967] [G loss: 1.000066]\n",
      "3250 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3251 [D loss: 0.999970] [G loss: 1.000069]\n",
      "3252 [D loss: 0.999972] [G loss: 1.000061]\n",
      "3253 [D loss: 0.999972] [G loss: 1.000068]\n",
      "3254 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3255 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3256 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3257 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3258 [D loss: 0.999968] [G loss: 1.000059]\n",
      "3259 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3260 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3261 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3262 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3263 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3264 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3265 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3266 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3267 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3268 [D loss: 0.999967] [G loss: 1.000068]\n",
      "3269 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3270 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3271 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3272 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3273 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3274 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3275 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3276 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3277 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3278 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3279 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3280 [D loss: 0.999973] [G loss: 1.000060]\n",
      "3281 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3282 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3283 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3284 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3285 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3286 [D loss: 0.999969] [G loss: 1.000062]\n",
      "3287 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3288 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3289 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3290 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3291 [D loss: 0.999967] [G loss: 1.000066]\n",
      "3292 [D loss: 0.999973] [G loss: 1.000065]\n",
      "3293 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3294 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3295 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3296 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3297 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3298 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3299 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3300 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3301 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3302 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3303 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3304 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3305 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3306 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3307 [D loss: 0.999969] [G loss: 1.000062]\n",
      "3308 [D loss: 0.999967] [G loss: 1.000064]\n",
      "3309 [D loss: 0.999973] [G loss: 1.000061]\n",
      "3310 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3311 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3312 [D loss: 0.999973] [G loss: 1.000065]\n",
      "3313 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3314 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3315 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3316 [D loss: 0.999969] [G loss: 1.000061]\n",
      "3317 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3318 [D loss: 0.999968] [G loss: 1.000064]\n",
      "3319 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3320 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3321 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3322 [D loss: 0.999973] [G loss: 1.000060]\n",
      "3323 [D loss: 0.999968] [G loss: 1.000069]\n",
      "3324 [D loss: 0.999967] [G loss: 1.000064]\n",
      "3325 [D loss: 0.999973] [G loss: 1.000062]\n",
      "3326 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3327 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3328 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3329 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3330 [D loss: 0.999970] [G loss: 1.000069]\n",
      "3331 [D loss: 0.999967] [G loss: 1.000063]\n",
      "3332 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3333 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3334 [D loss: 0.999967] [G loss: 1.000065]\n",
      "3335 [D loss: 0.999973] [G loss: 1.000063]\n",
      "3336 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3337 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3338 [D loss: 0.999973] [G loss: 1.000067]\n",
      "3339 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3340 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3341 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3342 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3343 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3344 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3345 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3346 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3347 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3348 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3349 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3350 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3351 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3352 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3353 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3354 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3355 [D loss: 0.999970] [G loss: 1.000066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3356 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3357 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3358 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3359 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3360 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3361 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3362 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3363 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3364 [D loss: 0.999968] [G loss: 1.000064]\n",
      "3365 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3366 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3367 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3368 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3369 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3370 [D loss: 0.999968] [G loss: 1.000066]\n",
      "3371 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3372 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3373 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3374 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3375 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3376 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3377 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3378 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3379 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3380 [D loss: 0.999972] [G loss: 1.000068]\n",
      "3381 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3382 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3383 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3384 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3385 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3386 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3387 [D loss: 0.999973] [G loss: 1.000065]\n",
      "3388 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3389 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3390 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3391 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3392 [D loss: 0.999971] [G loss: 1.000061]\n",
      "3393 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3394 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3395 [D loss: 0.999971] [G loss: 1.000070]\n",
      "3396 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3397 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3398 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3399 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3400 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3401 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3402 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3403 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3404 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3405 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3406 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3407 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3408 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3409 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3410 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3411 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3412 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3413 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3414 [D loss: 0.999968] [G loss: 1.000062]\n",
      "3415 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3416 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3417 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3418 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3419 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3420 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3421 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3422 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3423 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3424 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3425 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3426 [D loss: 0.999968] [G loss: 1.000063]\n",
      "3427 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3428 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3429 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3430 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3431 [D loss: 0.999966] [G loss: 1.000065]\n",
      "3432 [D loss: 0.999969] [G loss: 1.000070]\n",
      "3433 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3434 [D loss: 0.999967] [G loss: 1.000065]\n",
      "3435 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3436 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3437 [D loss: 0.999973] [G loss: 1.000065]\n",
      "3438 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3439 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3440 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3441 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3442 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3443 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3444 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3445 [D loss: 0.999973] [G loss: 1.000063]\n",
      "3446 [D loss: 0.999970] [G loss: 1.000060]\n",
      "3447 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3448 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3449 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3450 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3451 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3452 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3453 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3454 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3455 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3456 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3457 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3458 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3459 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3460 [D loss: 0.999971] [G loss: 1.000061]\n",
      "3461 [D loss: 0.999970] [G loss: 1.000059]\n",
      "3462 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3463 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3464 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3465 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3466 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3467 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3468 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3469 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3470 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3471 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3472 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3473 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3474 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3475 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3476 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3477 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3478 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3479 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3480 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3481 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3482 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3483 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3484 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3485 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3486 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3487 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3488 [D loss: 0.999969] [G loss: 1.000069]\n",
      "3489 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3490 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3491 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3492 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3493 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3494 [D loss: 0.999968] [G loss: 1.000066]\n",
      "3495 [D loss: 0.999973] [G loss: 1.000064]\n",
      "3496 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3497 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3498 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3499 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3500 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3501 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3502 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3503 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3504 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3505 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3506 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3507 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3508 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3509 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3510 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3511 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3512 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3513 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3514 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3515 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3516 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3517 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3518 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3519 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3520 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3521 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3522 [D loss: 0.999969] [G loss: 1.000060]\n",
      "3523 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3524 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3525 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3526 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3527 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3528 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3529 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3530 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3531 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3532 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3533 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3534 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3535 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3536 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3537 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3538 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3539 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3540 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3541 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3542 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3543 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3544 [D loss: 0.999970] [G loss: 1.000069]\n",
      "3545 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3546 [D loss: 0.999972] [G loss: 1.000066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3548 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3549 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3550 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3551 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3552 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3553 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3554 [D loss: 0.999968] [G loss: 1.000064]\n",
      "3555 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3556 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3557 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3558 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3559 [D loss: 0.999968] [G loss: 1.000063]\n",
      "3560 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3561 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3562 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3563 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3564 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3565 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3566 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3567 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3568 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3569 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3570 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3571 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3572 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3573 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3574 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3575 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3576 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3577 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3578 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3579 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3580 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3581 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3582 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3583 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3584 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3585 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3586 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3587 [D loss: 0.999972] [G loss: 1.000068]\n",
      "3588 [D loss: 0.999973] [G loss: 1.000064]\n",
      "3589 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3590 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3591 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3592 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3593 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3594 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3595 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3596 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3597 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3598 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3599 [D loss: 0.999973] [G loss: 1.000065]\n",
      "3600 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3601 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3602 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3603 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3604 [D loss: 0.999967] [G loss: 1.000066]\n",
      "3605 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3606 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3607 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3608 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3609 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3610 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3611 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3612 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3613 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3614 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3615 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3616 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3617 [D loss: 0.999968] [G loss: 1.000066]\n",
      "3618 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3619 [D loss: 0.999969] [G loss: 1.000069]\n",
      "3620 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3621 [D loss: 0.999971] [G loss: 1.000071]\n",
      "3622 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3623 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3624 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3625 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3626 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3627 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3628 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3629 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3630 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3631 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3632 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3633 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3634 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3635 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3636 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3637 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3638 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3639 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3640 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3641 [D loss: 0.999972] [G loss: 1.000068]\n",
      "3642 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3643 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3644 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3645 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3646 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3647 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3648 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3649 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3650 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3651 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3652 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3653 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3654 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3655 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3656 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3657 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3658 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3659 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3660 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3661 [D loss: 0.999969] [G loss: 1.000062]\n",
      "3662 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3663 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3664 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3665 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3666 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3667 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3668 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3669 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3670 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3671 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3672 [D loss: 0.999973] [G loss: 1.000061]\n",
      "3673 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3674 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3675 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3676 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3677 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3678 [D loss: 0.999967] [G loss: 1.000065]\n",
      "3679 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3680 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3681 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3682 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3683 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3684 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3685 [D loss: 0.999973] [G loss: 1.000065]\n",
      "3686 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3687 [D loss: 0.999969] [G loss: 1.000060]\n",
      "3688 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3689 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3690 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3691 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3692 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3693 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3694 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3695 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3696 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3697 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3698 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3699 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3700 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3701 [D loss: 0.999970] [G loss: 1.000058]\n",
      "3702 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3703 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3704 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3705 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3706 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3707 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3708 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3709 [D loss: 0.999968] [G loss: 1.000069]\n",
      "3710 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3711 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3712 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3713 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3714 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3715 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3716 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3717 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3718 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3719 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3720 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3721 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3722 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3723 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3724 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3725 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3726 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3727 [D loss: 0.999972] [G loss: 1.000069]\n",
      "3728 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3729 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3730 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3731 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3732 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3733 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3734 [D loss: 0.999969] [G loss: 1.000069]\n",
      "3735 [D loss: 0.999967] [G loss: 1.000063]\n",
      "3736 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3737 [D loss: 0.999972] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3738 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3739 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3740 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3741 [D loss: 0.999967] [G loss: 1.000067]\n",
      "3742 [D loss: 0.999967] [G loss: 1.000063]\n",
      "3743 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3744 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3745 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3746 [D loss: 0.999972] [G loss: 1.000060]\n",
      "3747 [D loss: 0.999967] [G loss: 1.000067]\n",
      "3748 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3749 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3750 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3751 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3752 [D loss: 0.999972] [G loss: 1.000060]\n",
      "3753 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3754 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3755 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3756 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3757 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3758 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3759 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3760 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3761 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3762 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3763 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3764 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3765 [D loss: 0.999973] [G loss: 1.000063]\n",
      "3766 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3767 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3768 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3769 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3770 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3771 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3772 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3773 [D loss: 0.999968] [G loss: 1.000064]\n",
      "3774 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3775 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3776 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3777 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3778 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3779 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3780 [D loss: 0.999972] [G loss: 1.000061]\n",
      "3781 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3782 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3783 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3784 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3785 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3786 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3787 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3788 [D loss: 0.999972] [G loss: 1.000071]\n",
      "3789 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3790 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3791 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3792 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3793 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3794 [D loss: 0.999969] [G loss: 1.000062]\n",
      "3795 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3796 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3797 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3798 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3799 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3800 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3801 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3802 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3803 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3804 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3805 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3806 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3807 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3808 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3809 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3810 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3811 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3812 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3813 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3814 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3815 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3816 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3817 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3818 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3819 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3820 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3821 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3822 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3823 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3824 [D loss: 0.999966] [G loss: 1.000065]\n",
      "3825 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3826 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3827 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3828 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3829 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3830 [D loss: 0.999970] [G loss: 1.000070]\n",
      "3831 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3832 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3833 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3834 [D loss: 0.999974] [G loss: 1.000065]\n",
      "3835 [D loss: 0.999970] [G loss: 1.000062]\n",
      "3836 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3837 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3838 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3839 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3840 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3841 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3842 [D loss: 0.999968] [G loss: 1.000066]\n",
      "3843 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3844 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3845 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3846 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3847 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3848 [D loss: 0.999974] [G loss: 1.000059]\n",
      "3849 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3850 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3851 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3852 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3853 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3854 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3855 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3856 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3857 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3858 [D loss: 0.999973] [G loss: 1.000066]\n",
      "3859 [D loss: 0.999972] [G loss: 1.000071]\n",
      "3860 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3861 [D loss: 0.999974] [G loss: 1.000066]\n",
      "3862 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3863 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3864 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3865 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3866 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3867 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3868 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3869 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3870 [D loss: 0.999972] [G loss: 1.000063]\n",
      "3871 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3872 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3873 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3874 [D loss: 0.999970] [G loss: 1.000068]\n",
      "3875 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3876 [D loss: 0.999968] [G loss: 1.000068]\n",
      "3877 [D loss: 0.999969] [G loss: 1.000070]\n",
      "3878 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3879 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3880 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3881 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3882 [D loss: 0.999971] [G loss: 1.000060]\n",
      "3883 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3884 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3885 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3886 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3887 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3888 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3889 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3890 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3891 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3892 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3893 [D loss: 0.999967] [G loss: 1.000066]\n",
      "3894 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3895 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3896 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3897 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3898 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3899 [D loss: 0.999973] [G loss: 1.000064]\n",
      "3900 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3901 [D loss: 0.999971] [G loss: 1.000060]\n",
      "3902 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3903 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3904 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3905 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3906 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3907 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3908 [D loss: 0.999971] [G loss: 1.000071]\n",
      "3909 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3910 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3911 [D loss: 0.999968] [G loss: 1.000065]\n",
      "3912 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3913 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3914 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3915 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3916 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3917 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3918 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3919 [D loss: 0.999971] [G loss: 1.000063]\n",
      "3920 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3921 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3922 [D loss: 0.999969] [G loss: 1.000060]\n",
      "3923 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3924 [D loss: 0.999972] [G loss: 1.000068]\n",
      "3925 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3926 [D loss: 0.999971] [G loss: 1.000068]\n",
      "3927 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3928 [D loss: 0.999970] [G loss: 1.000069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3929 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3930 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3931 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3932 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3933 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3934 [D loss: 0.999968] [G loss: 1.000067]\n",
      "3935 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3936 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3937 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3938 [D loss: 0.999968] [G loss: 1.000069]\n",
      "3939 [D loss: 0.999972] [G loss: 1.000059]\n",
      "3940 [D loss: 0.999972] [G loss: 1.000064]\n",
      "3941 [D loss: 0.999968] [G loss: 1.000066]\n",
      "3942 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3943 [D loss: 0.999973] [G loss: 1.000063]\n",
      "3944 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3945 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3946 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3947 [D loss: 0.999971] [G loss: 1.000069]\n",
      "3948 [D loss: 0.999970] [G loss: 1.000061]\n",
      "3949 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3950 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3951 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3952 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3953 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3954 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3955 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3956 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3957 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3958 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3959 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3960 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3961 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3962 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3963 [D loss: 0.999972] [G loss: 1.000062]\n",
      "3964 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3965 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3966 [D loss: 0.999971] [G loss: 1.000061]\n",
      "3967 [D loss: 0.999972] [G loss: 1.000066]\n",
      "3968 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3969 [D loss: 0.999972] [G loss: 1.000065]\n",
      "3970 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3971 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3972 [D loss: 0.999971] [G loss: 1.000064]\n",
      "3973 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3974 [D loss: 0.999969] [G loss: 1.000066]\n",
      "3975 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3976 [D loss: 0.999969] [G loss: 1.000067]\n",
      "3977 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3978 [D loss: 0.999971] [G loss: 1.000067]\n",
      "3979 [D loss: 0.999969] [G loss: 1.000060]\n",
      "3980 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3981 [D loss: 0.999969] [G loss: 1.000068]\n",
      "3982 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3983 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3984 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3985 [D loss: 0.999971] [G loss: 1.000065]\n",
      "3986 [D loss: 0.999970] [G loss: 1.000066]\n",
      "3987 [D loss: 0.999970] [G loss: 1.000067]\n",
      "3988 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3989 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3990 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3991 [D loss: 0.999972] [G loss: 1.000067]\n",
      "3992 [D loss: 0.999970] [G loss: 1.000065]\n",
      "3993 [D loss: 0.999969] [G loss: 1.000064]\n",
      "3994 [D loss: 0.999970] [G loss: 1.000063]\n",
      "3995 [D loss: 0.999969] [G loss: 1.000062]\n",
      "3996 [D loss: 0.999971] [G loss: 1.000066]\n",
      "3997 [D loss: 0.999970] [G loss: 1.000064]\n",
      "3998 [D loss: 0.999971] [G loss: 1.000062]\n",
      "3999 [D loss: 0.999972] [G loss: 1.000070]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.005\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the critic\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        # The critic takes generated images as input and determines validity\n",
    "        valid = self.critic(img)\n",
    "\n",
    "        # The combined model  (stacked generator and critic)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        x=2\n",
    "        model.add(Conv2D(x*16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(x*32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(x*64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(x*128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            self.critic.trainable = True\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                \n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the critic\n",
    "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "                # Clip critic weights\n",
    "                for l in self.critic.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            #self.critic.trainable = False\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            #print(epoch)\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/wgan_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wgan = WGAN()\n",
    "wgan.train(epochs=4000, batch_size=32, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=np.random.normal(0, 1, (32, 100))\n",
    "valid=-np.ones((32, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00015029908"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgan.combined.train_on_batch(noise,valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_123 (Conv2D)          (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPaddi (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_17 (Reshape)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_34 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_35 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.999845] [G loss: 1.000332]\n",
      "1 [D loss: 0.999869] [G loss: 1.000340]\n",
      "2 [D loss: 0.999862] [G loss: 1.000338]\n",
      "3 [D loss: 0.999866] [G loss: 1.000345]\n",
      "4 [D loss: 0.999869] [G loss: 1.000327]\n",
      "5 [D loss: 0.999872] [G loss: 1.000340]\n",
      "6 [D loss: 0.999868] [G loss: 1.000341]\n",
      "7 [D loss: 0.999869] [G loss: 1.000343]\n",
      "8 [D loss: 0.999871] [G loss: 1.000347]\n",
      "9 [D loss: 0.999873] [G loss: 1.000332]\n",
      "10 [D loss: 0.999872] [G loss: 1.000323]\n",
      "11 [D loss: 0.999877] [G loss: 1.000317]\n",
      "12 [D loss: 0.999874] [G loss: 1.000301]\n",
      "13 [D loss: 0.999878] [G loss: 1.000298]\n",
      "14 [D loss: 0.999885] [G loss: 1.000294]\n",
      "15 [D loss: 0.999887] [G loss: 1.000270]\n",
      "16 [D loss: 0.999894] [G loss: 1.000255]\n",
      "17 [D loss: 0.999893] [G loss: 1.000224]\n",
      "18 [D loss: 0.999903] [G loss: 1.000214]\n",
      "19 [D loss: 0.999906] [G loss: 1.000201]\n",
      "20 [D loss: 0.999912] [G loss: 1.000188]\n",
      "21 [D loss: 0.999917] [G loss: 1.000169]\n",
      "22 [D loss: 0.999923] [G loss: 1.000154]\n",
      "23 [D loss: 0.999922] [G loss: 1.000151]\n",
      "24 [D loss: 0.999931] [G loss: 1.000144]\n",
      "25 [D loss: 0.999939] [G loss: 1.000140]\n",
      "26 [D loss: 0.999942] [G loss: 1.000126]\n",
      "27 [D loss: 0.999937] [G loss: 1.000122]\n",
      "28 [D loss: 0.999944] [G loss: 1.000122]\n",
      "29 [D loss: 0.999946] [G loss: 1.000124]\n",
      "30 [D loss: 0.999950] [G loss: 1.000124]\n",
      "31 [D loss: 0.999941] [G loss: 1.000105]\n",
      "32 [D loss: 0.999956] [G loss: 1.000106]\n",
      "33 [D loss: 0.999953] [G loss: 1.000112]\n",
      "34 [D loss: 0.999954] [G loss: 1.000095]\n",
      "35 [D loss: 0.999962] [G loss: 1.000093]\n",
      "36 [D loss: 0.999959] [G loss: 1.000093]\n",
      "37 [D loss: 0.999968] [G loss: 1.000087]\n",
      "38 [D loss: 0.999963] [G loss: 1.000097]\n",
      "39 [D loss: 0.999983] [G loss: 1.000088]\n",
      "40 [D loss: 0.999992] [G loss: 1.000084]\n",
      "41 [D loss: 0.999988] [G loss: 1.000087]\n",
      "42 [D loss: 0.999992] [G loss: 1.000104]\n",
      "43 [D loss: 0.999984] [G loss: 1.000131]\n",
      "44 [D loss: 0.999994] [G loss: 1.000118]\n",
      "45 [D loss: 0.999995] [G loss: 1.000121]\n",
      "46 [D loss: 1.000015] [G loss: 1.000139]\n",
      "47 [D loss: 0.999985] [G loss: 1.000149]\n",
      "48 [D loss: 1.000023] [G loss: 1.000169]\n",
      "49 [D loss: 1.000016] [G loss: 1.000157]\n",
      "50 [D loss: 0.999995] [G loss: 1.000178]\n",
      "51 [D loss: 0.999998] [G loss: 1.000235]\n",
      "52 [D loss: 1.000041] [G loss: 1.000219]\n",
      "53 [D loss: 1.000009] [G loss: 1.000250]\n",
      "54 [D loss: 0.999979] [G loss: 1.000274]\n",
      "55 [D loss: 1.000024] [G loss: 1.000275]\n",
      "56 [D loss: 0.999983] [G loss: 1.000325]\n",
      "57 [D loss: 1.000021] [G loss: 1.000292]\n",
      "58 [D loss: 0.999972] [G loss: 1.000335]\n",
      "59 [D loss: 0.999971] [G loss: 1.000361]\n",
      "60 [D loss: 0.999979] [G loss: 1.000342]\n",
      "61 [D loss: 1.000003] [G loss: 1.000277]\n",
      "62 [D loss: 0.999964] [G loss: 1.000328]\n",
      "63 [D loss: 0.999965] [G loss: 1.000330]\n",
      "64 [D loss: 0.999976] [G loss: 1.000308]\n",
      "65 [D loss: 0.999987] [G loss: 1.000313]\n",
      "66 [D loss: 0.999945] [G loss: 1.000355]\n",
      "67 [D loss: 0.999943] [G loss: 1.000319]\n",
      "68 [D loss: 0.999954] [G loss: 1.000288]\n",
      "69 [D loss: 0.999944] [G loss: 1.000319]\n",
      "70 [D loss: 0.999957] [G loss: 1.000304]\n",
      "71 [D loss: 0.999957] [G loss: 1.000296]\n",
      "72 [D loss: 0.999955] [G loss: 1.000280]\n",
      "73 [D loss: 0.999986] [G loss: 1.000282]\n",
      "74 [D loss: 0.999954] [G loss: 1.000269]\n",
      "75 [D loss: 0.999961] [G loss: 1.000260]\n",
      "76 [D loss: 0.999954] [G loss: 1.000265]\n",
      "77 [D loss: 0.999965] [G loss: 1.000249]\n",
      "78 [D loss: 0.999966] [G loss: 1.000193]\n",
      "79 [D loss: 0.999963] [G loss: 1.000220]\n",
      "80 [D loss: 0.999959] [G loss: 1.000197]\n",
      "81 [D loss: 0.999957] [G loss: 1.000204]\n",
      "82 [D loss: 0.999958] [G loss: 1.000196]\n",
      "83 [D loss: 0.999950] [G loss: 1.000175]\n",
      "84 [D loss: 0.999967] [G loss: 1.000166]\n",
      "85 [D loss: 0.999971] [G loss: 1.000179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 [D loss: 0.999962] [G loss: 1.000165]\n",
      "87 [D loss: 0.999933] [G loss: 1.000184]\n",
      "88 [D loss: 0.999956] [G loss: 1.000174]\n",
      "89 [D loss: 0.999975] [G loss: 1.000145]\n",
      "90 [D loss: 0.999966] [G loss: 1.000138]\n",
      "91 [D loss: 0.999974] [G loss: 1.000134]\n",
      "92 [D loss: 0.999965] [G loss: 1.000143]\n",
      "93 [D loss: 0.999956] [G loss: 1.000136]\n",
      "94 [D loss: 0.999967] [G loss: 1.000111]\n",
      "95 [D loss: 0.999958] [G loss: 1.000117]\n",
      "96 [D loss: 0.999965] [G loss: 1.000109]\n",
      "97 [D loss: 0.999958] [G loss: 1.000100]\n",
      "98 [D loss: 0.999970] [G loss: 1.000101]\n",
      "99 [D loss: 0.999968] [G loss: 1.000092]\n",
      "100 [D loss: 0.999961] [G loss: 1.000084]\n",
      "101 [D loss: 0.999965] [G loss: 1.000105]\n",
      "102 [D loss: 0.999964] [G loss: 1.000106]\n",
      "103 [D loss: 0.999971] [G loss: 1.000094]\n",
      "104 [D loss: 0.999967] [G loss: 1.000097]\n",
      "105 [D loss: 0.999973] [G loss: 1.000098]\n",
      "106 [D loss: 0.999962] [G loss: 1.000093]\n",
      "107 [D loss: 0.999979] [G loss: 1.000093]\n",
      "108 [D loss: 0.999979] [G loss: 1.000091]\n",
      "109 [D loss: 0.999972] [G loss: 1.000111]\n",
      "110 [D loss: 0.999961] [G loss: 1.000071]\n",
      "111 [D loss: 0.999981] [G loss: 1.000082]\n",
      "112 [D loss: 0.999955] [G loss: 1.000077]\n",
      "113 [D loss: 0.999973] [G loss: 1.000103]\n",
      "114 [D loss: 0.999973] [G loss: 1.000096]\n",
      "115 [D loss: 0.999972] [G loss: 1.000088]\n",
      "116 [D loss: 0.999978] [G loss: 1.000088]\n",
      "117 [D loss: 0.999954] [G loss: 1.000118]\n",
      "118 [D loss: 0.999964] [G loss: 1.000116]\n",
      "119 [D loss: 0.999965] [G loss: 1.000089]\n",
      "120 [D loss: 0.999964] [G loss: 1.000090]\n",
      "121 [D loss: 0.999957] [G loss: 1.000112]\n",
      "122 [D loss: 0.999978] [G loss: 1.000108]\n",
      "123 [D loss: 0.999941] [G loss: 1.000115]\n",
      "124 [D loss: 0.999964] [G loss: 1.000107]\n",
      "125 [D loss: 0.999959] [G loss: 1.000084]\n",
      "126 [D loss: 0.999976] [G loss: 1.000106]\n",
      "127 [D loss: 0.999974] [G loss: 1.000097]\n",
      "128 [D loss: 0.999968] [G loss: 1.000102]\n",
      "129 [D loss: 0.999956] [G loss: 1.000093]\n",
      "130 [D loss: 0.999982] [G loss: 1.000064]\n",
      "131 [D loss: 0.999960] [G loss: 1.000107]\n",
      "132 [D loss: 0.999963] [G loss: 1.000103]\n",
      "133 [D loss: 0.999964] [G loss: 1.000094]\n",
      "134 [D loss: 0.999971] [G loss: 1.000084]\n",
      "135 [D loss: 0.999964] [G loss: 1.000095]\n",
      "136 [D loss: 0.999970] [G loss: 1.000102]\n",
      "137 [D loss: 0.999951] [G loss: 1.000114]\n",
      "138 [D loss: 0.999968] [G loss: 1.000105]\n",
      "139 [D loss: 0.999960] [G loss: 1.000089]\n",
      "140 [D loss: 0.999965] [G loss: 1.000105]\n",
      "141 [D loss: 0.999946] [G loss: 1.000085]\n",
      "142 [D loss: 0.999966] [G loss: 1.000102]\n",
      "143 [D loss: 0.999980] [G loss: 1.000069]\n",
      "144 [D loss: 0.999963] [G loss: 1.000084]\n",
      "145 [D loss: 0.999967] [G loss: 1.000084]\n",
      "146 [D loss: 0.999966] [G loss: 1.000080]\n",
      "147 [D loss: 0.999966] [G loss: 1.000088]\n",
      "148 [D loss: 0.999961] [G loss: 1.000083]\n",
      "149 [D loss: 0.999981] [G loss: 1.000074]\n",
      "150 [D loss: 0.999974] [G loss: 1.000097]\n",
      "151 [D loss: 0.999979] [G loss: 1.000096]\n",
      "152 [D loss: 0.999968] [G loss: 1.000084]\n",
      "153 [D loss: 0.999973] [G loss: 1.000090]\n",
      "154 [D loss: 0.999967] [G loss: 1.000087]\n",
      "155 [D loss: 0.999970] [G loss: 1.000082]\n",
      "156 [D loss: 0.999969] [G loss: 1.000064]\n",
      "157 [D loss: 0.999963] [G loss: 1.000081]\n",
      "158 [D loss: 0.999967] [G loss: 1.000078]\n",
      "159 [D loss: 0.999950] [G loss: 1.000098]\n",
      "160 [D loss: 0.999967] [G loss: 1.000077]\n",
      "161 [D loss: 0.999952] [G loss: 1.000099]\n",
      "162 [D loss: 0.999974] [G loss: 1.000084]\n",
      "163 [D loss: 0.999967] [G loss: 1.000072]\n",
      "164 [D loss: 0.999978] [G loss: 1.000073]\n",
      "165 [D loss: 0.999958] [G loss: 1.000060]\n",
      "166 [D loss: 0.999969] [G loss: 1.000051]\n",
      "167 [D loss: 0.999964] [G loss: 1.000076]\n",
      "168 [D loss: 0.999954] [G loss: 1.000082]\n",
      "169 [D loss: 0.999973] [G loss: 1.000083]\n",
      "170 [D loss: 0.999964] [G loss: 1.000096]\n",
      "171 [D loss: 0.999971] [G loss: 1.000069]\n",
      "172 [D loss: 0.999962] [G loss: 1.000090]\n",
      "173 [D loss: 0.999961] [G loss: 1.000064]\n",
      "174 [D loss: 0.999964] [G loss: 1.000061]\n",
      "175 [D loss: 0.999954] [G loss: 1.000069]\n",
      "176 [D loss: 0.999974] [G loss: 1.000067]\n",
      "177 [D loss: 0.999959] [G loss: 1.000061]\n",
      "178 [D loss: 0.999966] [G loss: 1.000068]\n",
      "179 [D loss: 0.999964] [G loss: 1.000056]\n",
      "180 [D loss: 0.999972] [G loss: 1.000066]\n",
      "181 [D loss: 0.999980] [G loss: 1.000071]\n",
      "182 [D loss: 0.999974] [G loss: 1.000037]\n",
      "183 [D loss: 0.999979] [G loss: 1.000061]\n",
      "184 [D loss: 0.999964] [G loss: 1.000057]\n",
      "185 [D loss: 0.999969] [G loss: 1.000055]\n",
      "186 [D loss: 0.999974] [G loss: 1.000049]\n",
      "187 [D loss: 0.999969] [G loss: 1.000055]\n",
      "188 [D loss: 0.999982] [G loss: 1.000046]\n",
      "189 [D loss: 0.999972] [G loss: 1.000053]\n",
      "190 [D loss: 0.999973] [G loss: 1.000059]\n",
      "191 [D loss: 0.999962] [G loss: 1.000058]\n",
      "192 [D loss: 0.999961] [G loss: 1.000057]\n",
      "193 [D loss: 0.999974] [G loss: 1.000050]\n",
      "194 [D loss: 0.999964] [G loss: 1.000065]\n",
      "195 [D loss: 0.999962] [G loss: 1.000064]\n",
      "196 [D loss: 0.999966] [G loss: 1.000071]\n",
      "197 [D loss: 0.999970] [G loss: 1.000079]\n",
      "198 [D loss: 0.999970] [G loss: 1.000051]\n",
      "199 [D loss: 0.999962] [G loss: 1.000043]\n",
      "200 [D loss: 0.999975] [G loss: 1.000048]\n",
      "201 [D loss: 0.999969] [G loss: 1.000049]\n",
      "202 [D loss: 0.999961] [G loss: 1.000044]\n",
      "203 [D loss: 0.999969] [G loss: 1.000071]\n",
      "204 [D loss: 0.999974] [G loss: 1.000066]\n",
      "205 [D loss: 0.999974] [G loss: 1.000054]\n",
      "206 [D loss: 0.999974] [G loss: 1.000061]\n",
      "207 [D loss: 0.999971] [G loss: 1.000065]\n",
      "208 [D loss: 0.999974] [G loss: 1.000057]\n",
      "209 [D loss: 0.999973] [G loss: 1.000047]\n",
      "210 [D loss: 0.999965] [G loss: 1.000069]\n",
      "211 [D loss: 0.999972] [G loss: 1.000063]\n",
      "212 [D loss: 0.999976] [G loss: 1.000066]\n",
      "213 [D loss: 0.999968] [G loss: 1.000070]\n",
      "214 [D loss: 0.999969] [G loss: 1.000047]\n",
      "215 [D loss: 0.999968] [G loss: 1.000059]\n",
      "216 [D loss: 0.999977] [G loss: 1.000045]\n",
      "217 [D loss: 0.999961] [G loss: 1.000043]\n",
      "218 [D loss: 0.999960] [G loss: 1.000059]\n",
      "219 [D loss: 0.999977] [G loss: 1.000035]\n",
      "220 [D loss: 0.999970] [G loss: 1.000064]\n",
      "221 [D loss: 0.999975] [G loss: 1.000044]\n",
      "222 [D loss: 0.999973] [G loss: 1.000063]\n",
      "223 [D loss: 0.999982] [G loss: 1.000044]\n",
      "224 [D loss: 0.999976] [G loss: 1.000070]\n",
      "225 [D loss: 0.999963] [G loss: 1.000049]\n",
      "226 [D loss: 0.999961] [G loss: 1.000083]\n",
      "227 [D loss: 0.999968] [G loss: 1.000047]\n",
      "228 [D loss: 0.999957] [G loss: 1.000052]\n",
      "229 [D loss: 0.999968] [G loss: 1.000067]\n",
      "230 [D loss: 0.999968] [G loss: 1.000054]\n",
      "231 [D loss: 0.999961] [G loss: 1.000069]\n",
      "232 [D loss: 0.999971] [G loss: 1.000061]\n",
      "233 [D loss: 0.999957] [G loss: 1.000028]\n",
      "234 [D loss: 0.999967] [G loss: 1.000044]\n",
      "235 [D loss: 0.999972] [G loss: 1.000054]\n",
      "236 [D loss: 0.999960] [G loss: 1.000044]\n",
      "237 [D loss: 0.999970] [G loss: 1.000071]\n",
      "238 [D loss: 0.999972] [G loss: 1.000036]\n",
      "239 [D loss: 0.999965] [G loss: 1.000058]\n",
      "240 [D loss: 0.999952] [G loss: 1.000071]\n",
      "241 [D loss: 0.999973] [G loss: 1.000079]\n",
      "242 [D loss: 0.999981] [G loss: 1.000083]\n",
      "243 [D loss: 0.999958] [G loss: 1.000067]\n",
      "244 [D loss: 0.999973] [G loss: 1.000047]\n",
      "245 [D loss: 0.999969] [G loss: 1.000057]\n",
      "246 [D loss: 0.999960] [G loss: 1.000044]\n",
      "247 [D loss: 0.999968] [G loss: 1.000047]\n",
      "248 [D loss: 0.999969] [G loss: 1.000051]\n",
      "249 [D loss: 0.999964] [G loss: 1.000081]\n",
      "250 [D loss: 0.999966] [G loss: 1.000055]\n",
      "251 [D loss: 0.999971] [G loss: 1.000072]\n",
      "252 [D loss: 0.999969] [G loss: 1.000090]\n",
      "253 [D loss: 0.999960] [G loss: 1.000062]\n",
      "254 [D loss: 0.999958] [G loss: 1.000053]\n",
      "255 [D loss: 0.999962] [G loss: 1.000053]\n",
      "256 [D loss: 0.999974] [G loss: 1.000074]\n",
      "257 [D loss: 0.999963] [G loss: 1.000051]\n",
      "258 [D loss: 0.999980] [G loss: 1.000049]\n",
      "259 [D loss: 0.999960] [G loss: 1.000071]\n",
      "260 [D loss: 0.999956] [G loss: 1.000050]\n",
      "261 [D loss: 0.999961] [G loss: 1.000047]\n",
      "262 [D loss: 0.999968] [G loss: 1.000046]\n",
      "263 [D loss: 0.999966] [G loss: 1.000063]\n",
      "264 [D loss: 0.999968] [G loss: 1.000036]\n",
      "265 [D loss: 0.999976] [G loss: 1.000077]\n",
      "266 [D loss: 0.999971] [G loss: 1.000058]\n",
      "267 [D loss: 0.999972] [G loss: 1.000062]\n",
      "268 [D loss: 0.999971] [G loss: 1.000070]\n",
      "269 [D loss: 0.999960] [G loss: 1.000044]\n",
      "270 [D loss: 0.999977] [G loss: 1.000060]\n",
      "271 [D loss: 0.999969] [G loss: 1.000038]\n",
      "272 [D loss: 0.999976] [G loss: 1.000060]\n",
      "273 [D loss: 0.999975] [G loss: 1.000047]\n",
      "274 [D loss: 0.999970] [G loss: 1.000071]\n",
      "275 [D loss: 0.999964] [G loss: 1.000060]\n",
      "276 [D loss: 0.999957] [G loss: 1.000066]\n",
      "277 [D loss: 0.999965] [G loss: 1.000055]\n",
      "278 [D loss: 0.999975] [G loss: 1.000057]\n",
      "279 [D loss: 0.999965] [G loss: 1.000037]\n",
      "280 [D loss: 0.999966] [G loss: 1.000063]\n",
      "281 [D loss: 0.999982] [G loss: 1.000060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 [D loss: 0.999966] [G loss: 1.000060]\n",
      "283 [D loss: 0.999973] [G loss: 1.000058]\n",
      "284 [D loss: 0.999970] [G loss: 1.000058]\n",
      "285 [D loss: 0.999975] [G loss: 1.000057]\n",
      "286 [D loss: 0.999956] [G loss: 1.000055]\n",
      "287 [D loss: 0.999973] [G loss: 1.000068]\n",
      "288 [D loss: 0.999968] [G loss: 1.000043]\n",
      "289 [D loss: 0.999971] [G loss: 1.000037]\n",
      "290 [D loss: 0.999974] [G loss: 1.000058]\n",
      "291 [D loss: 0.999966] [G loss: 1.000060]\n",
      "292 [D loss: 0.999965] [G loss: 1.000056]\n",
      "293 [D loss: 0.999977] [G loss: 1.000058]\n",
      "294 [D loss: 0.999961] [G loss: 1.000049]\n",
      "295 [D loss: 0.999968] [G loss: 1.000031]\n",
      "296 [D loss: 0.999972] [G loss: 1.000068]\n",
      "297 [D loss: 0.999950] [G loss: 1.000076]\n",
      "298 [D loss: 0.999971] [G loss: 1.000061]\n",
      "299 [D loss: 0.999963] [G loss: 1.000055]\n",
      "300 [D loss: 0.999985] [G loss: 1.000038]\n",
      "301 [D loss: 0.999964] [G loss: 1.000047]\n",
      "302 [D loss: 0.999969] [G loss: 1.000045]\n",
      "303 [D loss: 0.999972] [G loss: 1.000033]\n",
      "304 [D loss: 0.999961] [G loss: 1.000036]\n",
      "305 [D loss: 0.999973] [G loss: 1.000040]\n",
      "306 [D loss: 0.999960] [G loss: 1.000049]\n",
      "307 [D loss: 0.999976] [G loss: 1.000046]\n",
      "308 [D loss: 0.999975] [G loss: 1.000039]\n",
      "309 [D loss: 0.999978] [G loss: 1.000023]\n",
      "310 [D loss: 0.999966] [G loss: 1.000045]\n",
      "311 [D loss: 0.999972] [G loss: 1.000056]\n",
      "312 [D loss: 0.999960] [G loss: 1.000072]\n",
      "313 [D loss: 0.999971] [G loss: 1.000034]\n",
      "314 [D loss: 0.999973] [G loss: 1.000049]\n",
      "315 [D loss: 0.999962] [G loss: 1.000054]\n",
      "316 [D loss: 0.999974] [G loss: 1.000053]\n",
      "317 [D loss: 0.999981] [G loss: 1.000077]\n",
      "318 [D loss: 0.999959] [G loss: 1.000063]\n",
      "319 [D loss: 0.999968] [G loss: 1.000049]\n",
      "320 [D loss: 0.999966] [G loss: 1.000082]\n",
      "321 [D loss: 0.999959] [G loss: 1.000048]\n",
      "322 [D loss: 0.999973] [G loss: 1.000060]\n",
      "323 [D loss: 0.999960] [G loss: 1.000037]\n",
      "324 [D loss: 0.999967] [G loss: 1.000078]\n",
      "325 [D loss: 0.999966] [G loss: 1.000064]\n",
      "326 [D loss: 0.999961] [G loss: 1.000034]\n",
      "327 [D loss: 0.999968] [G loss: 1.000043]\n",
      "328 [D loss: 0.999985] [G loss: 1.000024]\n",
      "329 [D loss: 0.999946] [G loss: 1.000056]\n",
      "330 [D loss: 0.999965] [G loss: 1.000055]\n",
      "331 [D loss: 0.999974] [G loss: 1.000057]\n",
      "332 [D loss: 0.999953] [G loss: 1.000058]\n",
      "333 [D loss: 0.999968] [G loss: 1.000052]\n",
      "334 [D loss: 0.999964] [G loss: 1.000047]\n",
      "335 [D loss: 0.999960] [G loss: 1.000067]\n",
      "336 [D loss: 0.999958] [G loss: 1.000056]\n",
      "337 [D loss: 0.999959] [G loss: 1.000055]\n",
      "338 [D loss: 0.999969] [G loss: 1.000063]\n",
      "339 [D loss: 0.999964] [G loss: 1.000047]\n",
      "340 [D loss: 0.999962] [G loss: 1.000065]\n",
      "341 [D loss: 0.999975] [G loss: 1.000047]\n",
      "342 [D loss: 0.999962] [G loss: 1.000067]\n",
      "343 [D loss: 0.999963] [G loss: 1.000063]\n",
      "344 [D loss: 0.999951] [G loss: 1.000043]\n",
      "345 [D loss: 0.999965] [G loss: 1.000054]\n",
      "346 [D loss: 0.999969] [G loss: 1.000045]\n",
      "347 [D loss: 0.999962] [G loss: 1.000042]\n",
      "348 [D loss: 0.999974] [G loss: 1.000056]\n",
      "349 [D loss: 0.999971] [G loss: 1.000034]\n",
      "350 [D loss: 0.999958] [G loss: 1.000050]\n",
      "351 [D loss: 0.999975] [G loss: 1.000029]\n",
      "352 [D loss: 0.999983] [G loss: 1.000074]\n",
      "353 [D loss: 0.999971] [G loss: 1.000049]\n",
      "354 [D loss: 0.999965] [G loss: 1.000058]\n",
      "355 [D loss: 0.999958] [G loss: 1.000048]\n",
      "356 [D loss: 0.999973] [G loss: 1.000067]\n",
      "357 [D loss: 0.999972] [G loss: 1.000024]\n",
      "358 [D loss: 0.999981] [G loss: 1.000079]\n",
      "359 [D loss: 0.999964] [G loss: 1.000060]\n",
      "360 [D loss: 0.999956] [G loss: 1.000060]\n",
      "361 [D loss: 0.999971] [G loss: 1.000035]\n",
      "362 [D loss: 0.999962] [G loss: 1.000045]\n",
      "363 [D loss: 0.999992] [G loss: 1.000040]\n",
      "364 [D loss: 0.999969] [G loss: 1.000074]\n",
      "365 [D loss: 0.999973] [G loss: 1.000065]\n",
      "366 [D loss: 0.999962] [G loss: 1.000058]\n",
      "367 [D loss: 0.999970] [G loss: 1.000032]\n",
      "368 [D loss: 0.999968] [G loss: 1.000034]\n",
      "369 [D loss: 0.999982] [G loss: 1.000057]\n",
      "370 [D loss: 0.999972] [G loss: 1.000056]\n",
      "371 [D loss: 0.999986] [G loss: 1.000058]\n",
      "372 [D loss: 0.999980] [G loss: 1.000054]\n",
      "373 [D loss: 0.999978] [G loss: 1.000049]\n",
      "374 [D loss: 0.999976] [G loss: 1.000017]\n",
      "375 [D loss: 0.999947] [G loss: 1.000045]\n",
      "376 [D loss: 0.999953] [G loss: 1.000063]\n",
      "377 [D loss: 0.999973] [G loss: 1.000048]\n",
      "378 [D loss: 0.999959] [G loss: 1.000025]\n",
      "379 [D loss: 0.999976] [G loss: 1.000039]\n",
      "380 [D loss: 0.999972] [G loss: 1.000045]\n",
      "381 [D loss: 0.999988] [G loss: 1.000073]\n",
      "382 [D loss: 0.999966] [G loss: 1.000033]\n",
      "383 [D loss: 0.999971] [G loss: 1.000046]\n",
      "384 [D loss: 0.999956] [G loss: 1.000059]\n",
      "385 [D loss: 0.999982] [G loss: 1.000042]\n",
      "386 [D loss: 0.999965] [G loss: 1.000011]\n",
      "387 [D loss: 0.999960] [G loss: 1.000039]\n",
      "388 [D loss: 0.999967] [G loss: 1.000032]\n",
      "389 [D loss: 0.999982] [G loss: 1.000046]\n",
      "390 [D loss: 0.999963] [G loss: 1.000019]\n",
      "391 [D loss: 0.999981] [G loss: 1.000029]\n",
      "392 [D loss: 0.999985] [G loss: 1.000027]\n",
      "393 [D loss: 0.999947] [G loss: 1.000031]\n",
      "394 [D loss: 0.999974] [G loss: 1.000033]\n",
      "395 [D loss: 0.999971] [G loss: 1.000040]\n",
      "396 [D loss: 0.999953] [G loss: 1.000038]\n",
      "397 [D loss: 0.999968] [G loss: 1.000016]\n",
      "398 [D loss: 0.999965] [G loss: 1.000032]\n",
      "399 [D loss: 0.999965] [G loss: 1.000064]\n",
      "400 [D loss: 0.999956] [G loss: 1.000045]\n",
      "401 [D loss: 0.999961] [G loss: 1.000017]\n",
      "402 [D loss: 0.999960] [G loss: 1.000045]\n",
      "403 [D loss: 0.999989] [G loss: 1.000010]\n",
      "404 [D loss: 0.999959] [G loss: 1.000037]\n",
      "405 [D loss: 0.999964] [G loss: 1.000011]\n",
      "406 [D loss: 0.999980] [G loss: 1.000010]\n",
      "407 [D loss: 0.999963] [G loss: 1.000032]\n",
      "408 [D loss: 0.999988] [G loss: 1.000045]\n",
      "409 [D loss: 0.999977] [G loss: 1.000039]\n",
      "410 [D loss: 0.999969] [G loss: 1.000061]\n",
      "411 [D loss: 0.999974] [G loss: 1.000041]\n",
      "412 [D loss: 0.999952] [G loss: 1.000040]\n",
      "413 [D loss: 0.999961] [G loss: 1.000074]\n",
      "414 [D loss: 0.999972] [G loss: 1.000030]\n",
      "415 [D loss: 0.999981] [G loss: 1.000054]\n",
      "416 [D loss: 0.999977] [G loss: 1.000051]\n",
      "417 [D loss: 0.999967] [G loss: 1.000043]\n",
      "418 [D loss: 0.999956] [G loss: 1.000045]\n",
      "419 [D loss: 0.999977] [G loss: 1.000033]\n",
      "420 [D loss: 0.999962] [G loss: 1.000039]\n",
      "421 [D loss: 0.999959] [G loss: 1.000046]\n",
      "422 [D loss: 0.999995] [G loss: 1.000023]\n",
      "423 [D loss: 0.999960] [G loss: 1.000054]\n",
      "424 [D loss: 0.999976] [G loss: 1.000066]\n",
      "425 [D loss: 0.999966] [G loss: 1.000037]\n",
      "426 [D loss: 0.999970] [G loss: 1.000040]\n",
      "427 [D loss: 0.999979] [G loss: 1.000050]\n",
      "428 [D loss: 0.999967] [G loss: 1.000052]\n",
      "429 [D loss: 0.999963] [G loss: 1.000053]\n",
      "430 [D loss: 0.999974] [G loss: 1.000034]\n",
      "431 [D loss: 0.999981] [G loss: 1.000066]\n",
      "432 [D loss: 0.999962] [G loss: 1.000030]\n",
      "433 [D loss: 0.999967] [G loss: 1.000053]\n",
      "434 [D loss: 0.999971] [G loss: 1.000044]\n",
      "435 [D loss: 0.999971] [G loss: 1.000050]\n",
      "436 [D loss: 0.999968] [G loss: 1.000046]\n",
      "437 [D loss: 0.999973] [G loss: 1.000028]\n",
      "438 [D loss: 0.999968] [G loss: 1.000041]\n",
      "439 [D loss: 0.999967] [G loss: 1.000038]\n",
      "440 [D loss: 0.999982] [G loss: 1.000047]\n",
      "441 [D loss: 0.999962] [G loss: 1.000040]\n",
      "442 [D loss: 0.999968] [G loss: 1.000057]\n",
      "443 [D loss: 0.999982] [G loss: 1.000032]\n",
      "444 [D loss: 0.999971] [G loss: 1.000030]\n",
      "445 [D loss: 0.999968] [G loss: 1.000053]\n",
      "446 [D loss: 0.999969] [G loss: 1.000036]\n",
      "447 [D loss: 0.999975] [G loss: 1.000060]\n",
      "448 [D loss: 0.999961] [G loss: 1.000037]\n",
      "449 [D loss: 0.999968] [G loss: 1.000062]\n",
      "450 [D loss: 0.999983] [G loss: 1.000037]\n",
      "451 [D loss: 0.999969] [G loss: 1.000058]\n",
      "452 [D loss: 0.999964] [G loss: 1.000046]\n",
      "453 [D loss: 0.999957] [G loss: 1.000052]\n",
      "454 [D loss: 0.999969] [G loss: 1.000037]\n",
      "455 [D loss: 0.999963] [G loss: 1.000053]\n",
      "456 [D loss: 0.999971] [G loss: 1.000029]\n",
      "457 [D loss: 0.999987] [G loss: 1.000039]\n",
      "458 [D loss: 0.999969] [G loss: 1.000042]\n",
      "459 [D loss: 0.999970] [G loss: 1.000057]\n",
      "460 [D loss: 0.999966] [G loss: 1.000017]\n",
      "461 [D loss: 0.999967] [G loss: 1.000026]\n",
      "462 [D loss: 0.999988] [G loss: 1.000064]\n",
      "463 [D loss: 0.999986] [G loss: 1.000056]\n",
      "464 [D loss: 0.999959] [G loss: 1.000027]\n",
      "465 [D loss: 0.999970] [G loss: 1.000035]\n",
      "466 [D loss: 0.999972] [G loss: 1.000028]\n",
      "467 [D loss: 0.999957] [G loss: 1.000026]\n",
      "468 [D loss: 0.999970] [G loss: 1.000023]\n",
      "469 [D loss: 0.999971] [G loss: 1.000038]\n",
      "470 [D loss: 0.999981] [G loss: 1.000033]\n",
      "471 [D loss: 0.999957] [G loss: 1.000054]\n",
      "472 [D loss: 0.999950] [G loss: 1.000052]\n",
      "473 [D loss: 0.999977] [G loss: 1.000031]\n",
      "474 [D loss: 0.999983] [G loss: 1.000039]\n",
      "475 [D loss: 0.999965] [G loss: 1.000026]\n",
      "476 [D loss: 0.999962] [G loss: 1.000030]\n",
      "477 [D loss: 0.999972] [G loss: 1.000024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 [D loss: 0.999972] [G loss: 1.000037]\n",
      "479 [D loss: 0.999981] [G loss: 1.000032]\n",
      "480 [D loss: 0.999990] [G loss: 1.000017]\n",
      "481 [D loss: 0.999971] [G loss: 1.000023]\n",
      "482 [D loss: 0.999970] [G loss: 1.000023]\n",
      "483 [D loss: 0.999975] [G loss: 1.000054]\n",
      "484 [D loss: 0.999967] [G loss: 1.000048]\n",
      "485 [D loss: 0.999964] [G loss: 1.000040]\n",
      "486 [D loss: 0.999984] [G loss: 1.000048]\n",
      "487 [D loss: 0.999976] [G loss: 1.000017]\n",
      "488 [D loss: 0.999964] [G loss: 1.000018]\n",
      "489 [D loss: 0.999979] [G loss: 1.000047]\n",
      "490 [D loss: 0.999965] [G loss: 1.000066]\n",
      "491 [D loss: 0.999970] [G loss: 1.000020]\n",
      "492 [D loss: 0.999963] [G loss: 1.000053]\n",
      "493 [D loss: 0.999964] [G loss: 1.000037]\n",
      "494 [D loss: 0.999981] [G loss: 1.000040]\n",
      "495 [D loss: 0.999984] [G loss: 1.000018]\n",
      "496 [D loss: 0.999982] [G loss: 1.000015]\n",
      "497 [D loss: 0.999977] [G loss: 1.000033]\n",
      "498 [D loss: 0.999986] [G loss: 1.000021]\n",
      "499 [D loss: 0.999993] [G loss: 1.000028]\n",
      "500 [D loss: 0.999970] [G loss: 1.000044]\n",
      "501 [D loss: 0.999964] [G loss: 1.000042]\n",
      "502 [D loss: 0.999969] [G loss: 1.000020]\n",
      "503 [D loss: 0.999979] [G loss: 1.000036]\n",
      "504 [D loss: 0.999980] [G loss: 1.000035]\n",
      "505 [D loss: 0.999970] [G loss: 1.000034]\n",
      "506 [D loss: 0.999971] [G loss: 1.000063]\n",
      "507 [D loss: 0.999976] [G loss: 1.000031]\n",
      "508 [D loss: 0.999974] [G loss: 1.000041]\n",
      "509 [D loss: 0.999988] [G loss: 1.000017]\n",
      "510 [D loss: 0.999992] [G loss: 0.999994]\n",
      "511 [D loss: 0.999991] [G loss: 1.000028]\n",
      "512 [D loss: 0.999987] [G loss: 1.000020]\n",
      "513 [D loss: 0.999972] [G loss: 1.000041]\n",
      "514 [D loss: 0.999970] [G loss: 1.000013]\n",
      "515 [D loss: 0.999978] [G loss: 1.000015]\n",
      "516 [D loss: 0.999982] [G loss: 1.000018]\n",
      "517 [D loss: 0.999974] [G loss: 1.000022]\n",
      "518 [D loss: 0.999985] [G loss: 1.000000]\n",
      "519 [D loss: 0.999968] [G loss: 1.000023]\n",
      "520 [D loss: 0.999950] [G loss: 1.000046]\n",
      "521 [D loss: 0.999981] [G loss: 1.000042]\n",
      "522 [D loss: 0.999973] [G loss: 1.000022]\n",
      "523 [D loss: 0.999972] [G loss: 1.000039]\n",
      "524 [D loss: 0.999955] [G loss: 1.000056]\n",
      "525 [D loss: 0.999980] [G loss: 1.000047]\n",
      "526 [D loss: 0.999980] [G loss: 1.000025]\n",
      "527 [D loss: 0.999940] [G loss: 1.000043]\n",
      "528 [D loss: 0.999968] [G loss: 1.000031]\n",
      "529 [D loss: 0.999978] [G loss: 1.000017]\n",
      "530 [D loss: 0.999963] [G loss: 1.000021]\n",
      "531 [D loss: 0.999994] [G loss: 1.000021]\n",
      "532 [D loss: 0.999997] [G loss: 1.000022]\n",
      "533 [D loss: 0.999981] [G loss: 1.000015]\n",
      "534 [D loss: 0.999964] [G loss: 1.000033]\n",
      "535 [D loss: 0.999986] [G loss: 1.000037]\n",
      "536 [D loss: 0.999978] [G loss: 1.000061]\n",
      "537 [D loss: 0.999980] [G loss: 1.000054]\n",
      "538 [D loss: 0.999981] [G loss: 1.000045]\n",
      "539 [D loss: 0.999978] [G loss: 1.000028]\n",
      "540 [D loss: 0.999974] [G loss: 1.000032]\n",
      "541 [D loss: 1.000006] [G loss: 0.999994]\n",
      "542 [D loss: 0.999972] [G loss: 1.000041]\n",
      "543 [D loss: 0.999966] [G loss: 1.000036]\n",
      "544 [D loss: 0.999962] [G loss: 1.000022]\n",
      "545 [D loss: 0.999978] [G loss: 1.000022]\n",
      "546 [D loss: 0.999991] [G loss: 1.000036]\n",
      "547 [D loss: 1.000008] [G loss: 0.999985]\n",
      "548 [D loss: 0.999976] [G loss: 1.000028]\n",
      "549 [D loss: 0.999986] [G loss: 0.999991]\n",
      "550 [D loss: 0.999970] [G loss: 1.000050]\n",
      "551 [D loss: 0.999975] [G loss: 1.000023]\n",
      "552 [D loss: 0.999973] [G loss: 1.000037]\n",
      "553 [D loss: 0.999976] [G loss: 1.000022]\n",
      "554 [D loss: 0.999969] [G loss: 1.000021]\n",
      "555 [D loss: 0.999971] [G loss: 1.000039]\n",
      "556 [D loss: 0.999971] [G loss: 1.000039]\n",
      "557 [D loss: 0.999958] [G loss: 1.000017]\n",
      "558 [D loss: 0.999963] [G loss: 1.000046]\n",
      "559 [D loss: 0.999993] [G loss: 1.000024]\n",
      "560 [D loss: 0.999958] [G loss: 1.000061]\n",
      "561 [D loss: 0.999982] [G loss: 1.000023]\n",
      "562 [D loss: 0.999975] [G loss: 0.999983]\n",
      "563 [D loss: 0.999981] [G loss: 1.000022]\n",
      "564 [D loss: 0.999976] [G loss: 1.000034]\n",
      "565 [D loss: 0.999956] [G loss: 1.000002]\n",
      "566 [D loss: 0.999967] [G loss: 1.000060]\n",
      "567 [D loss: 0.999966] [G loss: 1.000021]\n",
      "568 [D loss: 0.999979] [G loss: 1.000009]\n",
      "569 [D loss: 0.999989] [G loss: 0.999996]\n",
      "570 [D loss: 0.999979] [G loss: 1.000008]\n",
      "571 [D loss: 0.999954] [G loss: 1.000033]\n",
      "572 [D loss: 0.999963] [G loss: 1.000022]\n",
      "573 [D loss: 0.999967] [G loss: 1.000014]\n",
      "574 [D loss: 0.999979] [G loss: 1.000056]\n",
      "575 [D loss: 0.999976] [G loss: 1.000026]\n",
      "576 [D loss: 0.999968] [G loss: 1.000009]\n",
      "577 [D loss: 0.999976] [G loss: 1.000003]\n",
      "578 [D loss: 0.999988] [G loss: 1.000025]\n",
      "579 [D loss: 1.000003] [G loss: 0.999998]\n",
      "580 [D loss: 0.999951] [G loss: 1.000016]\n",
      "581 [D loss: 1.000006] [G loss: 0.999989]\n",
      "582 [D loss: 0.999976] [G loss: 1.000009]\n",
      "583 [D loss: 0.999983] [G loss: 1.000014]\n",
      "584 [D loss: 0.999995] [G loss: 1.000030]\n",
      "585 [D loss: 0.999962] [G loss: 0.999987]\n",
      "586 [D loss: 0.999975] [G loss: 1.000022]\n",
      "587 [D loss: 0.999988] [G loss: 0.999996]\n",
      "588 [D loss: 1.000012] [G loss: 0.999997]\n",
      "589 [D loss: 0.999997] [G loss: 1.000023]\n",
      "590 [D loss: 0.999968] [G loss: 1.000047]\n",
      "591 [D loss: 0.999967] [G loss: 1.000005]\n",
      "592 [D loss: 0.999978] [G loss: 1.000046]\n",
      "593 [D loss: 1.000014] [G loss: 1.000026]\n",
      "594 [D loss: 0.999981] [G loss: 1.000026]\n",
      "595 [D loss: 1.000008] [G loss: 1.000030]\n",
      "596 [D loss: 0.999966] [G loss: 1.000021]\n",
      "597 [D loss: 1.000002] [G loss: 0.999990]\n",
      "598 [D loss: 0.999966] [G loss: 1.000046]\n",
      "599 [D loss: 1.000002] [G loss: 1.000020]\n",
      "600 [D loss: 0.999976] [G loss: 1.000031]\n",
      "601 [D loss: 0.999976] [G loss: 0.999974]\n",
      "602 [D loss: 1.000003] [G loss: 0.999978]\n",
      "603 [D loss: 0.999968] [G loss: 1.000005]\n",
      "604 [D loss: 0.999999] [G loss: 1.000008]\n",
      "605 [D loss: 0.999953] [G loss: 1.000033]\n",
      "606 [D loss: 0.999980] [G loss: 1.000022]\n",
      "607 [D loss: 1.000016] [G loss: 0.999990]\n",
      "608 [D loss: 0.999979] [G loss: 1.000012]\n",
      "609 [D loss: 0.999984] [G loss: 1.000038]\n",
      "610 [D loss: 0.999966] [G loss: 0.999979]\n",
      "611 [D loss: 0.999979] [G loss: 1.000015]\n",
      "612 [D loss: 0.999990] [G loss: 0.999990]\n",
      "613 [D loss: 0.999981] [G loss: 0.999970]\n",
      "614 [D loss: 0.999975] [G loss: 1.000004]\n",
      "615 [D loss: 0.999976] [G loss: 1.000018]\n",
      "616 [D loss: 1.000002] [G loss: 0.999975]\n",
      "617 [D loss: 0.999982] [G loss: 0.999982]\n",
      "618 [D loss: 0.999986] [G loss: 0.999990]\n",
      "619 [D loss: 0.999963] [G loss: 0.999998]\n",
      "620 [D loss: 0.999987] [G loss: 1.000032]\n",
      "621 [D loss: 0.999957] [G loss: 1.000040]\n",
      "622 [D loss: 0.999980] [G loss: 1.000005]\n",
      "623 [D loss: 0.999997] [G loss: 1.000019]\n",
      "624 [D loss: 0.999996] [G loss: 0.999963]\n",
      "625 [D loss: 0.999994] [G loss: 0.999983]\n",
      "626 [D loss: 0.999982] [G loss: 0.999993]\n",
      "627 [D loss: 1.000005] [G loss: 0.999990]\n",
      "628 [D loss: 1.000023] [G loss: 1.000019]\n",
      "629 [D loss: 0.999973] [G loss: 1.000031]\n",
      "630 [D loss: 0.999973] [G loss: 0.999996]\n",
      "631 [D loss: 1.000033] [G loss: 0.999962]\n",
      "632 [D loss: 0.999995] [G loss: 0.999980]\n",
      "633 [D loss: 0.999989] [G loss: 1.000003]\n",
      "634 [D loss: 1.000020] [G loss: 0.999954]\n",
      "635 [D loss: 0.999962] [G loss: 1.000026]\n",
      "636 [D loss: 0.999980] [G loss: 1.000002]\n",
      "637 [D loss: 0.999960] [G loss: 0.999984]\n",
      "638 [D loss: 0.999966] [G loss: 0.999979]\n",
      "639 [D loss: 0.999979] [G loss: 0.999947]\n",
      "640 [D loss: 0.999970] [G loss: 0.999960]\n",
      "641 [D loss: 0.999985] [G loss: 0.999928]\n",
      "642 [D loss: 1.000003] [G loss: 0.999983]\n",
      "643 [D loss: 1.000006] [G loss: 0.999989]\n",
      "644 [D loss: 0.999974] [G loss: 1.000016]\n",
      "645 [D loss: 1.000051] [G loss: 0.999966]\n",
      "646 [D loss: 0.999980] [G loss: 0.999998]\n",
      "647 [D loss: 1.000000] [G loss: 0.999913]\n",
      "648 [D loss: 0.999998] [G loss: 0.999953]\n",
      "649 [D loss: 0.999989] [G loss: 0.999957]\n",
      "650 [D loss: 0.999960] [G loss: 0.999982]\n",
      "651 [D loss: 0.999983] [G loss: 0.999933]\n",
      "652 [D loss: 1.000006] [G loss: 1.000000]\n",
      "653 [D loss: 1.000005] [G loss: 0.999995]\n",
      "654 [D loss: 1.000046] [G loss: 0.999947]\n",
      "655 [D loss: 0.999969] [G loss: 0.999982]\n",
      "656 [D loss: 1.000001] [G loss: 0.999956]\n",
      "657 [D loss: 1.000010] [G loss: 0.999975]\n",
      "658 [D loss: 0.999998] [G loss: 0.999998]\n",
      "659 [D loss: 0.999993] [G loss: 0.999955]\n",
      "660 [D loss: 0.999987] [G loss: 0.999948]\n",
      "661 [D loss: 0.999960] [G loss: 0.999934]\n",
      "662 [D loss: 1.000011] [G loss: 0.999952]\n",
      "663 [D loss: 1.000039] [G loss: 0.999965]\n",
      "664 [D loss: 1.000051] [G loss: 0.999940]\n",
      "665 [D loss: 0.999983] [G loss: 0.999970]\n",
      "666 [D loss: 0.999991] [G loss: 0.999995]\n",
      "667 [D loss: 0.999991] [G loss: 1.000026]\n",
      "668 [D loss: 0.999993] [G loss: 0.999985]\n",
      "669 [D loss: 0.999986] [G loss: 0.999981]\n",
      "670 [D loss: 1.000001] [G loss: 0.999958]\n",
      "671 [D loss: 0.999990] [G loss: 0.999950]\n",
      "672 [D loss: 0.999975] [G loss: 0.999971]\n",
      "673 [D loss: 0.999980] [G loss: 0.999971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 [D loss: 1.000048] [G loss: 0.999986]\n",
      "675 [D loss: 1.000021] [G loss: 0.999928]\n",
      "676 [D loss: 1.000005] [G loss: 0.999983]\n",
      "677 [D loss: 0.999990] [G loss: 1.000008]\n",
      "678 [D loss: 1.000007] [G loss: 0.999972]\n",
      "679 [D loss: 1.000005] [G loss: 0.999968]\n",
      "680 [D loss: 0.999991] [G loss: 0.999986]\n",
      "681 [D loss: 0.999994] [G loss: 0.999921]\n",
      "682 [D loss: 1.000021] [G loss: 0.999944]\n",
      "683 [D loss: 0.999958] [G loss: 0.999988]\n",
      "684 [D loss: 0.999989] [G loss: 0.999927]\n",
      "685 [D loss: 1.000026] [G loss: 0.999948]\n",
      "686 [D loss: 1.000022] [G loss: 0.999944]\n",
      "687 [D loss: 1.000030] [G loss: 1.000010]\n",
      "688 [D loss: 1.000024] [G loss: 0.999997]\n",
      "689 [D loss: 0.999949] [G loss: 1.000061]\n",
      "690 [D loss: 0.999977] [G loss: 0.999971]\n",
      "691 [D loss: 1.000046] [G loss: 0.999947]\n",
      "692 [D loss: 0.999987] [G loss: 1.000005]\n",
      "693 [D loss: 1.000031] [G loss: 0.999923]\n",
      "694 [D loss: 1.000000] [G loss: 0.999934]\n",
      "695 [D loss: 1.000012] [G loss: 0.999983]\n",
      "696 [D loss: 1.000010] [G loss: 1.000011]\n",
      "697 [D loss: 1.000012] [G loss: 0.999950]\n",
      "698 [D loss: 1.000013] [G loss: 0.999987]\n",
      "699 [D loss: 0.999995] [G loss: 0.999980]\n",
      "700 [D loss: 1.000029] [G loss: 0.999959]\n",
      "701 [D loss: 1.000017] [G loss: 0.999951]\n",
      "702 [D loss: 1.000013] [G loss: 0.999939]\n",
      "703 [D loss: 0.999999] [G loss: 0.999888]\n",
      "704 [D loss: 0.999983] [G loss: 0.999944]\n",
      "705 [D loss: 1.000031] [G loss: 0.999966]\n",
      "706 [D loss: 0.999997] [G loss: 0.999964]\n",
      "707 [D loss: 1.000003] [G loss: 0.999979]\n",
      "708 [D loss: 0.999975] [G loss: 1.000023]\n",
      "709 [D loss: 1.000017] [G loss: 0.999985]\n",
      "710 [D loss: 0.999970] [G loss: 1.000037]\n",
      "711 [D loss: 1.000035] [G loss: 0.999918]\n",
      "712 [D loss: 1.000031] [G loss: 0.999882]\n",
      "713 [D loss: 0.999987] [G loss: 0.999923]\n",
      "714 [D loss: 0.999982] [G loss: 0.999923]\n",
      "715 [D loss: 0.999956] [G loss: 0.999883]\n",
      "716 [D loss: 1.000006] [G loss: 1.000009]\n",
      "717 [D loss: 1.000038] [G loss: 0.999974]\n",
      "718 [D loss: 1.000000] [G loss: 0.999977]\n",
      "719 [D loss: 1.000023] [G loss: 0.999938]\n",
      "720 [D loss: 1.000006] [G loss: 0.999943]\n",
      "721 [D loss: 1.000007] [G loss: 0.999878]\n",
      "722 [D loss: 0.999973] [G loss: 0.999869]\n",
      "723 [D loss: 0.999969] [G loss: 0.999974]\n",
      "724 [D loss: 1.000058] [G loss: 0.999889]\n",
      "725 [D loss: 1.000052] [G loss: 0.999926]\n",
      "726 [D loss: 0.999957] [G loss: 0.999991]\n",
      "727 [D loss: 0.999952] [G loss: 1.000020]\n",
      "728 [D loss: 1.000031] [G loss: 1.000009]\n",
      "729 [D loss: 1.000007] [G loss: 0.999890]\n",
      "730 [D loss: 0.999982] [G loss: 0.999929]\n",
      "731 [D loss: 1.000036] [G loss: 0.999930]\n",
      "732 [D loss: 0.999983] [G loss: 0.999877]\n",
      "733 [D loss: 1.000075] [G loss: 0.999934]\n",
      "734 [D loss: 1.000038] [G loss: 0.999912]\n",
      "735 [D loss: 1.000024] [G loss: 0.999918]\n",
      "736 [D loss: 0.999963] [G loss: 0.999880]\n",
      "737 [D loss: 1.000021] [G loss: 0.999943]\n",
      "738 [D loss: 1.000031] [G loss: 0.999824]\n",
      "739 [D loss: 0.999977] [G loss: 0.999961]\n",
      "740 [D loss: 1.000019] [G loss: 0.999994]\n",
      "741 [D loss: 0.999960] [G loss: 1.000022]\n",
      "742 [D loss: 1.000031] [G loss: 0.999955]\n",
      "743 [D loss: 1.000006] [G loss: 0.999908]\n",
      "744 [D loss: 1.000016] [G loss: 0.999972]\n",
      "745 [D loss: 0.999978] [G loss: 1.000035]\n",
      "746 [D loss: 0.999960] [G loss: 0.999968]\n",
      "747 [D loss: 1.000025] [G loss: 0.999968]\n",
      "748 [D loss: 0.999975] [G loss: 0.999983]\n",
      "749 [D loss: 1.000040] [G loss: 0.999946]\n",
      "750 [D loss: 1.000023] [G loss: 0.999934]\n",
      "751 [D loss: 1.000049] [G loss: 0.999938]\n",
      "752 [D loss: 1.000011] [G loss: 0.999892]\n",
      "753 [D loss: 1.000048] [G loss: 0.999894]\n",
      "754 [D loss: 1.000000] [G loss: 0.999926]\n",
      "755 [D loss: 0.999974] [G loss: 0.999909]\n",
      "756 [D loss: 1.000033] [G loss: 0.999873]\n",
      "757 [D loss: 1.000004] [G loss: 1.000003]\n",
      "758 [D loss: 1.000019] [G loss: 0.999929]\n",
      "759 [D loss: 1.000053] [G loss: 0.999866]\n",
      "760 [D loss: 1.000077] [G loss: 0.999901]\n",
      "761 [D loss: 1.000015] [G loss: 0.999940]\n",
      "762 [D loss: 1.000053] [G loss: 0.999830]\n",
      "763 [D loss: 1.000070] [G loss: 0.999801]\n",
      "764 [D loss: 0.999988] [G loss: 0.999898]\n",
      "765 [D loss: 0.999984] [G loss: 1.000023]\n",
      "766 [D loss: 1.000081] [G loss: 0.999839]\n",
      "767 [D loss: 1.000006] [G loss: 0.999935]\n",
      "768 [D loss: 0.999963] [G loss: 1.000028]\n",
      "769 [D loss: 1.000047] [G loss: 0.999910]\n",
      "770 [D loss: 1.000004] [G loss: 0.999891]\n",
      "771 [D loss: 0.999992] [G loss: 0.999930]\n",
      "772 [D loss: 1.000002] [G loss: 0.999953]\n",
      "773 [D loss: 0.999999] [G loss: 0.999955]\n",
      "774 [D loss: 1.000022] [G loss: 0.999907]\n",
      "775 [D loss: 1.000029] [G loss: 0.999881]\n",
      "776 [D loss: 1.000029] [G loss: 0.999983]\n",
      "777 [D loss: 1.000014] [G loss: 0.999979]\n",
      "778 [D loss: 0.999986] [G loss: 0.999891]\n",
      "779 [D loss: 0.999970] [G loss: 0.999986]\n",
      "780 [D loss: 0.999974] [G loss: 0.999862]\n",
      "781 [D loss: 1.000070] [G loss: 0.999839]\n",
      "782 [D loss: 0.999994] [G loss: 1.000019]\n",
      "783 [D loss: 0.999991] [G loss: 1.000017]\n",
      "784 [D loss: 0.999989] [G loss: 0.999971]\n",
      "785 [D loss: 0.999990] [G loss: 0.999908]\n",
      "786 [D loss: 1.000033] [G loss: 0.999881]\n",
      "787 [D loss: 1.000051] [G loss: 0.999864]\n",
      "788 [D loss: 1.000011] [G loss: 0.999869]\n",
      "789 [D loss: 0.999993] [G loss: 1.000007]\n",
      "790 [D loss: 1.000052] [G loss: 0.999995]\n",
      "791 [D loss: 1.000006] [G loss: 0.999960]\n",
      "792 [D loss: 1.000044] [G loss: 0.999865]\n",
      "793 [D loss: 1.000062] [G loss: 0.999853]\n",
      "794 [D loss: 1.000030] [G loss: 0.999901]\n",
      "795 [D loss: 0.999948] [G loss: 0.999913]\n",
      "796 [D loss: 0.999973] [G loss: 0.999917]\n",
      "797 [D loss: 1.000037] [G loss: 0.999897]\n",
      "798 [D loss: 0.999987] [G loss: 0.999975]\n",
      "799 [D loss: 1.000005] [G loss: 0.999945]\n",
      "800 [D loss: 1.000046] [G loss: 0.999890]\n",
      "801 [D loss: 0.999991] [G loss: 0.999888]\n",
      "802 [D loss: 1.000024] [G loss: 0.999952]\n",
      "803 [D loss: 1.000004] [G loss: 0.999925]\n",
      "804 [D loss: 0.999962] [G loss: 0.999909]\n",
      "805 [D loss: 0.999990] [G loss: 0.999812]\n",
      "806 [D loss: 1.000015] [G loss: 0.999921]\n",
      "807 [D loss: 1.000027] [G loss: 0.999949]\n",
      "808 [D loss: 1.000002] [G loss: 0.999919]\n",
      "809 [D loss: 1.000131] [G loss: 0.999822]\n",
      "810 [D loss: 1.000002] [G loss: 1.000003]\n",
      "811 [D loss: 0.999983] [G loss: 0.999906]\n",
      "812 [D loss: 1.000015] [G loss: 0.999944]\n",
      "813 [D loss: 1.000111] [G loss: 0.999881]\n",
      "814 [D loss: 1.000076] [G loss: 0.999816]\n",
      "815 [D loss: 1.000060] [G loss: 0.999803]\n",
      "816 [D loss: 0.999993] [G loss: 0.999894]\n",
      "817 [D loss: 1.000005] [G loss: 0.999974]\n",
      "818 [D loss: 1.000040] [G loss: 0.999951]\n",
      "819 [D loss: 0.999985] [G loss: 0.999921]\n",
      "820 [D loss: 1.000023] [G loss: 0.999994]\n",
      "821 [D loss: 0.999998] [G loss: 0.999898]\n",
      "822 [D loss: 1.000007] [G loss: 0.999906]\n",
      "823 [D loss: 0.999923] [G loss: 0.999956]\n",
      "824 [D loss: 0.999975] [G loss: 0.999869]\n",
      "825 [D loss: 1.000016] [G loss: 0.999880]\n",
      "826 [D loss: 1.000004] [G loss: 1.000069]\n",
      "827 [D loss: 1.000025] [G loss: 1.000056]\n",
      "828 [D loss: 1.000048] [G loss: 0.999968]\n",
      "829 [D loss: 1.000056] [G loss: 0.999905]\n",
      "830 [D loss: 1.000034] [G loss: 0.999946]\n",
      "831 [D loss: 1.000042] [G loss: 0.999885]\n",
      "832 [D loss: 1.000050] [G loss: 0.999903]\n",
      "833 [D loss: 1.000099] [G loss: 0.999743]\n",
      "834 [D loss: 1.000032] [G loss: 0.999809]\n",
      "835 [D loss: 0.999969] [G loss: 0.999823]\n",
      "836 [D loss: 0.999987] [G loss: 0.999957]\n",
      "837 [D loss: 0.999991] [G loss: 0.999906]\n",
      "838 [D loss: 1.000033] [G loss: 0.999844]\n",
      "839 [D loss: 0.999984] [G loss: 0.999964]\n",
      "840 [D loss: 0.999994] [G loss: 0.999992]\n",
      "841 [D loss: 0.999968] [G loss: 0.999969]\n",
      "842 [D loss: 1.000139] [G loss: 0.999887]\n",
      "843 [D loss: 0.999950] [G loss: 0.999938]\n",
      "844 [D loss: 1.000008] [G loss: 0.999871]\n",
      "845 [D loss: 0.999929] [G loss: 0.999907]\n",
      "846 [D loss: 1.000068] [G loss: 0.999864]\n",
      "847 [D loss: 1.000051] [G loss: 0.999907]\n",
      "848 [D loss: 1.000036] [G loss: 0.999882]\n",
      "849 [D loss: 1.000033] [G loss: 0.999877]\n",
      "850 [D loss: 1.000021] [G loss: 0.999947]\n",
      "851 [D loss: 1.000036] [G loss: 0.999923]\n",
      "852 [D loss: 1.000053] [G loss: 0.999931]\n",
      "853 [D loss: 0.999977] [G loss: 0.999835]\n",
      "854 [D loss: 0.999983] [G loss: 0.999934]\n",
      "855 [D loss: 1.000015] [G loss: 0.999851]\n",
      "856 [D loss: 1.000003] [G loss: 0.999913]\n",
      "857 [D loss: 1.000003] [G loss: 0.999930]\n",
      "858 [D loss: 1.000001] [G loss: 0.999805]\n",
      "859 [D loss: 1.000055] [G loss: 0.999888]\n",
      "860 [D loss: 1.000126] [G loss: 0.999887]\n",
      "861 [D loss: 1.000046] [G loss: 0.999930]\n",
      "862 [D loss: 1.000013] [G loss: 0.999933]\n",
      "863 [D loss: 1.000034] [G loss: 0.999880]\n",
      "864 [D loss: 1.000078] [G loss: 0.999846]\n",
      "865 [D loss: 1.000131] [G loss: 0.999827]\n",
      "866 [D loss: 1.000057] [G loss: 0.999764]\n",
      "867 [D loss: 0.999997] [G loss: 1.000002]\n",
      "868 [D loss: 1.000021] [G loss: 0.999939]\n",
      "869 [D loss: 1.000016] [G loss: 0.999869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870 [D loss: 1.000054] [G loss: 0.999849]\n",
      "871 [D loss: 1.000018] [G loss: 1.000007]\n",
      "872 [D loss: 1.000001] [G loss: 0.999929]\n",
      "873 [D loss: 1.000046] [G loss: 0.999939]\n",
      "874 [D loss: 1.000054] [G loss: 1.000005]\n",
      "875 [D loss: 1.000140] [G loss: 0.999892]\n",
      "876 [D loss: 1.000056] [G loss: 0.999945]\n",
      "877 [D loss: 1.000017] [G loss: 0.999935]\n",
      "878 [D loss: 1.000126] [G loss: 0.999872]\n",
      "879 [D loss: 1.000055] [G loss: 1.000029]\n",
      "880 [D loss: 0.999960] [G loss: 0.999997]\n",
      "881 [D loss: 1.000021] [G loss: 0.999852]\n",
      "882 [D loss: 1.000009] [G loss: 1.000002]\n",
      "883 [D loss: 1.000068] [G loss: 0.999892]\n",
      "884 [D loss: 1.000059] [G loss: 0.999898]\n",
      "885 [D loss: 1.000091] [G loss: 0.999988]\n",
      "886 [D loss: 1.000060] [G loss: 0.999732]\n",
      "887 [D loss: 1.000081] [G loss: 0.999793]\n",
      "888 [D loss: 1.000139] [G loss: 0.999881]\n",
      "889 [D loss: 1.000009] [G loss: 0.999915]\n",
      "890 [D loss: 0.999988] [G loss: 0.999947]\n",
      "891 [D loss: 1.000114] [G loss: 0.999839]\n",
      "892 [D loss: 1.000048] [G loss: 0.999948]\n",
      "893 [D loss: 1.000057] [G loss: 0.999874]\n",
      "894 [D loss: 1.000057] [G loss: 0.999866]\n",
      "895 [D loss: 1.000073] [G loss: 0.999952]\n",
      "896 [D loss: 1.000031] [G loss: 0.999801]\n",
      "897 [D loss: 1.000008] [G loss: 0.999979]\n",
      "898 [D loss: 1.000141] [G loss: 0.999840]\n",
      "899 [D loss: 1.000029] [G loss: 0.999929]\n",
      "900 [D loss: 1.000034] [G loss: 0.999852]\n",
      "901 [D loss: 1.000015] [G loss: 0.999921]\n",
      "902 [D loss: 1.000003] [G loss: 0.999888]\n",
      "903 [D loss: 1.000084] [G loss: 0.999852]\n",
      "904 [D loss: 1.000071] [G loss: 0.999952]\n",
      "905 [D loss: 1.000093] [G loss: 0.999732]\n",
      "906 [D loss: 0.999902] [G loss: 1.000014]\n",
      "907 [D loss: 1.000035] [G loss: 0.999907]\n",
      "908 [D loss: 1.000019] [G loss: 0.999880]\n",
      "909 [D loss: 1.000061] [G loss: 0.999883]\n",
      "910 [D loss: 1.000033] [G loss: 0.999985]\n",
      "911 [D loss: 1.000056] [G loss: 0.999807]\n",
      "912 [D loss: 1.000071] [G loss: 0.999920]\n",
      "913 [D loss: 1.000067] [G loss: 0.999875]\n",
      "914 [D loss: 1.000021] [G loss: 0.999808]\n",
      "915 [D loss: 1.000043] [G loss: 0.999728]\n",
      "916 [D loss: 1.000036] [G loss: 0.999817]\n",
      "917 [D loss: 1.000055] [G loss: 0.999795]\n",
      "918 [D loss: 1.000032] [G loss: 0.999977]\n",
      "919 [D loss: 1.000089] [G loss: 0.999884]\n",
      "920 [D loss: 1.000154] [G loss: 0.999854]\n",
      "921 [D loss: 1.000038] [G loss: 1.000018]\n",
      "922 [D loss: 1.000081] [G loss: 0.999842]\n",
      "923 [D loss: 1.000044] [G loss: 0.999900]\n",
      "924 [D loss: 1.000015] [G loss: 0.999920]\n",
      "925 [D loss: 1.000076] [G loss: 0.999796]\n",
      "926 [D loss: 1.000021] [G loss: 0.999895]\n",
      "927 [D loss: 1.000129] [G loss: 0.999817]\n",
      "928 [D loss: 1.000080] [G loss: 0.999796]\n",
      "929 [D loss: 0.999983] [G loss: 0.999764]\n",
      "930 [D loss: 1.000072] [G loss: 0.999891]\n",
      "931 [D loss: 1.000035] [G loss: 1.000003]\n",
      "932 [D loss: 1.000098] [G loss: 1.000004]\n",
      "933 [D loss: 0.999955] [G loss: 0.999983]\n",
      "934 [D loss: 0.999991] [G loss: 0.999990]\n",
      "935 [D loss: 1.000108] [G loss: 0.999874]\n",
      "936 [D loss: 0.999994] [G loss: 0.999911]\n",
      "937 [D loss: 1.000008] [G loss: 0.999981]\n",
      "938 [D loss: 1.000095] [G loss: 0.999861]\n",
      "939 [D loss: 0.999976] [G loss: 0.999966]\n",
      "940 [D loss: 1.000015] [G loss: 0.999925]\n",
      "941 [D loss: 1.000067] [G loss: 0.999906]\n",
      "942 [D loss: 0.999997] [G loss: 0.999951]\n",
      "943 [D loss: 1.000037] [G loss: 0.999978]\n",
      "944 [D loss: 0.999979] [G loss: 0.999920]\n",
      "945 [D loss: 1.000002] [G loss: 1.000093]\n",
      "946 [D loss: 1.000167] [G loss: 0.999879]\n",
      "947 [D loss: 1.000060] [G loss: 1.000009]\n",
      "948 [D loss: 0.999974] [G loss: 0.999887]\n",
      "949 [D loss: 1.000092] [G loss: 0.999907]\n",
      "950 [D loss: 1.000045] [G loss: 0.999774]\n",
      "951 [D loss: 0.999960] [G loss: 0.999953]\n",
      "952 [D loss: 1.000040] [G loss: 0.999870]\n",
      "953 [D loss: 1.000016] [G loss: 0.999868]\n",
      "954 [D loss: 0.999962] [G loss: 0.999955]\n",
      "955 [D loss: 0.999970] [G loss: 0.999906]\n",
      "956 [D loss: 1.000095] [G loss: 0.999933]\n",
      "957 [D loss: 1.000044] [G loss: 0.999969]\n",
      "958 [D loss: 0.999977] [G loss: 1.000022]\n",
      "959 [D loss: 1.000030] [G loss: 0.999944]\n",
      "960 [D loss: 1.000006] [G loss: 0.999968]\n",
      "961 [D loss: 1.000083] [G loss: 0.999848]\n",
      "962 [D loss: 1.000036] [G loss: 0.999967]\n",
      "963 [D loss: 1.000051] [G loss: 1.000025]\n",
      "964 [D loss: 1.000082] [G loss: 0.999904]\n",
      "965 [D loss: 0.999984] [G loss: 0.999917]\n",
      "966 [D loss: 0.999930] [G loss: 1.000035]\n",
      "967 [D loss: 0.999979] [G loss: 0.999876]\n",
      "968 [D loss: 1.000118] [G loss: 0.999924]\n",
      "969 [D loss: 1.000082] [G loss: 0.999980]\n",
      "970 [D loss: 1.000093] [G loss: 0.999869]\n",
      "971 [D loss: 1.000006] [G loss: 0.999929]\n",
      "972 [D loss: 1.000118] [G loss: 0.999842]\n",
      "973 [D loss: 1.000094] [G loss: 0.999873]\n",
      "974 [D loss: 1.000046] [G loss: 0.999793]\n",
      "975 [D loss: 1.000019] [G loss: 0.999847]\n",
      "976 [D loss: 1.000064] [G loss: 0.999720]\n",
      "977 [D loss: 1.000097] [G loss: 0.999904]\n",
      "978 [D loss: 1.000018] [G loss: 0.999905]\n",
      "979 [D loss: 0.999935] [G loss: 0.999947]\n",
      "980 [D loss: 1.000022] [G loss: 0.999856]\n",
      "981 [D loss: 1.000090] [G loss: 0.999844]\n",
      "982 [D loss: 1.000053] [G loss: 0.999826]\n",
      "983 [D loss: 0.999997] [G loss: 0.999926]\n",
      "984 [D loss: 1.000047] [G loss: 0.999780]\n",
      "985 [D loss: 1.000011] [G loss: 0.999779]\n",
      "986 [D loss: 1.000065] [G loss: 0.999723]\n",
      "987 [D loss: 1.000084] [G loss: 0.999788]\n",
      "988 [D loss: 0.999982] [G loss: 0.999944]\n",
      "989 [D loss: 1.000097] [G loss: 0.999894]\n",
      "990 [D loss: 1.000060] [G loss: 0.999896]\n",
      "991 [D loss: 1.000122] [G loss: 0.999798]\n",
      "992 [D loss: 1.000094] [G loss: 0.999832]\n",
      "993 [D loss: 1.000027] [G loss: 0.999937]\n",
      "994 [D loss: 1.000038] [G loss: 0.999963]\n",
      "995 [D loss: 1.000074] [G loss: 0.999893]\n",
      "996 [D loss: 1.000034] [G loss: 0.999925]\n",
      "997 [D loss: 1.000016] [G loss: 0.999874]\n",
      "998 [D loss: 1.000055] [G loss: 0.999795]\n",
      "999 [D loss: 1.000035] [G loss: 0.999833]\n",
      "1000 [D loss: 1.000016] [G loss: 1.000077]\n",
      "1001 [D loss: 1.000097] [G loss: 0.999891]\n",
      "1002 [D loss: 1.000058] [G loss: 0.999920]\n",
      "1003 [D loss: 0.999878] [G loss: 1.000006]\n",
      "1004 [D loss: 1.000071] [G loss: 0.999898]\n",
      "1005 [D loss: 1.000135] [G loss: 0.999848]\n",
      "1006 [D loss: 1.000003] [G loss: 0.999777]\n",
      "1007 [D loss: 1.000048] [G loss: 0.999969]\n",
      "1008 [D loss: 1.000056] [G loss: 0.999877]\n",
      "1009 [D loss: 1.000051] [G loss: 0.999776]\n",
      "1010 [D loss: 0.999981] [G loss: 0.999890]\n",
      "1011 [D loss: 1.000006] [G loss: 1.000004]\n",
      "1012 [D loss: 1.000036] [G loss: 0.999945]\n",
      "1013 [D loss: 1.000093] [G loss: 0.999810]\n",
      "1014 [D loss: 1.000026] [G loss: 0.999941]\n",
      "1015 [D loss: 1.000167] [G loss: 0.999993]\n",
      "1016 [D loss: 1.000015] [G loss: 0.999865]\n",
      "1017 [D loss: 1.000003] [G loss: 0.999790]\n",
      "1018 [D loss: 1.000083] [G loss: 0.999911]\n",
      "1019 [D loss: 1.000029] [G loss: 0.999633]\n",
      "1020 [D loss: 1.000113] [G loss: 0.999869]\n",
      "1021 [D loss: 1.000062] [G loss: 0.999809]\n",
      "1022 [D loss: 1.000029] [G loss: 0.999887]\n",
      "1023 [D loss: 1.000021] [G loss: 0.999879]\n",
      "1024 [D loss: 1.000067] [G loss: 0.999766]\n",
      "1025 [D loss: 1.000074] [G loss: 0.999863]\n",
      "1026 [D loss: 1.000011] [G loss: 1.000001]\n",
      "1027 [D loss: 0.999955] [G loss: 1.000053]\n",
      "1028 [D loss: 0.999990] [G loss: 0.999871]\n",
      "1029 [D loss: 0.999983] [G loss: 1.000017]\n",
      "1030 [D loss: 0.999976] [G loss: 1.000127]\n",
      "1031 [D loss: 1.000051] [G loss: 0.999895]\n",
      "1032 [D loss: 1.000152] [G loss: 0.999963]\n",
      "1033 [D loss: 0.999989] [G loss: 1.000032]\n",
      "1034 [D loss: 0.999994] [G loss: 0.999993]\n",
      "1035 [D loss: 0.999913] [G loss: 1.000033]\n",
      "1036 [D loss: 0.999924] [G loss: 0.999969]\n",
      "1037 [D loss: 1.000004] [G loss: 1.000071]\n",
      "1038 [D loss: 1.000131] [G loss: 0.999941]\n",
      "1039 [D loss: 1.000049] [G loss: 0.999925]\n",
      "1040 [D loss: 1.000098] [G loss: 0.999793]\n",
      "1041 [D loss: 1.000067] [G loss: 0.999958]\n",
      "1042 [D loss: 1.000133] [G loss: 0.999828]\n",
      "1043 [D loss: 0.999969] [G loss: 0.999899]\n",
      "1044 [D loss: 0.999928] [G loss: 0.999904]\n",
      "1045 [D loss: 1.000049] [G loss: 0.999854]\n",
      "1046 [D loss: 1.000051] [G loss: 0.999898]\n",
      "1047 [D loss: 0.999940] [G loss: 1.000045]\n",
      "1048 [D loss: 1.000032] [G loss: 0.999935]\n",
      "1049 [D loss: 1.000069] [G loss: 0.999984]\n",
      "1050 [D loss: 1.000045] [G loss: 0.999919]\n",
      "1051 [D loss: 1.000013] [G loss: 1.000052]\n",
      "1052 [D loss: 1.000054] [G loss: 0.999935]\n",
      "1053 [D loss: 1.000080] [G loss: 0.999927]\n",
      "1054 [D loss: 0.999996] [G loss: 0.999787]\n",
      "1055 [D loss: 1.000051] [G loss: 0.999862]\n",
      "1056 [D loss: 1.000043] [G loss: 0.999916]\n",
      "1057 [D loss: 1.000123] [G loss: 0.999890]\n",
      "1058 [D loss: 0.999986] [G loss: 0.999975]\n",
      "1059 [D loss: 1.000054] [G loss: 0.999971]\n",
      "1060 [D loss: 0.999984] [G loss: 0.999987]\n",
      "1061 [D loss: 1.000012] [G loss: 0.999834]\n",
      "1062 [D loss: 0.999971] [G loss: 0.999911]\n",
      "1063 [D loss: 1.000001] [G loss: 1.000008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064 [D loss: 1.000071] [G loss: 0.999901]\n",
      "1065 [D loss: 1.000025] [G loss: 0.999997]\n",
      "1066 [D loss: 1.000097] [G loss: 0.999842]\n",
      "1067 [D loss: 1.000001] [G loss: 0.999869]\n",
      "1068 [D loss: 0.999945] [G loss: 0.999992]\n",
      "1069 [D loss: 1.000063] [G loss: 0.999851]\n",
      "1070 [D loss: 1.000105] [G loss: 0.999756]\n",
      "1071 [D loss: 0.999950] [G loss: 0.999909]\n",
      "1072 [D loss: 1.000080] [G loss: 1.000044]\n",
      "1073 [D loss: 1.000013] [G loss: 1.000011]\n",
      "1074 [D loss: 0.999976] [G loss: 0.999908]\n",
      "1075 [D loss: 0.999968] [G loss: 1.000013]\n",
      "1076 [D loss: 1.000052] [G loss: 0.999864]\n",
      "1077 [D loss: 0.999996] [G loss: 0.999955]\n",
      "1078 [D loss: 1.000132] [G loss: 0.999819]\n",
      "1079 [D loss: 1.000046] [G loss: 0.999933]\n",
      "1080 [D loss: 1.000095] [G loss: 0.999867]\n",
      "1081 [D loss: 1.000085] [G loss: 0.999885]\n",
      "1082 [D loss: 1.000054] [G loss: 0.999975]\n",
      "1083 [D loss: 0.999976] [G loss: 0.999893]\n",
      "1084 [D loss: 0.999993] [G loss: 0.999854]\n",
      "1085 [D loss: 1.000040] [G loss: 0.999820]\n",
      "1086 [D loss: 1.000159] [G loss: 0.999856]\n",
      "1087 [D loss: 1.000129] [G loss: 0.999812]\n",
      "1088 [D loss: 1.000133] [G loss: 0.999829]\n",
      "1089 [D loss: 1.000089] [G loss: 0.999921]\n",
      "1090 [D loss: 1.000040] [G loss: 0.999863]\n",
      "1091 [D loss: 1.000101] [G loss: 0.999830]\n",
      "1092 [D loss: 1.000023] [G loss: 1.000042]\n",
      "1093 [D loss: 1.000039] [G loss: 0.999974]\n",
      "1094 [D loss: 0.999956] [G loss: 0.999975]\n",
      "1095 [D loss: 1.000037] [G loss: 0.999921]\n",
      "1096 [D loss: 1.000178] [G loss: 0.999822]\n",
      "1097 [D loss: 1.000122] [G loss: 1.000006]\n",
      "1098 [D loss: 1.000099] [G loss: 0.999979]\n",
      "1099 [D loss: 1.000109] [G loss: 0.999873]\n",
      "1100 [D loss: 0.999993] [G loss: 1.000076]\n",
      "1101 [D loss: 1.000071] [G loss: 0.999918]\n",
      "1102 [D loss: 1.000045] [G loss: 1.000066]\n",
      "1103 [D loss: 1.000033] [G loss: 0.999905]\n",
      "1104 [D loss: 0.999996] [G loss: 1.000020]\n",
      "1105 [D loss: 1.000065] [G loss: 0.999897]\n",
      "1106 [D loss: 1.000086] [G loss: 1.000000]\n",
      "1107 [D loss: 1.000159] [G loss: 0.999880]\n",
      "1108 [D loss: 0.999992] [G loss: 1.000056]\n",
      "1109 [D loss: 1.000011] [G loss: 0.999888]\n",
      "1110 [D loss: 1.000083] [G loss: 0.999891]\n",
      "1111 [D loss: 1.000094] [G loss: 0.999763]\n",
      "1112 [D loss: 1.000005] [G loss: 1.000005]\n",
      "1113 [D loss: 1.000038] [G loss: 0.999964]\n",
      "1114 [D loss: 1.000101] [G loss: 0.999776]\n",
      "1115 [D loss: 0.999974] [G loss: 0.999753]\n",
      "1116 [D loss: 1.000102] [G loss: 0.999857]\n",
      "1117 [D loss: 1.000050] [G loss: 0.999755]\n",
      "1118 [D loss: 1.000038] [G loss: 0.999806]\n",
      "1119 [D loss: 1.000056] [G loss: 0.999870]\n",
      "1120 [D loss: 0.999882] [G loss: 0.999971]\n",
      "1121 [D loss: 1.000113] [G loss: 0.999943]\n",
      "1122 [D loss: 0.999936] [G loss: 0.999914]\n",
      "1123 [D loss: 1.000035] [G loss: 0.999941]\n",
      "1124 [D loss: 1.000057] [G loss: 0.999921]\n",
      "1125 [D loss: 1.000110] [G loss: 0.999888]\n",
      "1126 [D loss: 0.999936] [G loss: 1.000089]\n",
      "1127 [D loss: 1.000141] [G loss: 0.999935]\n",
      "1128 [D loss: 0.999984] [G loss: 0.999901]\n",
      "1129 [D loss: 1.000082] [G loss: 1.000048]\n",
      "1130 [D loss: 1.000025] [G loss: 0.999948]\n",
      "1131 [D loss: 0.999959] [G loss: 1.000054]\n",
      "1132 [D loss: 1.000024] [G loss: 0.999948]\n",
      "1133 [D loss: 1.000068] [G loss: 0.999887]\n",
      "1134 [D loss: 1.000065] [G loss: 0.999899]\n",
      "1135 [D loss: 1.000018] [G loss: 0.999997]\n",
      "1136 [D loss: 1.000090] [G loss: 0.999744]\n",
      "1137 [D loss: 1.000011] [G loss: 0.999854]\n",
      "1138 [D loss: 1.000004] [G loss: 1.000032]\n",
      "1139 [D loss: 1.000067] [G loss: 0.999983]\n",
      "1140 [D loss: 1.000048] [G loss: 0.999895]\n",
      "1141 [D loss: 1.000088] [G loss: 0.999941]\n",
      "1142 [D loss: 1.000080] [G loss: 1.000073]\n",
      "1143 [D loss: 1.000115] [G loss: 0.999955]\n",
      "1144 [D loss: 1.000031] [G loss: 0.999850]\n",
      "1145 [D loss: 0.999919] [G loss: 0.999993]\n",
      "1146 [D loss: 1.000087] [G loss: 0.999854]\n",
      "1147 [D loss: 1.000039] [G loss: 0.999914]\n",
      "1148 [D loss: 0.999992] [G loss: 0.999882]\n",
      "1149 [D loss: 1.000025] [G loss: 0.999890]\n",
      "1150 [D loss: 1.000047] [G loss: 0.999951]\n",
      "1151 [D loss: 1.000092] [G loss: 0.999945]\n",
      "1152 [D loss: 1.000096] [G loss: 1.000061]\n",
      "1153 [D loss: 1.000051] [G loss: 0.999982]\n",
      "1154 [D loss: 1.000116] [G loss: 0.999807]\n",
      "1155 [D loss: 0.999950] [G loss: 0.999990]\n",
      "1156 [D loss: 1.000115] [G loss: 0.999964]\n",
      "1157 [D loss: 1.000100] [G loss: 0.999903]\n",
      "1158 [D loss: 1.000018] [G loss: 0.999995]\n",
      "1159 [D loss: 1.000059] [G loss: 0.999812]\n",
      "1160 [D loss: 1.000111] [G loss: 0.999743]\n",
      "1161 [D loss: 1.000189] [G loss: 0.999789]\n",
      "1162 [D loss: 1.000041] [G loss: 1.000018]\n",
      "1163 [D loss: 0.999925] [G loss: 1.000004]\n",
      "1164 [D loss: 0.999963] [G loss: 0.999922]\n",
      "1165 [D loss: 1.000011] [G loss: 0.999912]\n",
      "1166 [D loss: 0.999966] [G loss: 0.999987]\n",
      "1167 [D loss: 1.000055] [G loss: 0.999720]\n",
      "1168 [D loss: 1.000006] [G loss: 0.999888]\n",
      "1169 [D loss: 1.000008] [G loss: 0.999703]\n",
      "1170 [D loss: 0.999995] [G loss: 0.999908]\n",
      "1171 [D loss: 1.000008] [G loss: 1.000079]\n",
      "1172 [D loss: 0.999984] [G loss: 1.000077]\n",
      "1173 [D loss: 1.000030] [G loss: 0.999973]\n",
      "1174 [D loss: 0.999928] [G loss: 0.999971]\n",
      "1175 [D loss: 0.999974] [G loss: 0.999995]\n",
      "1176 [D loss: 1.000094] [G loss: 0.999903]\n",
      "1177 [D loss: 1.000004] [G loss: 0.999953]\n",
      "1178 [D loss: 1.000002] [G loss: 0.999875]\n",
      "1179 [D loss: 1.000129] [G loss: 1.000063]\n",
      "1180 [D loss: 1.000029] [G loss: 0.999919]\n",
      "1181 [D loss: 1.000035] [G loss: 1.000032]\n",
      "1182 [D loss: 0.999972] [G loss: 1.000016]\n",
      "1183 [D loss: 1.000060] [G loss: 0.999858]\n",
      "1184 [D loss: 1.000093] [G loss: 0.999969]\n",
      "1185 [D loss: 1.000090] [G loss: 0.999844]\n",
      "1186 [D loss: 1.000000] [G loss: 0.999950]\n",
      "1187 [D loss: 1.000044] [G loss: 0.999934]\n",
      "1188 [D loss: 1.000016] [G loss: 0.999831]\n",
      "1189 [D loss: 1.000105] [G loss: 0.999667]\n",
      "1190 [D loss: 1.000040] [G loss: 0.999958]\n",
      "1191 [D loss: 1.000002] [G loss: 0.999918]\n",
      "1192 [D loss: 1.000144] [G loss: 0.999934]\n",
      "1193 [D loss: 0.999985] [G loss: 0.999889]\n",
      "1194 [D loss: 0.999926] [G loss: 1.000036]\n",
      "1195 [D loss: 1.000140] [G loss: 0.999888]\n",
      "1196 [D loss: 0.999988] [G loss: 0.999818]\n",
      "1197 [D loss: 1.000020] [G loss: 0.999940]\n",
      "1198 [D loss: 0.999990] [G loss: 0.999992]\n",
      "1199 [D loss: 1.000004] [G loss: 0.999719]\n",
      "1200 [D loss: 1.000081] [G loss: 0.999883]\n",
      "1201 [D loss: 1.000079] [G loss: 0.999950]\n",
      "1202 [D loss: 1.000042] [G loss: 0.999953]\n",
      "1203 [D loss: 1.000025] [G loss: 0.999865]\n",
      "1204 [D loss: 1.000014] [G loss: 1.000084]\n",
      "1205 [D loss: 1.000036] [G loss: 0.999871]\n",
      "1206 [D loss: 1.000103] [G loss: 0.999978]\n",
      "1207 [D loss: 1.000011] [G loss: 0.999999]\n",
      "1208 [D loss: 1.000073] [G loss: 0.999915]\n",
      "1209 [D loss: 1.000073] [G loss: 0.999773]\n",
      "1210 [D loss: 1.000008] [G loss: 0.999865]\n",
      "1211 [D loss: 1.000123] [G loss: 0.999760]\n",
      "1212 [D loss: 1.000119] [G loss: 0.999867]\n",
      "1213 [D loss: 1.000046] [G loss: 0.999970]\n",
      "1214 [D loss: 1.000042] [G loss: 0.999759]\n",
      "1215 [D loss: 1.000153] [G loss: 0.999729]\n",
      "1216 [D loss: 1.000007] [G loss: 0.999932]\n",
      "1217 [D loss: 1.000034] [G loss: 0.999899]\n",
      "1218 [D loss: 1.000091] [G loss: 0.999893]\n",
      "1219 [D loss: 1.000023] [G loss: 0.999667]\n",
      "1220 [D loss: 0.999988] [G loss: 1.000066]\n",
      "1221 [D loss: 0.999984] [G loss: 0.999945]\n",
      "1222 [D loss: 1.000068] [G loss: 0.999952]\n",
      "1223 [D loss: 0.999924] [G loss: 1.000039]\n",
      "1224 [D loss: 0.999944] [G loss: 0.999913]\n",
      "1225 [D loss: 1.000021] [G loss: 0.999946]\n",
      "1226 [D loss: 1.000067] [G loss: 0.999813]\n",
      "1227 [D loss: 0.999943] [G loss: 0.999956]\n",
      "1228 [D loss: 1.000067] [G loss: 0.999751]\n",
      "1229 [D loss: 1.000017] [G loss: 1.000088]\n",
      "1230 [D loss: 1.000090] [G loss: 0.999950]\n",
      "1231 [D loss: 0.999980] [G loss: 0.999987]\n",
      "1232 [D loss: 1.000103] [G loss: 0.999841]\n",
      "1233 [D loss: 1.000122] [G loss: 0.999838]\n",
      "1234 [D loss: 1.000114] [G loss: 0.999909]\n",
      "1235 [D loss: 0.999983] [G loss: 1.000144]\n",
      "1236 [D loss: 0.999948] [G loss: 1.000026]\n",
      "1237 [D loss: 1.000042] [G loss: 0.999922]\n",
      "1238 [D loss: 1.000079] [G loss: 0.999768]\n",
      "1239 [D loss: 1.000026] [G loss: 0.999897]\n",
      "1240 [D loss: 1.000058] [G loss: 0.999825]\n",
      "1241 [D loss: 0.999984] [G loss: 0.999804]\n",
      "1242 [D loss: 1.000096] [G loss: 0.999721]\n",
      "1243 [D loss: 1.000048] [G loss: 0.999903]\n",
      "1244 [D loss: 1.000103] [G loss: 0.999783]\n",
      "1245 [D loss: 0.999893] [G loss: 1.000083]\n",
      "1246 [D loss: 1.000047] [G loss: 0.999837]\n",
      "1247 [D loss: 1.000088] [G loss: 0.999851]\n",
      "1248 [D loss: 1.000061] [G loss: 0.999962]\n",
      "1249 [D loss: 1.000035] [G loss: 0.999823]\n",
      "1250 [D loss: 1.000024] [G loss: 1.000091]\n",
      "1251 [D loss: 1.000054] [G loss: 0.999933]\n",
      "1252 [D loss: 1.000036] [G loss: 0.999899]\n",
      "1253 [D loss: 1.000132] [G loss: 0.999706]\n",
      "1254 [D loss: 1.000096] [G loss: 1.000106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1255 [D loss: 0.999995] [G loss: 0.999983]\n",
      "1256 [D loss: 1.000068] [G loss: 0.999782]\n",
      "1257 [D loss: 1.000134] [G loss: 0.999791]\n",
      "1258 [D loss: 1.000083] [G loss: 0.999939]\n",
      "1259 [D loss: 1.000069] [G loss: 0.999793]\n",
      "1260 [D loss: 0.999987] [G loss: 1.000132]\n",
      "1261 [D loss: 1.000000] [G loss: 1.000227]\n",
      "1262 [D loss: 1.000058] [G loss: 1.000124]\n",
      "1263 [D loss: 0.999974] [G loss: 1.000019]\n",
      "1264 [D loss: 0.999986] [G loss: 1.000007]\n",
      "1265 [D loss: 1.000050] [G loss: 0.999901]\n",
      "1266 [D loss: 1.000091] [G loss: 0.999817]\n",
      "1267 [D loss: 1.000027] [G loss: 0.999883]\n",
      "1268 [D loss: 1.000126] [G loss: 0.999715]\n",
      "1269 [D loss: 0.999997] [G loss: 0.999876]\n",
      "1270 [D loss: 0.999830] [G loss: 0.999923]\n",
      "1271 [D loss: 0.999945] [G loss: 0.999959]\n",
      "1272 [D loss: 1.000003] [G loss: 1.000001]\n",
      "1273 [D loss: 0.999945] [G loss: 1.000067]\n",
      "1274 [D loss: 0.999982] [G loss: 0.999979]\n",
      "1275 [D loss: 0.999896] [G loss: 1.000044]\n",
      "1276 [D loss: 1.000048] [G loss: 0.999866]\n",
      "1277 [D loss: 0.999994] [G loss: 0.999861]\n",
      "1278 [D loss: 1.000005] [G loss: 1.000005]\n",
      "1279 [D loss: 0.999988] [G loss: 0.999831]\n",
      "1280 [D loss: 1.000097] [G loss: 0.999694]\n",
      "1281 [D loss: 1.000069] [G loss: 0.999927]\n",
      "1282 [D loss: 1.000065] [G loss: 0.999902]\n",
      "1283 [D loss: 1.000016] [G loss: 0.999903]\n",
      "1284 [D loss: 1.000005] [G loss: 1.000024]\n",
      "1285 [D loss: 1.000101] [G loss: 0.999902]\n",
      "1286 [D loss: 1.000015] [G loss: 0.999877]\n",
      "1287 [D loss: 1.000058] [G loss: 0.999996]\n",
      "1288 [D loss: 1.000174] [G loss: 0.999868]\n",
      "1289 [D loss: 1.000008] [G loss: 0.999957]\n",
      "1290 [D loss: 1.000070] [G loss: 1.000024]\n",
      "1291 [D loss: 0.999979] [G loss: 1.000222]\n",
      "1292 [D loss: 1.000123] [G loss: 0.999932]\n",
      "1293 [D loss: 0.999902] [G loss: 1.000112]\n",
      "1294 [D loss: 0.999993] [G loss: 0.999919]\n",
      "1295 [D loss: 0.999933] [G loss: 1.000063]\n",
      "1296 [D loss: 1.000121] [G loss: 0.999775]\n",
      "1297 [D loss: 1.000037] [G loss: 0.999912]\n",
      "1298 [D loss: 1.000083] [G loss: 0.999778]\n",
      "1299 [D loss: 1.000045] [G loss: 0.999877]\n",
      "1300 [D loss: 0.999936] [G loss: 0.999978]\n",
      "1301 [D loss: 1.000066] [G loss: 0.999835]\n",
      "1302 [D loss: 0.999970] [G loss: 0.999913]\n",
      "1303 [D loss: 1.000132] [G loss: 0.999800]\n",
      "1304 [D loss: 1.000015] [G loss: 0.999796]\n",
      "1305 [D loss: 1.000067] [G loss: 0.999951]\n",
      "1306 [D loss: 1.000096] [G loss: 0.999718]\n",
      "1307 [D loss: 0.999876] [G loss: 0.999940]\n",
      "1308 [D loss: 1.000033] [G loss: 0.999987]\n",
      "1309 [D loss: 0.999811] [G loss: 1.000027]\n",
      "1310 [D loss: 0.999941] [G loss: 0.999896]\n",
      "1311 [D loss: 1.000066] [G loss: 1.000034]\n",
      "1312 [D loss: 0.999955] [G loss: 1.000051]\n",
      "1313 [D loss: 0.999980] [G loss: 0.999917]\n",
      "1314 [D loss: 1.000026] [G loss: 0.999810]\n",
      "1315 [D loss: 1.000114] [G loss: 0.999934]\n",
      "1316 [D loss: 0.999993] [G loss: 0.999868]\n",
      "1317 [D loss: 0.999950] [G loss: 0.999978]\n",
      "1318 [D loss: 1.000100] [G loss: 0.999759]\n",
      "1319 [D loss: 0.999978] [G loss: 0.999979]\n",
      "1320 [D loss: 1.000060] [G loss: 0.999768]\n",
      "1321 [D loss: 1.000097] [G loss: 0.999957]\n",
      "1322 [D loss: 1.000049] [G loss: 1.000014]\n",
      "1323 [D loss: 0.999986] [G loss: 1.000028]\n",
      "1324 [D loss: 1.000097] [G loss: 0.999904]\n",
      "1325 [D loss: 1.000076] [G loss: 1.000088]\n",
      "1326 [D loss: 1.000160] [G loss: 0.999910]\n",
      "1327 [D loss: 1.000088] [G loss: 1.000150]\n",
      "1328 [D loss: 1.000015] [G loss: 0.999959]\n",
      "1329 [D loss: 0.999965] [G loss: 0.999936]\n",
      "1330 [D loss: 1.000022] [G loss: 1.000000]\n",
      "1331 [D loss: 0.999986] [G loss: 0.999978]\n",
      "1332 [D loss: 0.999903] [G loss: 1.000174]\n",
      "1333 [D loss: 1.000074] [G loss: 0.999930]\n",
      "1334 [D loss: 0.999974] [G loss: 1.000098]\n",
      "1335 [D loss: 0.999993] [G loss: 1.000077]\n",
      "1336 [D loss: 1.000017] [G loss: 0.999962]\n",
      "1337 [D loss: 0.999996] [G loss: 0.999902]\n",
      "1338 [D loss: 0.999969] [G loss: 1.000096]\n",
      "1339 [D loss: 0.999977] [G loss: 0.999890]\n",
      "1340 [D loss: 1.000070] [G loss: 0.999911]\n",
      "1341 [D loss: 1.000006] [G loss: 1.000054]\n",
      "1342 [D loss: 1.000144] [G loss: 0.999941]\n",
      "1343 [D loss: 0.999908] [G loss: 1.000190]\n",
      "1344 [D loss: 1.000017] [G loss: 1.000000]\n",
      "1345 [D loss: 0.999992] [G loss: 1.000075]\n",
      "1346 [D loss: 1.000008] [G loss: 0.999894]\n",
      "1347 [D loss: 1.000104] [G loss: 0.999817]\n",
      "1348 [D loss: 0.999957] [G loss: 0.999932]\n",
      "1349 [D loss: 1.000014] [G loss: 0.999899]\n",
      "1350 [D loss: 0.999991] [G loss: 1.000060]\n",
      "1351 [D loss: 0.999988] [G loss: 0.999952]\n",
      "1352 [D loss: 1.000086] [G loss: 0.999770]\n",
      "1353 [D loss: 0.999976] [G loss: 0.999956]\n",
      "1354 [D loss: 0.999992] [G loss: 0.999873]\n",
      "1355 [D loss: 1.000032] [G loss: 0.999934]\n",
      "1356 [D loss: 1.000158] [G loss: 0.999672]\n",
      "1357 [D loss: 1.000042] [G loss: 0.999899]\n",
      "1358 [D loss: 1.000001] [G loss: 0.999917]\n",
      "1359 [D loss: 1.000117] [G loss: 0.999905]\n",
      "1360 [D loss: 1.000013] [G loss: 1.000015]\n",
      "1361 [D loss: 1.000018] [G loss: 0.999970]\n",
      "1362 [D loss: 0.999958] [G loss: 1.000100]\n",
      "1363 [D loss: 0.999947] [G loss: 1.000126]\n",
      "1364 [D loss: 1.000055] [G loss: 0.999883]\n",
      "1365 [D loss: 0.999960] [G loss: 0.999897]\n",
      "1366 [D loss: 0.999987] [G loss: 0.999974]\n",
      "1367 [D loss: 1.000081] [G loss: 0.999874]\n",
      "1368 [D loss: 0.999997] [G loss: 0.999810]\n",
      "1369 [D loss: 1.000067] [G loss: 1.000045]\n",
      "1370 [D loss: 0.999957] [G loss: 0.999955]\n",
      "1371 [D loss: 1.000079] [G loss: 1.000014]\n",
      "1372 [D loss: 1.000043] [G loss: 0.999938]\n",
      "1373 [D loss: 1.000124] [G loss: 0.999824]\n",
      "1374 [D loss: 1.000187] [G loss: 0.999947]\n",
      "1375 [D loss: 1.000023] [G loss: 0.999891]\n",
      "1376 [D loss: 0.999999] [G loss: 0.999903]\n",
      "1377 [D loss: 1.000131] [G loss: 0.999857]\n",
      "1378 [D loss: 1.000021] [G loss: 1.000008]\n",
      "1379 [D loss: 1.000024] [G loss: 0.999891]\n",
      "1380 [D loss: 0.999984] [G loss: 0.999842]\n",
      "1381 [D loss: 1.000109] [G loss: 0.999918]\n",
      "1382 [D loss: 1.000066] [G loss: 0.999797]\n",
      "1383 [D loss: 1.000078] [G loss: 0.999876]\n",
      "1384 [D loss: 0.999977] [G loss: 0.999966]\n",
      "1385 [D loss: 0.999983] [G loss: 0.999913]\n",
      "1386 [D loss: 0.999999] [G loss: 1.000011]\n",
      "1387 [D loss: 0.999946] [G loss: 0.999990]\n",
      "1388 [D loss: 0.999977] [G loss: 0.999938]\n",
      "1389 [D loss: 1.000074] [G loss: 1.000007]\n",
      "1390 [D loss: 1.000026] [G loss: 0.999887]\n",
      "1391 [D loss: 0.999961] [G loss: 1.000021]\n",
      "1392 [D loss: 0.999966] [G loss: 0.999965]\n",
      "1393 [D loss: 1.000082] [G loss: 0.999894]\n",
      "1394 [D loss: 1.000091] [G loss: 0.999864]\n",
      "1395 [D loss: 1.000039] [G loss: 0.999919]\n",
      "1396 [D loss: 1.000069] [G loss: 0.999871]\n",
      "1397 [D loss: 0.999988] [G loss: 0.999815]\n",
      "1398 [D loss: 1.000090] [G loss: 0.999802]\n",
      "1399 [D loss: 0.999951] [G loss: 0.999822]\n",
      "1400 [D loss: 1.000074] [G loss: 0.999832]\n",
      "1401 [D loss: 1.000074] [G loss: 0.999917]\n",
      "1402 [D loss: 1.000081] [G loss: 0.999971]\n",
      "1403 [D loss: 0.999900] [G loss: 0.999965]\n",
      "1404 [D loss: 1.000009] [G loss: 0.999844]\n",
      "1405 [D loss: 1.000038] [G loss: 0.999967]\n",
      "1406 [D loss: 1.000130] [G loss: 0.999804]\n",
      "1407 [D loss: 1.000021] [G loss: 0.999860]\n",
      "1408 [D loss: 1.000061] [G loss: 0.999837]\n",
      "1409 [D loss: 0.999958] [G loss: 0.999968]\n",
      "1410 [D loss: 0.999970] [G loss: 0.999950]\n",
      "1411 [D loss: 0.999984] [G loss: 1.000010]\n",
      "1412 [D loss: 1.000032] [G loss: 1.000011]\n",
      "1413 [D loss: 1.000021] [G loss: 0.999785]\n",
      "1414 [D loss: 1.000073] [G loss: 0.999913]\n",
      "1415 [D loss: 0.999977] [G loss: 0.999902]\n",
      "1416 [D loss: 1.000079] [G loss: 0.999956]\n",
      "1417 [D loss: 0.999969] [G loss: 1.000058]\n",
      "1418 [D loss: 0.999998] [G loss: 0.999890]\n",
      "1419 [D loss: 0.999999] [G loss: 0.999896]\n",
      "1420 [D loss: 0.999971] [G loss: 0.999881]\n",
      "1421 [D loss: 1.000110] [G loss: 0.999920]\n",
      "1422 [D loss: 1.000047] [G loss: 0.999988]\n",
      "1423 [D loss: 0.999930] [G loss: 1.000023]\n",
      "1424 [D loss: 0.999984] [G loss: 0.999972]\n",
      "1425 [D loss: 1.000062] [G loss: 0.999914]\n",
      "1426 [D loss: 1.000017] [G loss: 0.999988]\n",
      "1427 [D loss: 1.000071] [G loss: 0.999998]\n",
      "1428 [D loss: 1.000021] [G loss: 0.999957]\n",
      "1429 [D loss: 0.999966] [G loss: 0.999968]\n",
      "1430 [D loss: 1.000003] [G loss: 0.999919]\n",
      "1431 [D loss: 0.999957] [G loss: 1.000067]\n",
      "1432 [D loss: 0.999897] [G loss: 0.999954]\n",
      "1433 [D loss: 0.999932] [G loss: 0.999964]\n",
      "1434 [D loss: 1.000010] [G loss: 0.999791]\n",
      "1435 [D loss: 0.999965] [G loss: 0.999858]\n",
      "1436 [D loss: 0.999956] [G loss: 1.000014]\n",
      "1437 [D loss: 0.999946] [G loss: 0.999974]\n",
      "1438 [D loss: 1.000036] [G loss: 0.999957]\n",
      "1439 [D loss: 1.000007] [G loss: 1.000056]\n",
      "1440 [D loss: 1.000053] [G loss: 0.999973]\n",
      "1441 [D loss: 1.000028] [G loss: 0.999879]\n",
      "1442 [D loss: 1.000100] [G loss: 0.999876]\n",
      "1443 [D loss: 1.000020] [G loss: 1.000043]\n",
      "1444 [D loss: 1.000025] [G loss: 0.999796]\n",
      "1445 [D loss: 0.999992] [G loss: 0.999936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446 [D loss: 1.000046] [G loss: 0.999848]\n",
      "1447 [D loss: 0.999973] [G loss: 0.999944]\n",
      "1448 [D loss: 1.000073] [G loss: 0.999982]\n",
      "1449 [D loss: 1.000003] [G loss: 1.000004]\n",
      "1450 [D loss: 1.000015] [G loss: 0.999923]\n",
      "1451 [D loss: 0.999997] [G loss: 0.999892]\n",
      "1452 [D loss: 0.999904] [G loss: 1.000057]\n",
      "1453 [D loss: 1.000042] [G loss: 1.000020]\n",
      "1454 [D loss: 1.000020] [G loss: 0.999888]\n",
      "1455 [D loss: 1.000006] [G loss: 0.999875]\n",
      "1456 [D loss: 0.999919] [G loss: 1.000053]\n",
      "1457 [D loss: 0.999998] [G loss: 0.999875]\n",
      "1458 [D loss: 0.999994] [G loss: 1.000022]\n",
      "1459 [D loss: 0.999942] [G loss: 1.000051]\n",
      "1460 [D loss: 0.999953] [G loss: 0.999867]\n",
      "1461 [D loss: 1.000001] [G loss: 0.999981]\n",
      "1462 [D loss: 1.000036] [G loss: 0.999875]\n",
      "1463 [D loss: 1.000046] [G loss: 0.999766]\n",
      "1464 [D loss: 1.000099] [G loss: 0.999764]\n",
      "1465 [D loss: 1.000017] [G loss: 0.999917]\n",
      "1466 [D loss: 1.000084] [G loss: 0.999921]\n",
      "1467 [D loss: 1.000011] [G loss: 0.999926]\n",
      "1468 [D loss: 0.999927] [G loss: 0.999979]\n",
      "1469 [D loss: 0.999923] [G loss: 1.000004]\n",
      "1470 [D loss: 1.000112] [G loss: 0.999980]\n",
      "1471 [D loss: 0.999945] [G loss: 1.000061]\n",
      "1472 [D loss: 0.999962] [G loss: 0.999830]\n",
      "1473 [D loss: 1.000051] [G loss: 0.999993]\n",
      "1474 [D loss: 1.000047] [G loss: 0.999947]\n",
      "1475 [D loss: 0.999958] [G loss: 1.000005]\n",
      "1476 [D loss: 1.000007] [G loss: 0.999996]\n",
      "1477 [D loss: 1.000111] [G loss: 0.999992]\n",
      "1478 [D loss: 0.999983] [G loss: 1.000010]\n",
      "1479 [D loss: 1.000093] [G loss: 0.999870]\n",
      "1480 [D loss: 1.000057] [G loss: 1.000105]\n",
      "1481 [D loss: 0.999954] [G loss: 1.000027]\n",
      "1482 [D loss: 0.999919] [G loss: 1.000110]\n",
      "1483 [D loss: 1.000004] [G loss: 1.000030]\n",
      "1484 [D loss: 1.000055] [G loss: 0.999805]\n",
      "1485 [D loss: 1.000048] [G loss: 0.999854]\n",
      "1486 [D loss: 0.999965] [G loss: 0.999816]\n",
      "1487 [D loss: 1.000068] [G loss: 0.999921]\n",
      "1488 [D loss: 1.000120] [G loss: 0.999841]\n",
      "1489 [D loss: 0.999972] [G loss: 1.000013]\n",
      "1490 [D loss: 1.000032] [G loss: 1.000010]\n",
      "1491 [D loss: 0.999955] [G loss: 1.000045]\n",
      "1492 [D loss: 0.999966] [G loss: 0.999974]\n",
      "1493 [D loss: 1.000049] [G loss: 0.999976]\n",
      "1494 [D loss: 0.999984] [G loss: 0.999862]\n",
      "1495 [D loss: 1.000042] [G loss: 0.999924]\n",
      "1496 [D loss: 0.999962] [G loss: 0.999929]\n",
      "1497 [D loss: 1.000013] [G loss: 0.999955]\n",
      "1498 [D loss: 1.000010] [G loss: 0.999940]\n",
      "1499 [D loss: 0.999932] [G loss: 1.000040]\n",
      "1500 [D loss: 0.999980] [G loss: 0.999964]\n",
      "1501 [D loss: 1.000123] [G loss: 1.000014]\n",
      "1502 [D loss: 1.000045] [G loss: 0.999935]\n",
      "1503 [D loss: 0.999944] [G loss: 0.999927]\n",
      "1504 [D loss: 1.000032] [G loss: 0.999906]\n",
      "1505 [D loss: 0.999914] [G loss: 0.999872]\n",
      "1506 [D loss: 1.000027] [G loss: 0.999937]\n",
      "1507 [D loss: 0.999972] [G loss: 0.999974]\n",
      "1508 [D loss: 0.999980] [G loss: 1.000025]\n",
      "1509 [D loss: 1.000043] [G loss: 0.999970]\n",
      "1510 [D loss: 1.000005] [G loss: 0.999822]\n",
      "1511 [D loss: 0.999961] [G loss: 0.999838]\n",
      "1512 [D loss: 1.000053] [G loss: 0.999922]\n",
      "1513 [D loss: 1.000039] [G loss: 0.999845]\n",
      "1514 [D loss: 1.000008] [G loss: 0.999825]\n",
      "1515 [D loss: 0.999977] [G loss: 0.999912]\n",
      "1516 [D loss: 1.000080] [G loss: 0.999831]\n",
      "1517 [D loss: 1.000005] [G loss: 0.999905]\n",
      "1518 [D loss: 1.000032] [G loss: 0.999898]\n",
      "1519 [D loss: 0.999959] [G loss: 0.999960]\n",
      "1520 [D loss: 1.000078] [G loss: 0.999820]\n",
      "1521 [D loss: 0.999993] [G loss: 0.999857]\n",
      "1522 [D loss: 1.000006] [G loss: 0.999806]\n",
      "1523 [D loss: 0.999960] [G loss: 0.999893]\n",
      "1524 [D loss: 0.999953] [G loss: 0.999908]\n",
      "1525 [D loss: 1.000006] [G loss: 0.999979]\n",
      "1526 [D loss: 1.000019] [G loss: 0.999891]\n",
      "1527 [D loss: 0.999982] [G loss: 0.999986]\n",
      "1528 [D loss: 1.000045] [G loss: 0.999939]\n",
      "1529 [D loss: 1.000020] [G loss: 1.000028]\n",
      "1530 [D loss: 0.999951] [G loss: 0.999976]\n",
      "1531 [D loss: 1.000023] [G loss: 1.000019]\n",
      "1532 [D loss: 0.999955] [G loss: 0.999852]\n",
      "1533 [D loss: 1.000043] [G loss: 0.999823]\n",
      "1534 [D loss: 0.999948] [G loss: 1.000009]\n",
      "1535 [D loss: 0.999978] [G loss: 0.999996]\n",
      "1536 [D loss: 1.000015] [G loss: 0.999921]\n",
      "1537 [D loss: 1.000083] [G loss: 1.000032]\n",
      "1538 [D loss: 1.000026] [G loss: 0.999947]\n",
      "1539 [D loss: 0.999971] [G loss: 0.999969]\n",
      "1540 [D loss: 1.000048] [G loss: 1.000055]\n",
      "1541 [D loss: 0.999899] [G loss: 1.000089]\n",
      "1542 [D loss: 1.000012] [G loss: 0.999990]\n",
      "1543 [D loss: 0.999894] [G loss: 1.000061]\n",
      "1544 [D loss: 0.999974] [G loss: 0.999972]\n",
      "1545 [D loss: 1.000027] [G loss: 0.999912]\n",
      "1546 [D loss: 0.999987] [G loss: 1.000006]\n",
      "1547 [D loss: 0.999986] [G loss: 1.000031]\n",
      "1548 [D loss: 0.999939] [G loss: 0.999875]\n",
      "1549 [D loss: 0.999967] [G loss: 0.999950]\n",
      "1550 [D loss: 1.000011] [G loss: 0.999916]\n",
      "1551 [D loss: 1.000009] [G loss: 0.999902]\n",
      "1552 [D loss: 1.000005] [G loss: 0.999916]\n",
      "1553 [D loss: 0.999948] [G loss: 1.000073]\n",
      "1554 [D loss: 1.000117] [G loss: 0.999885]\n",
      "1555 [D loss: 0.999996] [G loss: 0.999955]\n",
      "1556 [D loss: 1.000000] [G loss: 0.999977]\n",
      "1557 [D loss: 1.000045] [G loss: 0.999956]\n",
      "1558 [D loss: 1.000033] [G loss: 0.999887]\n",
      "1559 [D loss: 1.000000] [G loss: 0.999955]\n",
      "1560 [D loss: 0.999992] [G loss: 0.999864]\n",
      "1561 [D loss: 1.000013] [G loss: 0.999919]\n",
      "1562 [D loss: 1.000002] [G loss: 0.999883]\n",
      "1563 [D loss: 1.000089] [G loss: 0.999897]\n",
      "1564 [D loss: 0.999976] [G loss: 0.999949]\n",
      "1565 [D loss: 0.999998] [G loss: 0.999881]\n",
      "1566 [D loss: 0.999993] [G loss: 1.000090]\n",
      "1567 [D loss: 0.999931] [G loss: 0.999848]\n",
      "1568 [D loss: 0.999927] [G loss: 1.000036]\n",
      "1569 [D loss: 0.999973] [G loss: 0.999863]\n",
      "1570 [D loss: 0.999846] [G loss: 1.000032]\n",
      "1571 [D loss: 1.000022] [G loss: 1.000008]\n",
      "1572 [D loss: 0.999943] [G loss: 1.000051]\n",
      "1573 [D loss: 1.000051] [G loss: 0.999891]\n",
      "1574 [D loss: 1.000086] [G loss: 0.999804]\n",
      "1575 [D loss: 0.999939] [G loss: 0.999981]\n",
      "1576 [D loss: 0.999978] [G loss: 0.999957]\n",
      "1577 [D loss: 0.999954] [G loss: 1.000009]\n",
      "1578 [D loss: 0.999968] [G loss: 0.999911]\n",
      "1579 [D loss: 0.999946] [G loss: 0.999972]\n",
      "1580 [D loss: 0.999999] [G loss: 0.999944]\n",
      "1581 [D loss: 0.999981] [G loss: 1.000084]\n",
      "1582 [D loss: 1.000119] [G loss: 0.999694]\n",
      "1583 [D loss: 1.000078] [G loss: 0.999699]\n",
      "1584 [D loss: 1.000026] [G loss: 0.999956]\n",
      "1585 [D loss: 0.999937] [G loss: 0.999932]\n",
      "1586 [D loss: 1.000015] [G loss: 0.999971]\n",
      "1587 [D loss: 1.000039] [G loss: 0.999864]\n",
      "1588 [D loss: 1.000099] [G loss: 0.999974]\n",
      "1589 [D loss: 0.999961] [G loss: 0.999961]\n",
      "1590 [D loss: 1.000000] [G loss: 1.000006]\n",
      "1591 [D loss: 0.999954] [G loss: 1.000011]\n",
      "1592 [D loss: 1.000020] [G loss: 0.999887]\n",
      "1593 [D loss: 1.000045] [G loss: 0.999885]\n",
      "1594 [D loss: 0.999966] [G loss: 0.999945]\n",
      "1595 [D loss: 1.000007] [G loss: 1.000066]\n",
      "1596 [D loss: 1.000059] [G loss: 0.999866]\n",
      "1597 [D loss: 0.999990] [G loss: 0.999906]\n",
      "1598 [D loss: 1.000052] [G loss: 0.999915]\n",
      "1599 [D loss: 1.000053] [G loss: 0.999841]\n",
      "1600 [D loss: 0.999897] [G loss: 1.000024]\n",
      "1601 [D loss: 0.999893] [G loss: 1.000136]\n",
      "1602 [D loss: 1.000046] [G loss: 0.999970]\n",
      "1603 [D loss: 1.000117] [G loss: 0.999792]\n",
      "1604 [D loss: 0.999953] [G loss: 0.999999]\n",
      "1605 [D loss: 1.000041] [G loss: 0.999991]\n",
      "1606 [D loss: 0.999928] [G loss: 0.999924]\n",
      "1607 [D loss: 1.000023] [G loss: 0.999947]\n",
      "1608 [D loss: 0.999970] [G loss: 0.999982]\n",
      "1609 [D loss: 0.999982] [G loss: 0.999965]\n",
      "1610 [D loss: 0.999936] [G loss: 1.000089]\n",
      "1611 [D loss: 0.999945] [G loss: 1.000021]\n",
      "1612 [D loss: 1.000012] [G loss: 0.999908]\n",
      "1613 [D loss: 1.000052] [G loss: 0.999905]\n",
      "1614 [D loss: 0.999980] [G loss: 0.999956]\n",
      "1615 [D loss: 1.000008] [G loss: 0.999874]\n",
      "1616 [D loss: 0.999997] [G loss: 0.999855]\n",
      "1617 [D loss: 1.000032] [G loss: 0.999875]\n",
      "1618 [D loss: 0.999998] [G loss: 1.000119]\n",
      "1619 [D loss: 1.000113] [G loss: 0.999892]\n",
      "1620 [D loss: 0.999953] [G loss: 0.999998]\n",
      "1621 [D loss: 1.000068] [G loss: 0.999851]\n",
      "1622 [D loss: 1.000016] [G loss: 0.999874]\n",
      "1623 [D loss: 1.000020] [G loss: 0.999949]\n",
      "1624 [D loss: 0.999949] [G loss: 0.999890]\n",
      "1625 [D loss: 1.000010] [G loss: 0.999751]\n",
      "1626 [D loss: 0.999922] [G loss: 0.999866]\n",
      "1627 [D loss: 1.000044] [G loss: 0.999841]\n",
      "1628 [D loss: 0.999980] [G loss: 0.999883]\n",
      "1629 [D loss: 1.000054] [G loss: 0.999926]\n",
      "1630 [D loss: 1.000055] [G loss: 1.000024]\n",
      "1631 [D loss: 0.999939] [G loss: 1.000049]\n",
      "1632 [D loss: 1.000077] [G loss: 0.999775]\n",
      "1633 [D loss: 1.000019] [G loss: 0.999959]\n",
      "1634 [D loss: 1.000063] [G loss: 1.000008]\n",
      "1635 [D loss: 1.000080] [G loss: 0.999798]\n",
      "1636 [D loss: 0.999873] [G loss: 0.999944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637 [D loss: 1.000063] [G loss: 1.000008]\n",
      "1638 [D loss: 1.000038] [G loss: 0.999874]\n",
      "1639 [D loss: 1.000000] [G loss: 0.999975]\n",
      "1640 [D loss: 0.999969] [G loss: 1.000003]\n",
      "1641 [D loss: 0.999984] [G loss: 0.999988]\n",
      "1642 [D loss: 1.000034] [G loss: 1.000033]\n",
      "1643 [D loss: 0.999994] [G loss: 0.999903]\n",
      "1644 [D loss: 0.999924] [G loss: 1.000006]\n",
      "1645 [D loss: 0.999977] [G loss: 1.000079]\n",
      "1646 [D loss: 1.000052] [G loss: 0.999860]\n",
      "1647 [D loss: 1.000005] [G loss: 0.999880]\n",
      "1648 [D loss: 1.000026] [G loss: 1.000042]\n",
      "1649 [D loss: 0.999978] [G loss: 0.999960]\n",
      "1650 [D loss: 0.999894] [G loss: 1.000016]\n",
      "1651 [D loss: 0.999990] [G loss: 0.999909]\n",
      "1652 [D loss: 1.000036] [G loss: 0.999920]\n",
      "1653 [D loss: 0.999938] [G loss: 0.999961]\n",
      "1654 [D loss: 0.999869] [G loss: 0.999999]\n",
      "1655 [D loss: 1.000016] [G loss: 0.999996]\n",
      "1656 [D loss: 1.000006] [G loss: 0.999894]\n",
      "1657 [D loss: 0.999964] [G loss: 0.999933]\n",
      "1658 [D loss: 1.000015] [G loss: 0.999916]\n",
      "1659 [D loss: 1.000039] [G loss: 0.999909]\n",
      "1660 [D loss: 0.999972] [G loss: 1.000018]\n",
      "1661 [D loss: 0.999958] [G loss: 0.999920]\n",
      "1662 [D loss: 0.999981] [G loss: 0.999886]\n",
      "1663 [D loss: 1.000009] [G loss: 0.999900]\n",
      "1664 [D loss: 1.000089] [G loss: 0.999893]\n",
      "1665 [D loss: 0.999973] [G loss: 1.000028]\n",
      "1666 [D loss: 1.000021] [G loss: 0.999877]\n",
      "1667 [D loss: 0.999997] [G loss: 0.999910]\n",
      "1668 [D loss: 0.999956] [G loss: 0.999879]\n",
      "1669 [D loss: 0.999971] [G loss: 0.999975]\n",
      "1670 [D loss: 1.000071] [G loss: 0.999895]\n",
      "1671 [D loss: 1.000093] [G loss: 0.999783]\n",
      "1672 [D loss: 0.999946] [G loss: 0.999944]\n",
      "1673 [D loss: 0.999997] [G loss: 0.999865]\n",
      "1674 [D loss: 0.999992] [G loss: 0.999959]\n",
      "1675 [D loss: 1.000008] [G loss: 1.000038]\n",
      "1676 [D loss: 1.000014] [G loss: 1.000005]\n",
      "1677 [D loss: 1.000041] [G loss: 0.999938]\n",
      "1678 [D loss: 0.999959] [G loss: 0.999873]\n",
      "1679 [D loss: 1.000063] [G loss: 0.999838]\n",
      "1680 [D loss: 1.000037] [G loss: 0.999914]\n",
      "1681 [D loss: 1.000033] [G loss: 0.999951]\n",
      "1682 [D loss: 0.999959] [G loss: 0.999957]\n",
      "1683 [D loss: 0.999978] [G loss: 1.000041]\n",
      "1684 [D loss: 0.999989] [G loss: 0.999968]\n",
      "1685 [D loss: 1.000037] [G loss: 0.999975]\n",
      "1686 [D loss: 1.000009] [G loss: 0.999851]\n",
      "1687 [D loss: 1.000088] [G loss: 0.999961]\n",
      "1688 [D loss: 1.000046] [G loss: 0.999992]\n",
      "1689 [D loss: 1.000018] [G loss: 0.999931]\n",
      "1690 [D loss: 1.000062] [G loss: 0.999904]\n",
      "1691 [D loss: 1.000023] [G loss: 0.999875]\n",
      "1692 [D loss: 1.000093] [G loss: 0.999934]\n",
      "1693 [D loss: 1.000021] [G loss: 0.999960]\n",
      "1694 [D loss: 1.000076] [G loss: 0.999882]\n",
      "1695 [D loss: 0.999946] [G loss: 1.000126]\n",
      "1696 [D loss: 1.000043] [G loss: 0.999928]\n",
      "1697 [D loss: 0.999965] [G loss: 0.999980]\n",
      "1698 [D loss: 0.999998] [G loss: 0.999918]\n",
      "1699 [D loss: 1.000010] [G loss: 0.999916]\n",
      "1700 [D loss: 0.999930] [G loss: 0.999966]\n",
      "1701 [D loss: 1.000018] [G loss: 0.999900]\n",
      "1702 [D loss: 0.999996] [G loss: 0.999940]\n",
      "1703 [D loss: 1.000007] [G loss: 0.999975]\n",
      "1704 [D loss: 0.999988] [G loss: 0.999993]\n",
      "1705 [D loss: 0.999966] [G loss: 0.999986]\n",
      "1706 [D loss: 0.999986] [G loss: 0.999928]\n",
      "1707 [D loss: 0.999984] [G loss: 0.999913]\n",
      "1708 [D loss: 0.999934] [G loss: 0.999968]\n",
      "1709 [D loss: 1.000051] [G loss: 0.999957]\n",
      "1710 [D loss: 1.000070] [G loss: 0.999841]\n",
      "1711 [D loss: 0.999995] [G loss: 1.000058]\n",
      "1712 [D loss: 1.000018] [G loss: 0.999982]\n",
      "1713 [D loss: 0.999947] [G loss: 0.999971]\n",
      "1714 [D loss: 1.000024] [G loss: 0.999922]\n",
      "1715 [D loss: 0.999939] [G loss: 1.000038]\n",
      "1716 [D loss: 0.999927] [G loss: 0.999966]\n",
      "1717 [D loss: 1.000021] [G loss: 0.999924]\n",
      "1718 [D loss: 0.999988] [G loss: 0.999923]\n",
      "1719 [D loss: 1.000032] [G loss: 0.999994]\n",
      "1720 [D loss: 0.999938] [G loss: 1.000074]\n",
      "1721 [D loss: 1.000035] [G loss: 0.999946]\n",
      "1722 [D loss: 0.999995] [G loss: 0.999843]\n",
      "1723 [D loss: 1.000041] [G loss: 0.999890]\n",
      "1724 [D loss: 1.000054] [G loss: 0.999989]\n",
      "1725 [D loss: 1.000013] [G loss: 0.999971]\n",
      "1726 [D loss: 1.000094] [G loss: 0.999776]\n",
      "1727 [D loss: 0.999988] [G loss: 0.999991]\n",
      "1728 [D loss: 0.999920] [G loss: 1.000053]\n",
      "1729 [D loss: 0.999899] [G loss: 1.000003]\n",
      "1730 [D loss: 1.000047] [G loss: 0.999895]\n",
      "1731 [D loss: 0.999922] [G loss: 0.999878]\n",
      "1732 [D loss: 0.999983] [G loss: 0.999906]\n",
      "1733 [D loss: 1.000006] [G loss: 0.999926]\n",
      "1734 [D loss: 1.000026] [G loss: 0.999998]\n",
      "1735 [D loss: 0.999954] [G loss: 0.999989]\n",
      "1736 [D loss: 0.999995] [G loss: 1.000049]\n",
      "1737 [D loss: 1.000026] [G loss: 0.999863]\n",
      "1738 [D loss: 1.000095] [G loss: 0.999886]\n",
      "1739 [D loss: 0.999937] [G loss: 0.999931]\n",
      "1740 [D loss: 1.000171] [G loss: 0.999833]\n",
      "1741 [D loss: 0.999996] [G loss: 1.000079]\n",
      "1742 [D loss: 1.000021] [G loss: 0.999934]\n",
      "1743 [D loss: 0.999875] [G loss: 1.000052]\n",
      "1744 [D loss: 1.000065] [G loss: 0.999990]\n",
      "1745 [D loss: 0.999939] [G loss: 0.999915]\n",
      "1746 [D loss: 0.999991] [G loss: 0.999973]\n",
      "1747 [D loss: 0.999963] [G loss: 1.000070]\n",
      "1748 [D loss: 0.999935] [G loss: 0.999943]\n",
      "1749 [D loss: 0.999962] [G loss: 1.000058]\n",
      "1750 [D loss: 0.999999] [G loss: 1.000064]\n",
      "1751 [D loss: 0.999986] [G loss: 0.999895]\n",
      "1752 [D loss: 0.999959] [G loss: 1.000038]\n",
      "1753 [D loss: 1.000072] [G loss: 1.000000]\n",
      "1754 [D loss: 0.999913] [G loss: 0.999981]\n",
      "1755 [D loss: 0.999974] [G loss: 1.000071]\n",
      "1756 [D loss: 1.000032] [G loss: 0.999931]\n",
      "1757 [D loss: 1.000037] [G loss: 0.999954]\n",
      "1758 [D loss: 1.000023] [G loss: 1.000005]\n",
      "1759 [D loss: 1.000024] [G loss: 0.999946]\n",
      "1760 [D loss: 0.999983] [G loss: 1.000026]\n",
      "1761 [D loss: 0.999976] [G loss: 0.999931]\n",
      "1762 [D loss: 0.999912] [G loss: 1.000045]\n",
      "1763 [D loss: 0.999995] [G loss: 0.999863]\n",
      "1764 [D loss: 1.000008] [G loss: 0.999918]\n",
      "1765 [D loss: 0.999929] [G loss: 0.999921]\n",
      "1766 [D loss: 0.999983] [G loss: 1.000005]\n",
      "1767 [D loss: 1.000042] [G loss: 0.999856]\n",
      "1768 [D loss: 1.000049] [G loss: 0.999792]\n",
      "1769 [D loss: 1.000048] [G loss: 0.999837]\n",
      "1770 [D loss: 0.999999] [G loss: 1.000051]\n",
      "1771 [D loss: 0.999912] [G loss: 1.000003]\n",
      "1772 [D loss: 1.000094] [G loss: 0.999991]\n",
      "1773 [D loss: 0.999976] [G loss: 0.999966]\n",
      "1774 [D loss: 0.999923] [G loss: 0.999919]\n",
      "1775 [D loss: 0.999991] [G loss: 0.999916]\n",
      "1776 [D loss: 0.999963] [G loss: 0.999974]\n",
      "1777 [D loss: 0.999978] [G loss: 0.999924]\n",
      "1778 [D loss: 0.999970] [G loss: 0.999960]\n",
      "1779 [D loss: 1.000076] [G loss: 0.999909]\n",
      "1780 [D loss: 0.999863] [G loss: 1.000024]\n",
      "1781 [D loss: 0.999930] [G loss: 0.999998]\n",
      "1782 [D loss: 0.999962] [G loss: 0.999980]\n",
      "1783 [D loss: 0.999973] [G loss: 0.999812]\n",
      "1784 [D loss: 1.000149] [G loss: 0.999861]\n",
      "1785 [D loss: 1.000046] [G loss: 0.999901]\n",
      "1786 [D loss: 0.999989] [G loss: 0.999884]\n",
      "1787 [D loss: 0.999929] [G loss: 1.000003]\n",
      "1788 [D loss: 0.999931] [G loss: 0.999908]\n",
      "1789 [D loss: 0.999936] [G loss: 0.999893]\n",
      "1790 [D loss: 1.000056] [G loss: 0.999842]\n",
      "1791 [D loss: 1.000040] [G loss: 0.999934]\n",
      "1792 [D loss: 1.000055] [G loss: 0.999965]\n",
      "1793 [D loss: 0.999885] [G loss: 0.999845]\n",
      "1794 [D loss: 0.999989] [G loss: 1.000026]\n",
      "1795 [D loss: 1.000041] [G loss: 0.999916]\n",
      "1796 [D loss: 1.000014] [G loss: 1.000037]\n",
      "1797 [D loss: 1.000043] [G loss: 0.999972]\n",
      "1798 [D loss: 0.999944] [G loss: 0.999999]\n",
      "1799 [D loss: 0.999994] [G loss: 0.999967]\n",
      "1800 [D loss: 0.999931] [G loss: 0.999938]\n",
      "1801 [D loss: 1.000009] [G loss: 0.999949]\n",
      "1802 [D loss: 1.000005] [G loss: 0.999959]\n",
      "1803 [D loss: 1.000021] [G loss: 0.999912]\n",
      "1804 [D loss: 0.999988] [G loss: 1.000008]\n",
      "1805 [D loss: 1.000012] [G loss: 0.999904]\n",
      "1806 [D loss: 1.000019] [G loss: 0.999869]\n",
      "1807 [D loss: 1.000146] [G loss: 0.999833]\n",
      "1808 [D loss: 0.999934] [G loss: 0.999991]\n",
      "1809 [D loss: 0.999915] [G loss: 1.000043]\n",
      "1810 [D loss: 0.999975] [G loss: 0.999900]\n",
      "1811 [D loss: 0.999987] [G loss: 0.999977]\n",
      "1812 [D loss: 0.999965] [G loss: 0.999918]\n",
      "1813 [D loss: 0.999963] [G loss: 0.999863]\n",
      "1814 [D loss: 1.000006] [G loss: 1.000004]\n",
      "1815 [D loss: 1.000034] [G loss: 0.999867]\n",
      "1816 [D loss: 1.000014] [G loss: 0.999895]\n",
      "1817 [D loss: 0.999964] [G loss: 0.999964]\n",
      "1818 [D loss: 1.000009] [G loss: 0.999955]\n",
      "1819 [D loss: 1.000078] [G loss: 0.999934]\n",
      "1820 [D loss: 0.999999] [G loss: 0.999843]\n",
      "1821 [D loss: 1.000024] [G loss: 0.999882]\n",
      "1822 [D loss: 0.999944] [G loss: 0.999898]\n",
      "1823 [D loss: 1.000039] [G loss: 0.999881]\n",
      "1824 [D loss: 1.000020] [G loss: 0.999854]\n",
      "1825 [D loss: 0.999966] [G loss: 0.999984]\n",
      "1826 [D loss: 1.000021] [G loss: 1.000006]\n",
      "1827 [D loss: 1.000028] [G loss: 1.000062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828 [D loss: 0.999982] [G loss: 0.999920]\n",
      "1829 [D loss: 1.000027] [G loss: 0.999912]\n",
      "1830 [D loss: 1.000038] [G loss: 0.999879]\n",
      "1831 [D loss: 1.000051] [G loss: 0.999897]\n",
      "1832 [D loss: 0.999946] [G loss: 1.000080]\n",
      "1833 [D loss: 1.000032] [G loss: 0.999925]\n",
      "1834 [D loss: 1.000030] [G loss: 0.999985]\n",
      "1835 [D loss: 0.999941] [G loss: 0.999995]\n",
      "1836 [D loss: 0.999897] [G loss: 0.999975]\n",
      "1837 [D loss: 1.000039] [G loss: 0.999947]\n",
      "1838 [D loss: 0.999988] [G loss: 1.000041]\n",
      "1839 [D loss: 0.999886] [G loss: 0.999968]\n",
      "1840 [D loss: 1.000038] [G loss: 0.999950]\n",
      "1841 [D loss: 1.000014] [G loss: 0.999873]\n",
      "1842 [D loss: 0.999982] [G loss: 0.999870]\n",
      "1843 [D loss: 0.999956] [G loss: 0.999997]\n",
      "1844 [D loss: 0.999935] [G loss: 0.999969]\n",
      "1845 [D loss: 0.999990] [G loss: 0.999969]\n",
      "1846 [D loss: 0.999994] [G loss: 0.999868]\n",
      "1847 [D loss: 0.999977] [G loss: 1.000033]\n",
      "1848 [D loss: 1.000006] [G loss: 1.000035]\n",
      "1849 [D loss: 0.999943] [G loss: 0.999843]\n",
      "1850 [D loss: 0.999989] [G loss: 0.999892]\n",
      "1851 [D loss: 0.999972] [G loss: 1.000052]\n",
      "1852 [D loss: 0.999969] [G loss: 0.999900]\n",
      "1853 [D loss: 1.000086] [G loss: 0.999888]\n",
      "1854 [D loss: 0.999939] [G loss: 0.999924]\n",
      "1855 [D loss: 0.999973] [G loss: 0.999954]\n",
      "1856 [D loss: 0.999997] [G loss: 0.999898]\n",
      "1857 [D loss: 1.000020] [G loss: 0.999831]\n",
      "1858 [D loss: 1.000023] [G loss: 0.999912]\n",
      "1859 [D loss: 1.000062] [G loss: 0.999875]\n",
      "1860 [D loss: 0.999954] [G loss: 0.999968]\n",
      "1861 [D loss: 1.000044] [G loss: 0.999974]\n",
      "1862 [D loss: 1.000028] [G loss: 0.999779]\n",
      "1863 [D loss: 0.999973] [G loss: 0.999951]\n",
      "1864 [D loss: 1.000014] [G loss: 0.999843]\n",
      "1865 [D loss: 1.000008] [G loss: 0.999925]\n",
      "1866 [D loss: 1.000010] [G loss: 0.999946]\n",
      "1867 [D loss: 1.000045] [G loss: 0.999924]\n",
      "1868 [D loss: 1.000020] [G loss: 0.999946]\n",
      "1869 [D loss: 0.999981] [G loss: 0.999949]\n",
      "1870 [D loss: 1.000080] [G loss: 0.999875]\n",
      "1871 [D loss: 0.999949] [G loss: 0.999833]\n",
      "1872 [D loss: 0.999919] [G loss: 1.000086]\n",
      "1873 [D loss: 1.000018] [G loss: 0.999990]\n",
      "1874 [D loss: 1.000043] [G loss: 0.999889]\n",
      "1875 [D loss: 0.999946] [G loss: 1.000094]\n",
      "1876 [D loss: 1.000018] [G loss: 1.000080]\n",
      "1877 [D loss: 1.000059] [G loss: 0.999957]\n",
      "1878 [D loss: 0.999927] [G loss: 1.000005]\n",
      "1879 [D loss: 0.999964] [G loss: 0.999954]\n",
      "1880 [D loss: 1.000012] [G loss: 0.999948]\n",
      "1881 [D loss: 0.999992] [G loss: 1.000049]\n",
      "1882 [D loss: 1.000068] [G loss: 0.999966]\n",
      "1883 [D loss: 0.999884] [G loss: 1.000054]\n",
      "1884 [D loss: 1.000017] [G loss: 1.000015]\n",
      "1885 [D loss: 1.000074] [G loss: 1.000018]\n",
      "1886 [D loss: 0.999985] [G loss: 0.999970]\n",
      "1887 [D loss: 1.000044] [G loss: 0.999965]\n",
      "1888 [D loss: 1.000010] [G loss: 0.999981]\n",
      "1889 [D loss: 1.000017] [G loss: 0.999918]\n",
      "1890 [D loss: 0.999964] [G loss: 0.999922]\n",
      "1891 [D loss: 1.000106] [G loss: 0.999894]\n",
      "1892 [D loss: 0.999953] [G loss: 0.999911]\n",
      "1893 [D loss: 1.000065] [G loss: 0.999868]\n",
      "1894 [D loss: 0.999980] [G loss: 0.999888]\n",
      "1895 [D loss: 0.999845] [G loss: 0.999944]\n",
      "1896 [D loss: 1.000044] [G loss: 0.999819]\n",
      "1897 [D loss: 1.000023] [G loss: 0.999809]\n",
      "1898 [D loss: 0.999949] [G loss: 0.999979]\n",
      "1899 [D loss: 1.000043] [G loss: 0.999885]\n",
      "1900 [D loss: 0.999942] [G loss: 0.999937]\n",
      "1901 [D loss: 0.999939] [G loss: 0.999973]\n",
      "1902 [D loss: 1.000012] [G loss: 0.999939]\n",
      "1903 [D loss: 1.000050] [G loss: 0.999827]\n",
      "1904 [D loss: 0.999914] [G loss: 0.999970]\n",
      "1905 [D loss: 0.999973] [G loss: 0.999931]\n",
      "1906 [D loss: 1.000013] [G loss: 0.999905]\n",
      "1907 [D loss: 0.999948] [G loss: 1.000123]\n",
      "1908 [D loss: 0.999953] [G loss: 0.999912]\n",
      "1909 [D loss: 1.000070] [G loss: 0.999974]\n",
      "1910 [D loss: 0.999976] [G loss: 1.000031]\n",
      "1911 [D loss: 1.000009] [G loss: 0.999943]\n",
      "1912 [D loss: 1.000014] [G loss: 1.000007]\n",
      "1913 [D loss: 1.000027] [G loss: 0.999833]\n",
      "1914 [D loss: 1.000007] [G loss: 0.999899]\n",
      "1915 [D loss: 1.000076] [G loss: 0.999914]\n",
      "1916 [D loss: 1.000003] [G loss: 0.999853]\n",
      "1917 [D loss: 0.999986] [G loss: 0.999942]\n",
      "1918 [D loss: 0.999993] [G loss: 0.999991]\n",
      "1919 [D loss: 0.999972] [G loss: 0.999949]\n",
      "1920 [D loss: 0.999995] [G loss: 0.999916]\n",
      "1921 [D loss: 1.000071] [G loss: 0.999905]\n",
      "1922 [D loss: 1.000016] [G loss: 0.999856]\n",
      "1923 [D loss: 0.999945] [G loss: 1.000005]\n",
      "1924 [D loss: 0.999986] [G loss: 0.999872]\n",
      "1925 [D loss: 0.999930] [G loss: 0.999983]\n",
      "1926 [D loss: 1.000004] [G loss: 0.999861]\n",
      "1927 [D loss: 0.999995] [G loss: 0.999855]\n",
      "1928 [D loss: 0.999966] [G loss: 0.999897]\n",
      "1929 [D loss: 1.000041] [G loss: 0.999960]\n",
      "1930 [D loss: 1.000019] [G loss: 0.999958]\n",
      "1931 [D loss: 1.000027] [G loss: 1.000018]\n",
      "1932 [D loss: 1.000015] [G loss: 1.000021]\n",
      "1933 [D loss: 0.999979] [G loss: 0.999830]\n",
      "1934 [D loss: 0.999959] [G loss: 0.999946]\n",
      "1935 [D loss: 0.999985] [G loss: 1.000029]\n",
      "1936 [D loss: 1.000053] [G loss: 0.999954]\n",
      "1937 [D loss: 0.999976] [G loss: 0.999919]\n",
      "1938 [D loss: 1.000032] [G loss: 0.999813]\n",
      "1939 [D loss: 1.000077] [G loss: 0.999875]\n",
      "1940 [D loss: 0.999981] [G loss: 0.999987]\n",
      "1941 [D loss: 0.999938] [G loss: 1.000033]\n",
      "1942 [D loss: 0.999974] [G loss: 0.999977]\n",
      "1943 [D loss: 0.999940] [G loss: 1.000021]\n",
      "1944 [D loss: 0.999931] [G loss: 0.999982]\n",
      "1945 [D loss: 1.000038] [G loss: 0.999890]\n",
      "1946 [D loss: 0.999993] [G loss: 0.999958]\n",
      "1947 [D loss: 0.999943] [G loss: 0.999966]\n",
      "1948 [D loss: 0.999998] [G loss: 0.999918]\n",
      "1949 [D loss: 0.999946] [G loss: 0.999995]\n",
      "1950 [D loss: 0.999930] [G loss: 1.000006]\n",
      "1951 [D loss: 1.000065] [G loss: 0.999876]\n",
      "1952 [D loss: 0.999993] [G loss: 0.999866]\n",
      "1953 [D loss: 1.000012] [G loss: 0.999880]\n",
      "1954 [D loss: 0.999983] [G loss: 0.999900]\n",
      "1955 [D loss: 0.999989] [G loss: 0.999916]\n",
      "1956 [D loss: 0.999987] [G loss: 0.999895]\n",
      "1957 [D loss: 1.000048] [G loss: 0.999934]\n",
      "1958 [D loss: 0.999996] [G loss: 1.000112]\n",
      "1959 [D loss: 0.999985] [G loss: 0.999986]\n",
      "1960 [D loss: 1.000027] [G loss: 0.999969]\n",
      "1961 [D loss: 1.000058] [G loss: 0.999744]\n",
      "1962 [D loss: 1.000003] [G loss: 0.999900]\n",
      "1963 [D loss: 1.000058] [G loss: 0.999846]\n",
      "1964 [D loss: 0.999980] [G loss: 1.000003]\n",
      "1965 [D loss: 1.000003] [G loss: 0.999830]\n",
      "1966 [D loss: 1.000076] [G loss: 0.999990]\n",
      "1967 [D loss: 0.999945] [G loss: 1.000021]\n",
      "1968 [D loss: 1.000035] [G loss: 1.000017]\n",
      "1969 [D loss: 0.999974] [G loss: 1.000047]\n",
      "1970 [D loss: 0.999984] [G loss: 0.999954]\n",
      "1971 [D loss: 0.999970] [G loss: 0.999882]\n",
      "1972 [D loss: 0.999967] [G loss: 0.999817]\n",
      "1973 [D loss: 0.999956] [G loss: 0.999967]\n",
      "1974 [D loss: 0.999963] [G loss: 0.999932]\n",
      "1975 [D loss: 0.999972] [G loss: 0.999979]\n",
      "1976 [D loss: 1.000012] [G loss: 0.999945]\n",
      "1977 [D loss: 0.999956] [G loss: 0.999934]\n",
      "1978 [D loss: 1.000013] [G loss: 0.999931]\n",
      "1979 [D loss: 1.000029] [G loss: 0.999888]\n",
      "1980 [D loss: 1.000011] [G loss: 0.999877]\n",
      "1981 [D loss: 0.999995] [G loss: 0.999923]\n",
      "1982 [D loss: 0.999960] [G loss: 1.000058]\n",
      "1983 [D loss: 1.000022] [G loss: 0.999911]\n",
      "1984 [D loss: 0.999911] [G loss: 0.999937]\n",
      "1985 [D loss: 0.999996] [G loss: 0.999971]\n",
      "1986 [D loss: 1.000034] [G loss: 0.999930]\n",
      "1987 [D loss: 0.999976] [G loss: 0.999917]\n",
      "1988 [D loss: 0.999992] [G loss: 0.999810]\n",
      "1989 [D loss: 0.999909] [G loss: 0.999890]\n",
      "1990 [D loss: 1.000063] [G loss: 0.999814]\n",
      "1991 [D loss: 0.999961] [G loss: 0.999812]\n",
      "1992 [D loss: 0.999991] [G loss: 0.999922]\n",
      "1993 [D loss: 0.999987] [G loss: 0.999958]\n",
      "1994 [D loss: 0.999982] [G loss: 0.999966]\n",
      "1995 [D loss: 1.000038] [G loss: 0.999927]\n",
      "1996 [D loss: 0.999911] [G loss: 0.999982]\n",
      "1997 [D loss: 1.000026] [G loss: 1.000047]\n",
      "1998 [D loss: 1.000000] [G loss: 0.999902]\n",
      "1999 [D loss: 1.000023] [G loss: 0.999917]\n",
      "2000 [D loss: 1.000017] [G loss: 0.999913]\n",
      "2001 [D loss: 0.999957] [G loss: 0.999998]\n",
      "2002 [D loss: 0.999942] [G loss: 1.000015]\n",
      "2003 [D loss: 1.000064] [G loss: 0.999935]\n",
      "2004 [D loss: 0.999980] [G loss: 0.999989]\n",
      "2005 [D loss: 0.999989] [G loss: 0.999943]\n",
      "2006 [D loss: 0.999985] [G loss: 0.999832]\n",
      "2007 [D loss: 0.999967] [G loss: 1.000018]\n",
      "2008 [D loss: 1.000043] [G loss: 1.000028]\n",
      "2009 [D loss: 0.999976] [G loss: 1.000108]\n",
      "2010 [D loss: 0.999949] [G loss: 0.999999]\n",
      "2011 [D loss: 1.000072] [G loss: 0.999925]\n",
      "2012 [D loss: 0.999895] [G loss: 0.999974]\n",
      "2013 [D loss: 0.999929] [G loss: 1.000012]\n",
      "2014 [D loss: 1.000003] [G loss: 1.000017]\n",
      "2015 [D loss: 0.999998] [G loss: 0.999883]\n",
      "2016 [D loss: 0.999987] [G loss: 0.999832]\n",
      "2017 [D loss: 1.000029] [G loss: 0.999960]\n",
      "2018 [D loss: 0.999970] [G loss: 0.999973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 [D loss: 0.999994] [G loss: 0.999910]\n",
      "2020 [D loss: 0.999981] [G loss: 1.000001]\n",
      "2021 [D loss: 1.000109] [G loss: 0.999779]\n",
      "2022 [D loss: 0.999994] [G loss: 0.999992]\n",
      "2023 [D loss: 1.000063] [G loss: 0.999924]\n",
      "2024 [D loss: 1.000005] [G loss: 0.999775]\n",
      "2025 [D loss: 1.000027] [G loss: 0.999890]\n",
      "2026 [D loss: 0.999996] [G loss: 0.999884]\n",
      "2027 [D loss: 0.999987] [G loss: 0.999923]\n",
      "2028 [D loss: 0.999996] [G loss: 0.999946]\n",
      "2029 [D loss: 0.999992] [G loss: 0.999887]\n",
      "2030 [D loss: 0.999908] [G loss: 1.000040]\n",
      "2031 [D loss: 1.000012] [G loss: 1.000043]\n",
      "2032 [D loss: 0.999983] [G loss: 0.999873]\n",
      "2033 [D loss: 0.999980] [G loss: 1.000052]\n",
      "2034 [D loss: 0.999944] [G loss: 0.999953]\n",
      "2035 [D loss: 0.999927] [G loss: 0.999923]\n",
      "2036 [D loss: 1.000031] [G loss: 0.999994]\n",
      "2037 [D loss: 0.999978] [G loss: 0.999910]\n",
      "2038 [D loss: 0.999959] [G loss: 0.999919]\n",
      "2039 [D loss: 0.999913] [G loss: 0.999975]\n",
      "2040 [D loss: 0.999962] [G loss: 0.999929]\n",
      "2041 [D loss: 0.999983] [G loss: 0.999981]\n",
      "2042 [D loss: 0.999929] [G loss: 1.000030]\n",
      "2043 [D loss: 1.000015] [G loss: 0.999982]\n",
      "2044 [D loss: 1.000058] [G loss: 0.999877]\n",
      "2045 [D loss: 1.000012] [G loss: 0.999855]\n",
      "2046 [D loss: 1.000103] [G loss: 0.999837]\n",
      "2047 [D loss: 0.999845] [G loss: 0.999949]\n",
      "2048 [D loss: 1.000007] [G loss: 0.999893]\n",
      "2049 [D loss: 0.999927] [G loss: 1.000025]\n",
      "2050 [D loss: 0.999927] [G loss: 0.999904]\n",
      "2051 [D loss: 1.000083] [G loss: 0.999785]\n",
      "2052 [D loss: 1.000007] [G loss: 1.000071]\n",
      "2053 [D loss: 0.999985] [G loss: 0.999959]\n",
      "2054 [D loss: 1.000022] [G loss: 0.999920]\n",
      "2055 [D loss: 1.000050] [G loss: 0.999879]\n",
      "2056 [D loss: 1.000099] [G loss: 0.999880]\n",
      "2057 [D loss: 1.000023] [G loss: 0.999764]\n",
      "2058 [D loss: 0.999974] [G loss: 0.999974]\n",
      "2059 [D loss: 0.999966] [G loss: 0.999844]\n",
      "2060 [D loss: 0.999989] [G loss: 0.999941]\n",
      "2061 [D loss: 1.000027] [G loss: 0.999854]\n",
      "2062 [D loss: 1.000036] [G loss: 0.999871]\n",
      "2063 [D loss: 1.000015] [G loss: 1.000034]\n",
      "2064 [D loss: 1.000006] [G loss: 0.999830]\n",
      "2065 [D loss: 1.000047] [G loss: 0.999885]\n",
      "2066 [D loss: 0.999956] [G loss: 1.000050]\n",
      "2067 [D loss: 0.999943] [G loss: 0.999980]\n",
      "2068 [D loss: 1.000004] [G loss: 0.999977]\n",
      "2069 [D loss: 1.000023] [G loss: 1.000001]\n",
      "2070 [D loss: 1.000031] [G loss: 0.999796]\n",
      "2071 [D loss: 1.000004] [G loss: 0.999929]\n",
      "2072 [D loss: 0.999954] [G loss: 1.000101]\n",
      "2073 [D loss: 1.000058] [G loss: 0.999925]\n",
      "2074 [D loss: 0.999962] [G loss: 0.999912]\n",
      "2075 [D loss: 0.999951] [G loss: 0.999792]\n",
      "2076 [D loss: 0.999996] [G loss: 0.999905]\n",
      "2077 [D loss: 1.000045] [G loss: 0.999876]\n",
      "2078 [D loss: 1.000060] [G loss: 0.999967]\n",
      "2079 [D loss: 1.000048] [G loss: 0.999856]\n",
      "2080 [D loss: 0.999892] [G loss: 1.000046]\n",
      "2081 [D loss: 0.999972] [G loss: 0.999926]\n",
      "2082 [D loss: 0.999962] [G loss: 1.000013]\n",
      "2083 [D loss: 1.000041] [G loss: 0.999867]\n",
      "2084 [D loss: 0.999850] [G loss: 1.000151]\n",
      "2085 [D loss: 1.000025] [G loss: 0.999845]\n",
      "2086 [D loss: 0.999987] [G loss: 0.999872]\n",
      "2087 [D loss: 0.999927] [G loss: 1.000044]\n",
      "2088 [D loss: 1.000029] [G loss: 0.999829]\n",
      "2089 [D loss: 0.999959] [G loss: 0.999795]\n",
      "2090 [D loss: 0.999822] [G loss: 1.000033]\n",
      "2091 [D loss: 1.000114] [G loss: 0.999868]\n",
      "2092 [D loss: 1.000004] [G loss: 0.999923]\n",
      "2093 [D loss: 0.999960] [G loss: 1.000085]\n",
      "2094 [D loss: 1.000003] [G loss: 0.999906]\n",
      "2095 [D loss: 0.999943] [G loss: 0.999938]\n",
      "2096 [D loss: 1.000029] [G loss: 0.999925]\n",
      "2097 [D loss: 1.000017] [G loss: 0.999860]\n",
      "2098 [D loss: 0.999938] [G loss: 0.999948]\n",
      "2099 [D loss: 0.999994] [G loss: 0.999956]\n",
      "2100 [D loss: 0.999993] [G loss: 0.999901]\n",
      "2101 [D loss: 0.999942] [G loss: 1.000017]\n",
      "2102 [D loss: 1.000086] [G loss: 0.999930]\n",
      "2103 [D loss: 1.000038] [G loss: 0.999920]\n",
      "2104 [D loss: 0.999968] [G loss: 0.999940]\n",
      "2105 [D loss: 1.000102] [G loss: 0.999947]\n",
      "2106 [D loss: 1.000001] [G loss: 1.000035]\n",
      "2107 [D loss: 0.999950] [G loss: 0.999994]\n",
      "2108 [D loss: 0.999994] [G loss: 1.000029]\n",
      "2109 [D loss: 1.000011] [G loss: 1.000040]\n",
      "2110 [D loss: 0.999992] [G loss: 0.999998]\n",
      "2111 [D loss: 0.999958] [G loss: 0.999984]\n",
      "2112 [D loss: 0.999966] [G loss: 0.999971]\n",
      "2113 [D loss: 0.999967] [G loss: 0.999954]\n",
      "2114 [D loss: 0.999973] [G loss: 0.999861]\n",
      "2115 [D loss: 0.999998] [G loss: 0.999936]\n",
      "2116 [D loss: 0.999981] [G loss: 0.999890]\n",
      "2117 [D loss: 0.999965] [G loss: 0.999950]\n",
      "2118 [D loss: 0.999937] [G loss: 0.999977]\n",
      "2119 [D loss: 1.000006] [G loss: 0.999998]\n",
      "2120 [D loss: 0.999980] [G loss: 0.999960]\n",
      "2121 [D loss: 1.000063] [G loss: 1.000016]\n",
      "2122 [D loss: 0.999953] [G loss: 1.000023]\n",
      "2123 [D loss: 0.999954] [G loss: 0.999991]\n",
      "2124 [D loss: 0.999939] [G loss: 0.999949]\n",
      "2125 [D loss: 0.999978] [G loss: 1.000012]\n",
      "2126 [D loss: 0.999932] [G loss: 0.999978]\n",
      "2127 [D loss: 1.000010] [G loss: 1.000016]\n",
      "2128 [D loss: 0.999988] [G loss: 0.999955]\n",
      "2129 [D loss: 0.999972] [G loss: 1.000041]\n",
      "2130 [D loss: 0.999981] [G loss: 0.999888]\n",
      "2131 [D loss: 1.000048] [G loss: 0.999946]\n",
      "2132 [D loss: 1.000025] [G loss: 0.999979]\n",
      "2133 [D loss: 1.000008] [G loss: 0.999788]\n",
      "2134 [D loss: 0.999984] [G loss: 0.999808]\n",
      "2135 [D loss: 0.999974] [G loss: 0.999938]\n",
      "2136 [D loss: 0.999942] [G loss: 0.999898]\n",
      "2137 [D loss: 1.000014] [G loss: 1.000101]\n",
      "2138 [D loss: 0.999968] [G loss: 0.999936]\n",
      "2139 [D loss: 0.999985] [G loss: 0.999957]\n",
      "2140 [D loss: 0.999980] [G loss: 0.999895]\n",
      "2141 [D loss: 0.999881] [G loss: 1.000029]\n",
      "2142 [D loss: 0.999982] [G loss: 0.999985]\n",
      "2143 [D loss: 0.999973] [G loss: 0.999998]\n",
      "2144 [D loss: 0.999945] [G loss: 0.999905]\n",
      "2145 [D loss: 1.000006] [G loss: 0.999868]\n",
      "2146 [D loss: 0.999868] [G loss: 1.000074]\n",
      "2147 [D loss: 1.000036] [G loss: 0.999860]\n",
      "2148 [D loss: 0.999959] [G loss: 0.999988]\n",
      "2149 [D loss: 1.000039] [G loss: 1.000048]\n",
      "2150 [D loss: 1.000050] [G loss: 0.999889]\n",
      "2151 [D loss: 0.999982] [G loss: 0.999909]\n",
      "2152 [D loss: 0.999900] [G loss: 1.000013]\n",
      "2153 [D loss: 1.000007] [G loss: 0.999921]\n",
      "2154 [D loss: 0.999955] [G loss: 1.000054]\n",
      "2155 [D loss: 0.999985] [G loss: 0.999964]\n",
      "2156 [D loss: 1.000065] [G loss: 0.999878]\n",
      "2157 [D loss: 0.999989] [G loss: 0.999912]\n",
      "2158 [D loss: 1.000017] [G loss: 0.999870]\n",
      "2159 [D loss: 0.999979] [G loss: 0.999882]\n",
      "2160 [D loss: 1.000011] [G loss: 0.999903]\n",
      "2161 [D loss: 1.000027] [G loss: 0.999867]\n",
      "2162 [D loss: 1.000006] [G loss: 0.999994]\n",
      "2163 [D loss: 0.999997] [G loss: 0.999767]\n",
      "2164 [D loss: 0.999942] [G loss: 1.000022]\n",
      "2165 [D loss: 0.999986] [G loss: 1.000091]\n",
      "2166 [D loss: 0.999980] [G loss: 0.999970]\n",
      "2167 [D loss: 0.999989] [G loss: 0.999906]\n",
      "2168 [D loss: 0.999969] [G loss: 0.999988]\n",
      "2169 [D loss: 0.999985] [G loss: 0.999932]\n",
      "2170 [D loss: 0.999978] [G loss: 1.000016]\n",
      "2171 [D loss: 1.000007] [G loss: 0.999962]\n",
      "2172 [D loss: 0.999944] [G loss: 0.999991]\n",
      "2173 [D loss: 1.000012] [G loss: 0.999894]\n",
      "2174 [D loss: 1.000079] [G loss: 0.999864]\n",
      "2175 [D loss: 0.999903] [G loss: 0.999931]\n",
      "2176 [D loss: 1.000020] [G loss: 0.999987]\n",
      "2177 [D loss: 1.000008] [G loss: 0.999983]\n",
      "2178 [D loss: 0.999909] [G loss: 0.999953]\n",
      "2179 [D loss: 0.999946] [G loss: 1.000102]\n",
      "2180 [D loss: 0.999975] [G loss: 0.999845]\n",
      "2181 [D loss: 0.999955] [G loss: 0.999997]\n",
      "2182 [D loss: 0.999942] [G loss: 0.999863]\n",
      "2183 [D loss: 1.000012] [G loss: 0.999972]\n",
      "2184 [D loss: 0.999955] [G loss: 0.999968]\n",
      "2185 [D loss: 0.999984] [G loss: 0.999981]\n",
      "2186 [D loss: 0.999987] [G loss: 1.000060]\n",
      "2187 [D loss: 0.999969] [G loss: 0.999994]\n",
      "2188 [D loss: 1.000012] [G loss: 0.999989]\n",
      "2189 [D loss: 1.000033] [G loss: 0.999962]\n",
      "2190 [D loss: 1.000006] [G loss: 0.999859]\n",
      "2191 [D loss: 1.000016] [G loss: 0.999918]\n",
      "2192 [D loss: 0.999962] [G loss: 1.000056]\n",
      "2193 [D loss: 1.000046] [G loss: 0.999843]\n",
      "2194 [D loss: 0.999878] [G loss: 1.000055]\n",
      "2195 [D loss: 0.999963] [G loss: 0.999856]\n",
      "2196 [D loss: 1.000030] [G loss: 0.999859]\n",
      "2197 [D loss: 1.000007] [G loss: 0.999737]\n",
      "2198 [D loss: 1.000054] [G loss: 0.999938]\n",
      "2199 [D loss: 0.999943] [G loss: 0.999964]\n",
      "2200 [D loss: 0.999962] [G loss: 0.999909]\n",
      "2201 [D loss: 0.999944] [G loss: 0.999992]\n",
      "2202 [D loss: 0.999928] [G loss: 1.000023]\n",
      "2203 [D loss: 1.000001] [G loss: 1.000026]\n",
      "2204 [D loss: 0.999990] [G loss: 0.999954]\n",
      "2205 [D loss: 0.999949] [G loss: 1.000069]\n",
      "2206 [D loss: 1.000003] [G loss: 0.999994]\n",
      "2207 [D loss: 0.999980] [G loss: 0.999986]\n",
      "2208 [D loss: 0.999968] [G loss: 1.000026]\n",
      "2209 [D loss: 0.999950] [G loss: 1.000029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210 [D loss: 0.999965] [G loss: 1.000040]\n",
      "2211 [D loss: 0.999988] [G loss: 0.999971]\n",
      "2212 [D loss: 0.999987] [G loss: 1.000057]\n",
      "2213 [D loss: 1.000042] [G loss: 0.999900]\n",
      "2214 [D loss: 0.999968] [G loss: 0.999915]\n",
      "2215 [D loss: 0.999960] [G loss: 0.999932]\n",
      "2216 [D loss: 1.000011] [G loss: 1.000065]\n",
      "2217 [D loss: 1.000003] [G loss: 0.999967]\n",
      "2218 [D loss: 0.999951] [G loss: 1.000005]\n",
      "2219 [D loss: 1.000001] [G loss: 0.999946]\n",
      "2220 [D loss: 0.999991] [G loss: 0.999920]\n",
      "2221 [D loss: 0.999962] [G loss: 0.999860]\n",
      "2222 [D loss: 1.000036] [G loss: 0.999931]\n",
      "2223 [D loss: 0.999976] [G loss: 1.000132]\n",
      "2224 [D loss: 1.000043] [G loss: 0.999992]\n",
      "2225 [D loss: 0.999949] [G loss: 0.999952]\n",
      "2226 [D loss: 1.000064] [G loss: 0.999783]\n",
      "2227 [D loss: 1.000070] [G loss: 0.999864]\n",
      "2228 [D loss: 1.000003] [G loss: 0.999981]\n",
      "2229 [D loss: 0.999985] [G loss: 0.999942]\n",
      "2230 [D loss: 0.999923] [G loss: 0.999932]\n",
      "2231 [D loss: 0.999960] [G loss: 0.999935]\n",
      "2232 [D loss: 0.999983] [G loss: 0.999882]\n",
      "2233 [D loss: 1.000029] [G loss: 0.999996]\n",
      "2234 [D loss: 1.000015] [G loss: 0.999920]\n",
      "2235 [D loss: 0.999972] [G loss: 1.000044]\n",
      "2236 [D loss: 1.000060] [G loss: 1.000000]\n",
      "2237 [D loss: 1.000008] [G loss: 0.999880]\n",
      "2238 [D loss: 0.999988] [G loss: 0.999855]\n",
      "2239 [D loss: 0.999971] [G loss: 1.000000]\n",
      "2240 [D loss: 1.000011] [G loss: 0.999779]\n",
      "2241 [D loss: 1.000021] [G loss: 0.999826]\n",
      "2242 [D loss: 0.999901] [G loss: 1.000008]\n",
      "2243 [D loss: 0.999989] [G loss: 0.999946]\n",
      "2244 [D loss: 0.999969] [G loss: 0.999799]\n",
      "2245 [D loss: 1.000002] [G loss: 0.999911]\n",
      "2246 [D loss: 0.999967] [G loss: 0.999874]\n",
      "2247 [D loss: 0.999991] [G loss: 0.999865]\n",
      "2248 [D loss: 0.999985] [G loss: 0.999984]\n",
      "2249 [D loss: 1.000049] [G loss: 1.000048]\n",
      "2250 [D loss: 0.999986] [G loss: 0.999955]\n",
      "2251 [D loss: 1.000081] [G loss: 0.999948]\n",
      "2252 [D loss: 1.000035] [G loss: 0.999862]\n",
      "2253 [D loss: 0.999977] [G loss: 1.000074]\n",
      "2254 [D loss: 0.999952] [G loss: 0.999959]\n",
      "2255 [D loss: 0.999957] [G loss: 0.999963]\n",
      "2256 [D loss: 0.999972] [G loss: 0.999917]\n",
      "2257 [D loss: 1.000000] [G loss: 1.000035]\n",
      "2258 [D loss: 0.999980] [G loss: 1.000076]\n",
      "2259 [D loss: 0.999980] [G loss: 0.999930]\n",
      "2260 [D loss: 1.000000] [G loss: 0.999981]\n",
      "2261 [D loss: 0.999949] [G loss: 0.999912]\n",
      "2262 [D loss: 0.999967] [G loss: 0.999937]\n",
      "2263 [D loss: 1.000008] [G loss: 0.999944]\n",
      "2264 [D loss: 0.999976] [G loss: 0.999952]\n",
      "2265 [D loss: 1.000030] [G loss: 0.999989]\n",
      "2266 [D loss: 1.000002] [G loss: 1.000020]\n",
      "2267 [D loss: 0.999991] [G loss: 1.000056]\n",
      "2268 [D loss: 0.999975] [G loss: 0.999934]\n",
      "2269 [D loss: 1.000040] [G loss: 0.999939]\n",
      "2270 [D loss: 1.000073] [G loss: 0.999936]\n",
      "2271 [D loss: 1.000089] [G loss: 0.999888]\n",
      "2272 [D loss: 0.999924] [G loss: 1.000006]\n",
      "2273 [D loss: 0.999978] [G loss: 1.000032]\n",
      "2274 [D loss: 0.999957] [G loss: 0.999925]\n",
      "2275 [D loss: 1.000049] [G loss: 0.999917]\n",
      "2276 [D loss: 1.000074] [G loss: 0.999925]\n",
      "2277 [D loss: 0.999883] [G loss: 1.000008]\n",
      "2278 [D loss: 1.000013] [G loss: 0.999877]\n",
      "2279 [D loss: 0.999951] [G loss: 0.999829]\n",
      "2280 [D loss: 0.999963] [G loss: 1.000007]\n",
      "2281 [D loss: 1.000104] [G loss: 0.999929]\n",
      "2282 [D loss: 1.000012] [G loss: 0.999853]\n",
      "2283 [D loss: 0.999955] [G loss: 0.999945]\n",
      "2284 [D loss: 1.000002] [G loss: 1.000052]\n",
      "2285 [D loss: 0.999971] [G loss: 1.000018]\n",
      "2286 [D loss: 0.999936] [G loss: 0.999962]\n",
      "2287 [D loss: 0.999963] [G loss: 0.999950]\n",
      "2288 [D loss: 0.999861] [G loss: 1.000007]\n",
      "2289 [D loss: 0.999990] [G loss: 0.999811]\n",
      "2290 [D loss: 0.999969] [G loss: 0.999890]\n",
      "2291 [D loss: 0.999953] [G loss: 0.999944]\n",
      "2292 [D loss: 0.999912] [G loss: 0.999950]\n",
      "2293 [D loss: 0.999940] [G loss: 0.999989]\n",
      "2294 [D loss: 0.999857] [G loss: 0.999964]\n",
      "2295 [D loss: 0.999973] [G loss: 0.999901]\n",
      "2296 [D loss: 0.999922] [G loss: 0.999968]\n",
      "2297 [D loss: 0.999961] [G loss: 0.999990]\n",
      "2298 [D loss: 0.999948] [G loss: 0.999963]\n",
      "2299 [D loss: 0.999956] [G loss: 0.999980]\n",
      "2300 [D loss: 1.000035] [G loss: 0.999879]\n",
      "2301 [D loss: 1.000032] [G loss: 0.999903]\n",
      "2302 [D loss: 0.999978] [G loss: 0.999898]\n",
      "2303 [D loss: 0.999976] [G loss: 0.999863]\n",
      "2304 [D loss: 0.999920] [G loss: 0.999867]\n",
      "2305 [D loss: 1.000080] [G loss: 0.999869]\n",
      "2306 [D loss: 0.999966] [G loss: 0.999980]\n",
      "2307 [D loss: 0.999942] [G loss: 0.999875]\n",
      "2308 [D loss: 0.999950] [G loss: 1.000094]\n",
      "2309 [D loss: 0.999994] [G loss: 0.999886]\n",
      "2310 [D loss: 0.999965] [G loss: 1.000068]\n",
      "2311 [D loss: 0.999999] [G loss: 0.999862]\n",
      "2312 [D loss: 0.999946] [G loss: 0.999941]\n",
      "2313 [D loss: 0.999991] [G loss: 0.999932]\n",
      "2314 [D loss: 1.000019] [G loss: 0.999980]\n",
      "2315 [D loss: 0.999964] [G loss: 0.999884]\n",
      "2316 [D loss: 0.999949] [G loss: 1.000001]\n",
      "2317 [D loss: 0.999967] [G loss: 0.999853]\n",
      "2318 [D loss: 0.999981] [G loss: 0.999995]\n",
      "2319 [D loss: 0.999977] [G loss: 1.000029]\n",
      "2320 [D loss: 1.000019] [G loss: 0.999930]\n",
      "2321 [D loss: 0.999953] [G loss: 0.999964]\n",
      "2322 [D loss: 1.000011] [G loss: 0.999870]\n",
      "2323 [D loss: 0.999949] [G loss: 0.999982]\n",
      "2324 [D loss: 0.999973] [G loss: 0.999970]\n",
      "2325 [D loss: 1.000011] [G loss: 0.999837]\n",
      "2326 [D loss: 0.999995] [G loss: 0.999983]\n",
      "2327 [D loss: 0.999908] [G loss: 0.999972]\n",
      "2328 [D loss: 0.999970] [G loss: 0.999910]\n",
      "2329 [D loss: 1.000008] [G loss: 0.999989]\n",
      "2330 [D loss: 0.999893] [G loss: 0.999904]\n",
      "2331 [D loss: 0.999919] [G loss: 0.999987]\n",
      "2332 [D loss: 0.999946] [G loss: 0.999987]\n",
      "2333 [D loss: 0.999987] [G loss: 0.999969]\n",
      "2334 [D loss: 1.000020] [G loss: 0.999758]\n",
      "2335 [D loss: 0.999956] [G loss: 0.999985]\n",
      "2336 [D loss: 0.999998] [G loss: 0.999952]\n",
      "2337 [D loss: 0.999949] [G loss: 0.999890]\n",
      "2338 [D loss: 0.999995] [G loss: 0.999973]\n",
      "2339 [D loss: 0.999996] [G loss: 0.999837]\n",
      "2340 [D loss: 1.000002] [G loss: 1.000040]\n",
      "2341 [D loss: 0.999970] [G loss: 0.999902]\n",
      "2342 [D loss: 1.000003] [G loss: 0.999989]\n",
      "2343 [D loss: 1.000006] [G loss: 0.999965]\n",
      "2344 [D loss: 0.999951] [G loss: 1.000004]\n",
      "2345 [D loss: 0.999997] [G loss: 0.999935]\n",
      "2346 [D loss: 0.999976] [G loss: 0.999950]\n",
      "2347 [D loss: 0.999998] [G loss: 0.999881]\n",
      "2348 [D loss: 0.999927] [G loss: 0.999863]\n",
      "2349 [D loss: 0.999925] [G loss: 0.999962]\n",
      "2350 [D loss: 1.000085] [G loss: 0.999902]\n",
      "2351 [D loss: 0.999922] [G loss: 0.999916]\n",
      "2352 [D loss: 0.999974] [G loss: 0.999940]\n",
      "2353 [D loss: 0.999998] [G loss: 0.999925]\n",
      "2354 [D loss: 1.000010] [G loss: 0.999936]\n",
      "2355 [D loss: 0.999995] [G loss: 0.999781]\n",
      "2356 [D loss: 0.999982] [G loss: 0.999922]\n",
      "2357 [D loss: 1.000016] [G loss: 0.999818]\n",
      "2358 [D loss: 1.000021] [G loss: 1.000043]\n",
      "2359 [D loss: 0.999923] [G loss: 0.999950]\n",
      "2360 [D loss: 0.999926] [G loss: 1.000033]\n",
      "2361 [D loss: 1.000045] [G loss: 0.999845]\n",
      "2362 [D loss: 1.000016] [G loss: 1.000072]\n",
      "2363 [D loss: 1.000012] [G loss: 1.000015]\n",
      "2364 [D loss: 0.999986] [G loss: 0.999942]\n",
      "2365 [D loss: 0.999938] [G loss: 0.999900]\n",
      "2366 [D loss: 0.999940] [G loss: 0.999961]\n",
      "2367 [D loss: 0.999999] [G loss: 0.999886]\n",
      "2368 [D loss: 1.000043] [G loss: 0.999962]\n",
      "2369 [D loss: 0.999976] [G loss: 0.999923]\n",
      "2370 [D loss: 1.000008] [G loss: 0.999873]\n",
      "2371 [D loss: 1.000015] [G loss: 0.999919]\n",
      "2372 [D loss: 0.999862] [G loss: 0.999940]\n",
      "2373 [D loss: 1.000077] [G loss: 0.999858]\n",
      "2374 [D loss: 0.999983] [G loss: 1.000000]\n",
      "2375 [D loss: 0.999919] [G loss: 0.999903]\n",
      "2376 [D loss: 0.999999] [G loss: 0.999957]\n",
      "2377 [D loss: 1.000006] [G loss: 0.999870]\n",
      "2378 [D loss: 0.999929] [G loss: 0.999955]\n",
      "2379 [D loss: 1.000071] [G loss: 0.999980]\n",
      "2380 [D loss: 1.000063] [G loss: 0.999887]\n",
      "2381 [D loss: 0.999927] [G loss: 1.000003]\n",
      "2382 [D loss: 0.999952] [G loss: 1.000075]\n",
      "2383 [D loss: 1.000066] [G loss: 0.999919]\n",
      "2384 [D loss: 0.999990] [G loss: 0.999927]\n",
      "2385 [D loss: 0.999966] [G loss: 0.999976]\n",
      "2386 [D loss: 0.999949] [G loss: 0.999907]\n",
      "2387 [D loss: 1.000057] [G loss: 0.999945]\n",
      "2388 [D loss: 1.000022] [G loss: 1.000020]\n",
      "2389 [D loss: 0.999937] [G loss: 1.000013]\n",
      "2390 [D loss: 1.000030] [G loss: 0.999922]\n",
      "2391 [D loss: 0.999998] [G loss: 1.000034]\n",
      "2392 [D loss: 0.999980] [G loss: 0.999945]\n",
      "2393 [D loss: 1.000016] [G loss: 1.000008]\n",
      "2394 [D loss: 0.999958] [G loss: 1.000051]\n",
      "2395 [D loss: 0.999992] [G loss: 0.999835]\n",
      "2396 [D loss: 0.999930] [G loss: 0.999977]\n",
      "2397 [D loss: 0.999976] [G loss: 0.999860]\n",
      "2398 [D loss: 0.999949] [G loss: 0.999916]\n",
      "2399 [D loss: 0.999974] [G loss: 0.999897]\n",
      "2400 [D loss: 0.999954] [G loss: 1.000015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2401 [D loss: 1.000050] [G loss: 0.999880]\n",
      "2402 [D loss: 0.999916] [G loss: 0.999938]\n",
      "2403 [D loss: 1.000022] [G loss: 0.999893]\n",
      "2404 [D loss: 1.000124] [G loss: 0.999704]\n",
      "2405 [D loss: 0.999938] [G loss: 0.999982]\n",
      "2406 [D loss: 1.000008] [G loss: 0.999832]\n",
      "2407 [D loss: 1.000004] [G loss: 0.999935]\n",
      "2408 [D loss: 0.999972] [G loss: 0.999902]\n",
      "2409 [D loss: 1.000004] [G loss: 0.999953]\n",
      "2410 [D loss: 1.000004] [G loss: 0.999944]\n",
      "2411 [D loss: 0.999969] [G loss: 0.999998]\n",
      "2412 [D loss: 1.000097] [G loss: 0.999890]\n",
      "2413 [D loss: 0.999922] [G loss: 0.999963]\n",
      "2414 [D loss: 0.999985] [G loss: 0.999950]\n",
      "2415 [D loss: 0.999955] [G loss: 0.999968]\n",
      "2416 [D loss: 1.000035] [G loss: 0.999917]\n",
      "2417 [D loss: 0.999967] [G loss: 0.999982]\n",
      "2418 [D loss: 1.000030] [G loss: 0.999924]\n",
      "2419 [D loss: 0.999968] [G loss: 1.000063]\n",
      "2420 [D loss: 0.999890] [G loss: 0.999957]\n",
      "2421 [D loss: 0.999986] [G loss: 1.000033]\n",
      "2422 [D loss: 1.000019] [G loss: 0.999853]\n",
      "2423 [D loss: 0.999949] [G loss: 1.000004]\n",
      "2424 [D loss: 1.000039] [G loss: 0.999973]\n",
      "2425 [D loss: 0.999979] [G loss: 0.999988]\n",
      "2426 [D loss: 1.000023] [G loss: 0.999945]\n",
      "2427 [D loss: 0.999957] [G loss: 0.999985]\n",
      "2428 [D loss: 1.000076] [G loss: 0.999915]\n",
      "2429 [D loss: 1.000003] [G loss: 0.999993]\n",
      "2430 [D loss: 0.999953] [G loss: 0.999895]\n",
      "2431 [D loss: 0.999978] [G loss: 0.999823]\n",
      "2432 [D loss: 0.999955] [G loss: 0.999886]\n",
      "2433 [D loss: 1.000009] [G loss: 0.999885]\n",
      "2434 [D loss: 1.000015] [G loss: 0.999891]\n",
      "2435 [D loss: 0.999987] [G loss: 0.999948]\n",
      "2436 [D loss: 1.000025] [G loss: 0.999947]\n",
      "2437 [D loss: 0.999999] [G loss: 0.999999]\n",
      "2438 [D loss: 0.999901] [G loss: 0.999977]\n",
      "2439 [D loss: 1.000016] [G loss: 0.999935]\n",
      "2440 [D loss: 0.999964] [G loss: 0.999934]\n",
      "2441 [D loss: 0.999998] [G loss: 0.999974]\n",
      "2442 [D loss: 1.000016] [G loss: 0.999942]\n",
      "2443 [D loss: 0.999991] [G loss: 0.999894]\n",
      "2444 [D loss: 0.999983] [G loss: 0.999884]\n",
      "2445 [D loss: 0.999980] [G loss: 1.000076]\n",
      "2446 [D loss: 0.999991] [G loss: 0.999968]\n",
      "2447 [D loss: 0.999987] [G loss: 0.999946]\n",
      "2448 [D loss: 1.000009] [G loss: 0.999806]\n",
      "2449 [D loss: 0.999998] [G loss: 0.999958]\n",
      "2450 [D loss: 1.000036] [G loss: 0.999896]\n",
      "2451 [D loss: 1.000041] [G loss: 0.999949]\n",
      "2452 [D loss: 0.999979] [G loss: 0.999997]\n",
      "2453 [D loss: 1.000069] [G loss: 0.999940]\n",
      "2454 [D loss: 0.999893] [G loss: 1.000037]\n",
      "2455 [D loss: 0.999941] [G loss: 1.000011]\n",
      "2456 [D loss: 0.999988] [G loss: 0.999831]\n",
      "2457 [D loss: 0.999994] [G loss: 0.999981]\n",
      "2458 [D loss: 1.000031] [G loss: 0.999821]\n",
      "2459 [D loss: 1.000007] [G loss: 0.999867]\n",
      "2460 [D loss: 0.999913] [G loss: 0.999981]\n",
      "2461 [D loss: 0.999974] [G loss: 0.999919]\n",
      "2462 [D loss: 0.999947] [G loss: 1.000002]\n",
      "2463 [D loss: 1.000007] [G loss: 0.999905]\n",
      "2464 [D loss: 1.000043] [G loss: 0.999912]\n",
      "2465 [D loss: 0.999925] [G loss: 1.000028]\n",
      "2466 [D loss: 0.999916] [G loss: 0.999963]\n",
      "2467 [D loss: 0.999921] [G loss: 0.999948]\n",
      "2468 [D loss: 0.999955] [G loss: 0.999847]\n",
      "2469 [D loss: 1.000018] [G loss: 1.000025]\n",
      "2470 [D loss: 1.000019] [G loss: 1.000006]\n",
      "2471 [D loss: 1.000012] [G loss: 1.000019]\n",
      "2472 [D loss: 1.000021] [G loss: 0.999902]\n",
      "2473 [D loss: 1.000007] [G loss: 0.999893]\n",
      "2474 [D loss: 1.000053] [G loss: 1.000044]\n",
      "2475 [D loss: 1.000006] [G loss: 0.999951]\n",
      "2476 [D loss: 0.999967] [G loss: 0.999957]\n",
      "2477 [D loss: 1.000018] [G loss: 0.999922]\n",
      "2478 [D loss: 0.999940] [G loss: 0.999922]\n",
      "2479 [D loss: 0.999980] [G loss: 0.999960]\n",
      "2480 [D loss: 1.000064] [G loss: 1.000016]\n",
      "2481 [D loss: 0.999978] [G loss: 0.999943]\n",
      "2482 [D loss: 0.999886] [G loss: 1.000006]\n",
      "2483 [D loss: 1.000027] [G loss: 0.999912]\n",
      "2484 [D loss: 0.999958] [G loss: 0.999946]\n",
      "2485 [D loss: 0.999994] [G loss: 0.999830]\n",
      "2486 [D loss: 0.999928] [G loss: 1.000009]\n",
      "2487 [D loss: 0.999979] [G loss: 0.999946]\n",
      "2488 [D loss: 1.000052] [G loss: 0.999848]\n",
      "2489 [D loss: 0.999983] [G loss: 0.999895]\n",
      "2490 [D loss: 0.999949] [G loss: 0.999977]\n",
      "2491 [D loss: 1.000026] [G loss: 0.999866]\n",
      "2492 [D loss: 0.999912] [G loss: 1.000036]\n",
      "2493 [D loss: 0.999923] [G loss: 1.000028]\n",
      "2494 [D loss: 0.999966] [G loss: 0.999906]\n",
      "2495 [D loss: 0.999982] [G loss: 0.999994]\n",
      "2496 [D loss: 0.999945] [G loss: 0.999903]\n",
      "2497 [D loss: 1.000092] [G loss: 0.999890]\n",
      "2498 [D loss: 1.000032] [G loss: 0.999971]\n",
      "2499 [D loss: 0.999991] [G loss: 0.999995]\n",
      "2500 [D loss: 0.999992] [G loss: 0.999964]\n",
      "2501 [D loss: 0.999967] [G loss: 0.999961]\n",
      "2502 [D loss: 1.000068] [G loss: 0.999801]\n",
      "2503 [D loss: 0.999901] [G loss: 1.000049]\n",
      "2504 [D loss: 0.999960] [G loss: 0.999934]\n",
      "2505 [D loss: 0.999973] [G loss: 1.000005]\n",
      "2506 [D loss: 0.999901] [G loss: 0.999995]\n",
      "2507 [D loss: 0.999967] [G loss: 0.999958]\n",
      "2508 [D loss: 0.999938] [G loss: 1.000076]\n",
      "2509 [D loss: 0.999997] [G loss: 0.999958]\n",
      "2510 [D loss: 0.999977] [G loss: 0.999912]\n",
      "2511 [D loss: 0.999979] [G loss: 1.000017]\n",
      "2512 [D loss: 0.999991] [G loss: 1.000002]\n",
      "2513 [D loss: 1.000028] [G loss: 0.999881]\n",
      "2514 [D loss: 0.999999] [G loss: 0.999957]\n",
      "2515 [D loss: 0.999986] [G loss: 0.999988]\n",
      "2516 [D loss: 0.999938] [G loss: 0.999917]\n",
      "2517 [D loss: 0.999995] [G loss: 1.000025]\n",
      "2518 [D loss: 0.999953] [G loss: 0.999952]\n",
      "2519 [D loss: 0.999956] [G loss: 1.000007]\n",
      "2520 [D loss: 0.999927] [G loss: 1.000026]\n",
      "2521 [D loss: 1.000005] [G loss: 0.999913]\n",
      "2522 [D loss: 0.999977] [G loss: 0.999962]\n",
      "2523 [D loss: 0.999960] [G loss: 0.999851]\n",
      "2524 [D loss: 0.999949] [G loss: 0.999951]\n",
      "2525 [D loss: 0.999947] [G loss: 0.999881]\n",
      "2526 [D loss: 1.000005] [G loss: 0.999879]\n",
      "2527 [D loss: 1.000014] [G loss: 0.999986]\n",
      "2528 [D loss: 0.999960] [G loss: 1.000056]\n",
      "2529 [D loss: 1.000018] [G loss: 0.999928]\n",
      "2530 [D loss: 1.000012] [G loss: 0.999956]\n",
      "2531 [D loss: 1.000046] [G loss: 0.999957]\n",
      "2532 [D loss: 1.000014] [G loss: 0.999886]\n",
      "2533 [D loss: 0.999996] [G loss: 0.999922]\n",
      "2534 [D loss: 0.999885] [G loss: 0.999938]\n",
      "2535 [D loss: 0.999960] [G loss: 0.999938]\n",
      "2536 [D loss: 1.000025] [G loss: 0.999872]\n",
      "2537 [D loss: 0.999980] [G loss: 1.000039]\n",
      "2538 [D loss: 1.000058] [G loss: 0.999905]\n",
      "2539 [D loss: 0.999965] [G loss: 1.000050]\n",
      "2540 [D loss: 1.000005] [G loss: 0.999970]\n",
      "2541 [D loss: 0.999964] [G loss: 0.999996]\n",
      "2542 [D loss: 1.000016] [G loss: 0.999926]\n",
      "2543 [D loss: 1.000085] [G loss: 0.999892]\n",
      "2544 [D loss: 1.000063] [G loss: 0.999935]\n",
      "2545 [D loss: 1.000017] [G loss: 0.999902]\n",
      "2546 [D loss: 0.999951] [G loss: 0.999947]\n",
      "2547 [D loss: 1.000043] [G loss: 0.999885]\n",
      "2548 [D loss: 0.999954] [G loss: 0.999902]\n",
      "2549 [D loss: 1.000040] [G loss: 0.999877]\n",
      "2550 [D loss: 0.999987] [G loss: 0.999991]\n",
      "2551 [D loss: 1.000033] [G loss: 0.999861]\n",
      "2552 [D loss: 0.999924] [G loss: 1.000031]\n",
      "2553 [D loss: 0.999967] [G loss: 0.999998]\n",
      "2554 [D loss: 0.999977] [G loss: 0.999931]\n",
      "2555 [D loss: 0.999887] [G loss: 1.000008]\n",
      "2556 [D loss: 0.999999] [G loss: 0.999994]\n",
      "2557 [D loss: 1.000014] [G loss: 0.999926]\n",
      "2558 [D loss: 0.999999] [G loss: 0.999945]\n",
      "2559 [D loss: 0.999966] [G loss: 0.999950]\n",
      "2560 [D loss: 0.999921] [G loss: 0.999880]\n",
      "2561 [D loss: 0.999977] [G loss: 0.999949]\n",
      "2562 [D loss: 0.999993] [G loss: 0.999834]\n",
      "2563 [D loss: 0.999998] [G loss: 0.999761]\n",
      "2564 [D loss: 0.999897] [G loss: 1.000008]\n",
      "2565 [D loss: 0.999980] [G loss: 1.000003]\n",
      "2566 [D loss: 1.000032] [G loss: 0.999957]\n",
      "2567 [D loss: 0.999937] [G loss: 0.999921]\n",
      "2568 [D loss: 0.999963] [G loss: 0.999936]\n",
      "2569 [D loss: 0.999924] [G loss: 0.999863]\n",
      "2570 [D loss: 0.999956] [G loss: 0.999985]\n",
      "2571 [D loss: 0.999950] [G loss: 1.000004]\n",
      "2572 [D loss: 0.999988] [G loss: 1.000001]\n",
      "2573 [D loss: 0.999956] [G loss: 1.000057]\n",
      "2574 [D loss: 0.999930] [G loss: 0.999963]\n",
      "2575 [D loss: 0.999951] [G loss: 0.999961]\n",
      "2576 [D loss: 1.000005] [G loss: 0.999992]\n",
      "2577 [D loss: 0.999987] [G loss: 1.000004]\n",
      "2578 [D loss: 0.999976] [G loss: 0.999969]\n",
      "2579 [D loss: 1.000038] [G loss: 0.999867]\n",
      "2580 [D loss: 0.999965] [G loss: 0.999880]\n",
      "2581 [D loss: 1.000007] [G loss: 0.999842]\n",
      "2582 [D loss: 1.000000] [G loss: 0.999969]\n",
      "2583 [D loss: 1.000033] [G loss: 0.999925]\n",
      "2584 [D loss: 0.999981] [G loss: 0.999980]\n",
      "2585 [D loss: 0.999996] [G loss: 0.999958]\n",
      "2586 [D loss: 1.000050] [G loss: 0.999845]\n",
      "2587 [D loss: 1.000092] [G loss: 0.999878]\n",
      "2588 [D loss: 1.000031] [G loss: 0.999872]\n",
      "2589 [D loss: 0.999938] [G loss: 1.000030]\n",
      "2590 [D loss: 1.000078] [G loss: 0.999875]\n",
      "2591 [D loss: 0.999999] [G loss: 1.000028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592 [D loss: 0.999915] [G loss: 1.000027]\n",
      "2593 [D loss: 0.999977] [G loss: 0.999948]\n",
      "2594 [D loss: 1.000047] [G loss: 0.999957]\n",
      "2595 [D loss: 0.999975] [G loss: 0.999928]\n",
      "2596 [D loss: 0.999976] [G loss: 0.999999]\n",
      "2597 [D loss: 0.999956] [G loss: 0.999971]\n",
      "2598 [D loss: 0.999984] [G loss: 0.999925]\n",
      "2599 [D loss: 0.999922] [G loss: 0.999985]\n",
      "2600 [D loss: 1.000015] [G loss: 0.999996]\n",
      "2601 [D loss: 0.999981] [G loss: 1.000013]\n",
      "2602 [D loss: 1.000038] [G loss: 0.999931]\n",
      "2603 [D loss: 1.000029] [G loss: 0.999907]\n",
      "2604 [D loss: 1.000013] [G loss: 0.999949]\n",
      "2605 [D loss: 1.000036] [G loss: 0.999802]\n",
      "2606 [D loss: 0.999941] [G loss: 0.999951]\n",
      "2607 [D loss: 0.999951] [G loss: 1.000013]\n",
      "2608 [D loss: 0.999968] [G loss: 0.999994]\n",
      "2609 [D loss: 0.999955] [G loss: 0.999991]\n",
      "2610 [D loss: 1.000002] [G loss: 0.999974]\n",
      "2611 [D loss: 0.999978] [G loss: 1.000007]\n",
      "2612 [D loss: 0.999902] [G loss: 1.000158]\n",
      "2613 [D loss: 0.999985] [G loss: 0.999933]\n",
      "2614 [D loss: 0.999910] [G loss: 1.000004]\n",
      "2615 [D loss: 0.999892] [G loss: 1.000022]\n",
      "2616 [D loss: 0.999967] [G loss: 0.999945]\n",
      "2617 [D loss: 0.999997] [G loss: 0.999846]\n",
      "2618 [D loss: 0.999936] [G loss: 0.999961]\n",
      "2619 [D loss: 0.999945] [G loss: 0.999958]\n",
      "2620 [D loss: 0.999987] [G loss: 0.999930]\n",
      "2621 [D loss: 0.999938] [G loss: 0.999961]\n",
      "2622 [D loss: 0.999997] [G loss: 0.999999]\n",
      "2623 [D loss: 0.999984] [G loss: 0.999968]\n",
      "2624 [D loss: 0.999894] [G loss: 1.000049]\n",
      "2625 [D loss: 0.999923] [G loss: 1.000013]\n",
      "2626 [D loss: 0.999999] [G loss: 1.000010]\n",
      "2627 [D loss: 0.999988] [G loss: 0.999978]\n",
      "2628 [D loss: 0.999982] [G loss: 0.999960]\n",
      "2629 [D loss: 0.999984] [G loss: 1.000002]\n",
      "2630 [D loss: 1.000045] [G loss: 0.999932]\n",
      "2631 [D loss: 1.000072] [G loss: 0.999946]\n",
      "2632 [D loss: 0.999948] [G loss: 1.000042]\n",
      "2633 [D loss: 1.000046] [G loss: 0.999856]\n",
      "2634 [D loss: 1.000045] [G loss: 1.000012]\n",
      "2635 [D loss: 1.000003] [G loss: 0.999921]\n",
      "2636 [D loss: 0.999937] [G loss: 1.000028]\n",
      "2637 [D loss: 0.999971] [G loss: 0.999927]\n",
      "2638 [D loss: 1.000039] [G loss: 1.000023]\n",
      "2639 [D loss: 0.999978] [G loss: 0.999923]\n",
      "2640 [D loss: 0.999896] [G loss: 1.000030]\n",
      "2641 [D loss: 1.000008] [G loss: 0.999969]\n",
      "2642 [D loss: 0.999990] [G loss: 0.999942]\n",
      "2643 [D loss: 0.999904] [G loss: 0.999963]\n",
      "2644 [D loss: 0.999990] [G loss: 0.999910]\n",
      "2645 [D loss: 1.000051] [G loss: 0.999961]\n",
      "2646 [D loss: 0.999883] [G loss: 1.000013]\n",
      "2647 [D loss: 0.999991] [G loss: 0.999882]\n",
      "2648 [D loss: 1.000041] [G loss: 0.999884]\n",
      "2649 [D loss: 0.999963] [G loss: 1.000057]\n",
      "2650 [D loss: 0.999948] [G loss: 1.000023]\n",
      "2651 [D loss: 0.999998] [G loss: 0.999871]\n",
      "2652 [D loss: 0.999973] [G loss: 0.999961]\n",
      "2653 [D loss: 0.999945] [G loss: 1.000024]\n",
      "2654 [D loss: 0.999952] [G loss: 0.999988]\n",
      "2655 [D loss: 0.999988] [G loss: 0.999885]\n",
      "2656 [D loss: 0.999977] [G loss: 0.999925]\n",
      "2657 [D loss: 1.000051] [G loss: 0.999866]\n",
      "2658 [D loss: 0.999946] [G loss: 0.999856]\n",
      "2659 [D loss: 0.999961] [G loss: 1.000028]\n",
      "2660 [D loss: 0.999928] [G loss: 0.999959]\n",
      "2661 [D loss: 0.999996] [G loss: 0.999969]\n",
      "2662 [D loss: 0.999984] [G loss: 0.999952]\n",
      "2663 [D loss: 0.999979] [G loss: 0.999914]\n",
      "2664 [D loss: 1.000063] [G loss: 0.999884]\n",
      "2665 [D loss: 0.999962] [G loss: 0.999950]\n",
      "2666 [D loss: 1.000025] [G loss: 0.999852]\n",
      "2667 [D loss: 1.000094] [G loss: 0.999862]\n",
      "2668 [D loss: 1.000010] [G loss: 0.999834]\n",
      "2669 [D loss: 0.999937] [G loss: 0.999942]\n",
      "2670 [D loss: 0.999957] [G loss: 0.999980]\n",
      "2671 [D loss: 1.000008] [G loss: 0.999917]\n",
      "2672 [D loss: 0.999977] [G loss: 0.999950]\n",
      "2673 [D loss: 0.999968] [G loss: 0.999904]\n",
      "2674 [D loss: 0.999986] [G loss: 0.999910]\n",
      "2675 [D loss: 1.000017] [G loss: 0.999915]\n",
      "2676 [D loss: 0.999906] [G loss: 1.000110]\n",
      "2677 [D loss: 0.999997] [G loss: 0.999978]\n",
      "2678 [D loss: 1.000008] [G loss: 0.999827]\n",
      "2679 [D loss: 0.999990] [G loss: 0.999905]\n",
      "2680 [D loss: 1.000056] [G loss: 0.999873]\n",
      "2681 [D loss: 0.999938] [G loss: 0.999991]\n",
      "2682 [D loss: 0.999937] [G loss: 1.000000]\n",
      "2683 [D loss: 0.999976] [G loss: 1.000038]\n",
      "2684 [D loss: 0.999942] [G loss: 0.999996]\n",
      "2685 [D loss: 0.999931] [G loss: 0.999965]\n",
      "2686 [D loss: 0.999983] [G loss: 1.000035]\n",
      "2687 [D loss: 1.000045] [G loss: 1.000063]\n",
      "2688 [D loss: 1.000037] [G loss: 0.999942]\n",
      "2689 [D loss: 1.000010] [G loss: 0.999918]\n",
      "2690 [D loss: 0.999977] [G loss: 0.999964]\n",
      "2691 [D loss: 0.999964] [G loss: 1.000021]\n",
      "2692 [D loss: 0.999989] [G loss: 1.000040]\n",
      "2693 [D loss: 0.999927] [G loss: 1.000055]\n",
      "2694 [D loss: 0.999968] [G loss: 1.000005]\n",
      "2695 [D loss: 0.999951] [G loss: 1.000030]\n",
      "2696 [D loss: 1.000030] [G loss: 0.999882]\n",
      "2697 [D loss: 0.999962] [G loss: 1.000006]\n",
      "2698 [D loss: 1.000008] [G loss: 1.000058]\n",
      "2699 [D loss: 0.999982] [G loss: 0.999956]\n",
      "2700 [D loss: 0.999944] [G loss: 0.999928]\n",
      "2701 [D loss: 0.999946] [G loss: 0.999969]\n",
      "2702 [D loss: 0.999964] [G loss: 0.999985]\n",
      "2703 [D loss: 1.000026] [G loss: 0.999948]\n",
      "2704 [D loss: 0.999958] [G loss: 0.999913]\n",
      "2705 [D loss: 0.999979] [G loss: 0.999953]\n",
      "2706 [D loss: 0.999962] [G loss: 0.999876]\n",
      "2707 [D loss: 0.999949] [G loss: 0.999943]\n",
      "2708 [D loss: 0.999978] [G loss: 0.999956]\n",
      "2709 [D loss: 0.999923] [G loss: 1.000024]\n",
      "2710 [D loss: 0.999993] [G loss: 1.000036]\n",
      "2711 [D loss: 0.999920] [G loss: 1.000008]\n",
      "2712 [D loss: 0.999927] [G loss: 1.000031]\n",
      "2713 [D loss: 0.999988] [G loss: 0.999916]\n",
      "2714 [D loss: 0.999976] [G loss: 0.999983]\n",
      "2715 [D loss: 1.000015] [G loss: 1.000042]\n",
      "2716 [D loss: 0.999962] [G loss: 0.999995]\n",
      "2717 [D loss: 0.999944] [G loss: 1.000073]\n",
      "2718 [D loss: 0.999967] [G loss: 0.999968]\n",
      "2719 [D loss: 1.000045] [G loss: 0.999963]\n",
      "2720 [D loss: 1.000066] [G loss: 1.000042]\n",
      "2721 [D loss: 1.000074] [G loss: 1.000024]\n",
      "2722 [D loss: 0.999948] [G loss: 1.000106]\n",
      "2723 [D loss: 0.999979] [G loss: 1.000048]\n",
      "2724 [D loss: 0.999934] [G loss: 1.000128]\n",
      "2725 [D loss: 0.999836] [G loss: 1.000215]\n",
      "2726 [D loss: 1.000010] [G loss: 0.999975]\n",
      "2727 [D loss: 1.000041] [G loss: 0.999998]\n",
      "2728 [D loss: 0.999954] [G loss: 0.999995]\n",
      "2729 [D loss: 1.000028] [G loss: 1.000175]\n",
      "2730 [D loss: 0.999996] [G loss: 0.999972]\n",
      "2731 [D loss: 0.999965] [G loss: 1.000079]\n",
      "2732 [D loss: 0.999977] [G loss: 0.999980]\n",
      "2733 [D loss: 0.999990] [G loss: 1.000029]\n",
      "2734 [D loss: 1.000003] [G loss: 0.999961]\n",
      "2735 [D loss: 0.999939] [G loss: 0.999979]\n",
      "2736 [D loss: 1.000036] [G loss: 0.999864]\n",
      "2737 [D loss: 1.000004] [G loss: 0.999863]\n",
      "2738 [D loss: 1.000042] [G loss: 0.999838]\n",
      "2739 [D loss: 0.999919] [G loss: 0.999924]\n",
      "2740 [D loss: 1.000020] [G loss: 0.999870]\n",
      "2741 [D loss: 1.000003] [G loss: 0.999925]\n",
      "2742 [D loss: 0.999961] [G loss: 0.999921]\n",
      "2743 [D loss: 0.999957] [G loss: 0.999861]\n",
      "2744 [D loss: 1.000032] [G loss: 0.999972]\n",
      "2745 [D loss: 1.000015] [G loss: 0.999926]\n",
      "2746 [D loss: 0.999977] [G loss: 1.000019]\n",
      "2747 [D loss: 0.999943] [G loss: 0.999995]\n",
      "2748 [D loss: 0.999938] [G loss: 0.999973]\n",
      "2749 [D loss: 0.999999] [G loss: 0.999949]\n",
      "2750 [D loss: 1.000022] [G loss: 1.000019]\n",
      "2751 [D loss: 1.000013] [G loss: 0.999900]\n",
      "2752 [D loss: 0.999963] [G loss: 1.000092]\n",
      "2753 [D loss: 0.999956] [G loss: 0.999981]\n",
      "2754 [D loss: 0.999917] [G loss: 0.999964]\n",
      "2755 [D loss: 1.000019] [G loss: 1.000045]\n",
      "2756 [D loss: 1.000000] [G loss: 0.999947]\n",
      "2757 [D loss: 0.999985] [G loss: 0.999942]\n",
      "2758 [D loss: 0.999959] [G loss: 0.999950]\n",
      "2759 [D loss: 0.999977] [G loss: 0.999995]\n",
      "2760 [D loss: 1.000033] [G loss: 0.999869]\n",
      "2761 [D loss: 0.999933] [G loss: 0.999958]\n",
      "2762 [D loss: 1.000030] [G loss: 0.999995]\n",
      "2763 [D loss: 1.000028] [G loss: 0.999978]\n",
      "2764 [D loss: 1.000037] [G loss: 0.999968]\n",
      "2765 [D loss: 1.000021] [G loss: 1.000089]\n",
      "2766 [D loss: 1.000011] [G loss: 0.999988]\n",
      "2767 [D loss: 0.999972] [G loss: 0.999948]\n",
      "2768 [D loss: 1.000049] [G loss: 0.999914]\n",
      "2769 [D loss: 0.999949] [G loss: 0.999899]\n",
      "2770 [D loss: 0.999912] [G loss: 0.999974]\n",
      "2771 [D loss: 0.999940] [G loss: 0.999912]\n",
      "2772 [D loss: 0.999919] [G loss: 0.999950]\n",
      "2773 [D loss: 1.000033] [G loss: 0.999951]\n",
      "2774 [D loss: 0.999945] [G loss: 0.999984]\n",
      "2775 [D loss: 1.000012] [G loss: 0.999841]\n",
      "2776 [D loss: 0.999985] [G loss: 0.999905]\n",
      "2777 [D loss: 0.999963] [G loss: 0.999978]\n",
      "2778 [D loss: 0.999968] [G loss: 0.999928]\n",
      "2779 [D loss: 1.000010] [G loss: 0.999759]\n",
      "2780 [D loss: 0.999937] [G loss: 1.000018]\n",
      "2781 [D loss: 1.000003] [G loss: 0.999963]\n",
      "2782 [D loss: 1.000015] [G loss: 0.999955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2783 [D loss: 0.999969] [G loss: 0.999850]\n",
      "2784 [D loss: 0.999996] [G loss: 0.999892]\n",
      "2785 [D loss: 1.000020] [G loss: 1.000014]\n",
      "2786 [D loss: 0.999967] [G loss: 0.999927]\n",
      "2787 [D loss: 1.000012] [G loss: 0.999989]\n",
      "2788 [D loss: 1.000012] [G loss: 1.000012]\n",
      "2789 [D loss: 0.999894] [G loss: 0.999951]\n",
      "2790 [D loss: 0.999932] [G loss: 1.000026]\n",
      "2791 [D loss: 0.999941] [G loss: 0.999987]\n",
      "2792 [D loss: 0.999997] [G loss: 0.999913]\n",
      "2793 [D loss: 0.999952] [G loss: 0.999926]\n",
      "2794 [D loss: 1.000020] [G loss: 0.999963]\n",
      "2795 [D loss: 0.999992] [G loss: 0.999992]\n",
      "2796 [D loss: 0.999987] [G loss: 1.000020]\n",
      "2797 [D loss: 0.999963] [G loss: 0.999928]\n",
      "2798 [D loss: 0.999997] [G loss: 0.999991]\n",
      "2799 [D loss: 1.000020] [G loss: 0.999895]\n",
      "2800 [D loss: 0.999961] [G loss: 0.999991]\n",
      "2801 [D loss: 0.999956] [G loss: 1.000163]\n",
      "2802 [D loss: 0.999937] [G loss: 1.000038]\n",
      "2803 [D loss: 1.000038] [G loss: 0.999941]\n",
      "2804 [D loss: 1.000021] [G loss: 0.999885]\n",
      "2805 [D loss: 1.000005] [G loss: 0.999961]\n",
      "2806 [D loss: 1.000018] [G loss: 0.999996]\n",
      "2807 [D loss: 0.999917] [G loss: 0.999983]\n",
      "2808 [D loss: 0.999949] [G loss: 0.999978]\n",
      "2809 [D loss: 0.999979] [G loss: 1.000100]\n",
      "2810 [D loss: 0.999978] [G loss: 0.999900]\n",
      "2811 [D loss: 1.000004] [G loss: 0.999887]\n",
      "2812 [D loss: 1.000081] [G loss: 0.999850]\n",
      "2813 [D loss: 1.000022] [G loss: 0.999934]\n",
      "2814 [D loss: 0.999978] [G loss: 0.999922]\n",
      "2815 [D loss: 0.999987] [G loss: 0.999920]\n",
      "2816 [D loss: 0.999995] [G loss: 0.999913]\n",
      "2817 [D loss: 0.999932] [G loss: 0.999956]\n",
      "2818 [D loss: 0.999981] [G loss: 0.999938]\n",
      "2819 [D loss: 0.999976] [G loss: 0.999944]\n",
      "2820 [D loss: 0.999982] [G loss: 1.000049]\n",
      "2821 [D loss: 0.999973] [G loss: 0.999886]\n",
      "2822 [D loss: 0.999986] [G loss: 0.999882]\n",
      "2823 [D loss: 1.000034] [G loss: 0.999949]\n",
      "2824 [D loss: 1.000001] [G loss: 0.999865]\n",
      "2825 [D loss: 0.999960] [G loss: 0.999854]\n",
      "2826 [D loss: 0.999981] [G loss: 0.999866]\n",
      "2827 [D loss: 1.000057] [G loss: 0.999971]\n",
      "2828 [D loss: 0.999954] [G loss: 0.999839]\n",
      "2829 [D loss: 0.999959] [G loss: 1.000095]\n",
      "2830 [D loss: 1.000019] [G loss: 0.999909]\n",
      "2831 [D loss: 1.000023] [G loss: 1.000007]\n",
      "2832 [D loss: 0.999867] [G loss: 1.000186]\n",
      "2833 [D loss: 0.999991] [G loss: 0.999937]\n",
      "2834 [D loss: 0.999950] [G loss: 1.000097]\n",
      "2835 [D loss: 0.999990] [G loss: 0.999951]\n",
      "2836 [D loss: 0.999995] [G loss: 0.999827]\n",
      "2837 [D loss: 0.999955] [G loss: 0.999909]\n",
      "2838 [D loss: 0.999975] [G loss: 0.999955]\n",
      "2839 [D loss: 1.000118] [G loss: 0.999716]\n",
      "2840 [D loss: 0.999950] [G loss: 0.999851]\n",
      "2841 [D loss: 0.999909] [G loss: 0.999946]\n",
      "2842 [D loss: 0.999984] [G loss: 1.000046]\n",
      "2843 [D loss: 1.000016] [G loss: 0.999946]\n",
      "2844 [D loss: 1.000007] [G loss: 0.999945]\n",
      "2845 [D loss: 0.999934] [G loss: 0.999989]\n",
      "2846 [D loss: 1.000000] [G loss: 0.999937]\n",
      "2847 [D loss: 1.000016] [G loss: 0.999866]\n",
      "2848 [D loss: 0.999988] [G loss: 1.000011]\n",
      "2849 [D loss: 0.999943] [G loss: 0.999999]\n",
      "2850 [D loss: 0.999996] [G loss: 0.999927]\n",
      "2851 [D loss: 0.999969] [G loss: 0.999988]\n",
      "2852 [D loss: 0.999974] [G loss: 1.000050]\n",
      "2853 [D loss: 0.999940] [G loss: 0.999987]\n",
      "2854 [D loss: 0.999975] [G loss: 1.000062]\n",
      "2855 [D loss: 0.999985] [G loss: 1.000010]\n",
      "2856 [D loss: 0.999998] [G loss: 0.999995]\n",
      "2857 [D loss: 0.999975] [G loss: 1.000021]\n",
      "2858 [D loss: 0.999971] [G loss: 1.000037]\n",
      "2859 [D loss: 0.999954] [G loss: 1.000053]\n",
      "2860 [D loss: 0.999988] [G loss: 0.999984]\n",
      "2861 [D loss: 0.999957] [G loss: 0.999834]\n",
      "2862 [D loss: 1.000015] [G loss: 1.000092]\n",
      "2863 [D loss: 0.999982] [G loss: 1.000022]\n",
      "2864 [D loss: 1.000073] [G loss: 0.999771]\n",
      "2865 [D loss: 0.999955] [G loss: 0.999920]\n",
      "2866 [D loss: 1.000030] [G loss: 0.999752]\n",
      "2867 [D loss: 1.000017] [G loss: 0.999837]\n",
      "2868 [D loss: 1.000101] [G loss: 0.999899]\n",
      "2869 [D loss: 0.999858] [G loss: 0.999977]\n",
      "2870 [D loss: 0.999904] [G loss: 0.999928]\n",
      "2871 [D loss: 0.999987] [G loss: 0.999978]\n",
      "2872 [D loss: 1.000039] [G loss: 0.999973]\n",
      "2873 [D loss: 0.999977] [G loss: 0.999879]\n",
      "2874 [D loss: 0.999976] [G loss: 0.999918]\n",
      "2875 [D loss: 0.999947] [G loss: 1.000047]\n",
      "2876 [D loss: 0.999979] [G loss: 1.000004]\n",
      "2877 [D loss: 0.999944] [G loss: 1.000044]\n",
      "2878 [D loss: 0.999953] [G loss: 0.999967]\n",
      "2879 [D loss: 0.999961] [G loss: 1.000008]\n",
      "2880 [D loss: 0.999947] [G loss: 0.999999]\n",
      "2881 [D loss: 0.999977] [G loss: 0.999927]\n",
      "2882 [D loss: 0.999986] [G loss: 0.999909]\n",
      "2883 [D loss: 0.999924] [G loss: 0.999966]\n",
      "2884 [D loss: 1.000015] [G loss: 0.999994]\n",
      "2885 [D loss: 0.999965] [G loss: 0.999965]\n",
      "2886 [D loss: 0.999926] [G loss: 1.000066]\n",
      "2887 [D loss: 0.999984] [G loss: 0.999964]\n",
      "2888 [D loss: 1.000029] [G loss: 1.000005]\n",
      "2889 [D loss: 1.000002] [G loss: 0.999991]\n",
      "2890 [D loss: 0.999938] [G loss: 0.999998]\n",
      "2891 [D loss: 1.000037] [G loss: 0.999920]\n",
      "2892 [D loss: 1.000040] [G loss: 0.999856]\n",
      "2893 [D loss: 0.999931] [G loss: 1.000069]\n",
      "2894 [D loss: 1.000015] [G loss: 1.000052]\n",
      "2895 [D loss: 0.999987] [G loss: 0.999942]\n",
      "2896 [D loss: 0.999983] [G loss: 0.999978]\n",
      "2897 [D loss: 0.999933] [G loss: 1.000032]\n",
      "2898 [D loss: 1.000010] [G loss: 0.999901]\n",
      "2899 [D loss: 0.999957] [G loss: 0.999954]\n",
      "2900 [D loss: 1.000096] [G loss: 0.999923]\n",
      "2901 [D loss: 0.999947] [G loss: 1.000094]\n",
      "2902 [D loss: 0.999939] [G loss: 1.000095]\n",
      "2903 [D loss: 0.999997] [G loss: 0.999954]\n",
      "2904 [D loss: 1.000030] [G loss: 0.999959]\n",
      "2905 [D loss: 0.999972] [G loss: 0.999811]\n",
      "2906 [D loss: 0.999995] [G loss: 1.000031]\n",
      "2907 [D loss: 0.999987] [G loss: 0.999948]\n",
      "2908 [D loss: 1.000023] [G loss: 1.000009]\n",
      "2909 [D loss: 0.999969] [G loss: 1.000010]\n",
      "2910 [D loss: 0.999932] [G loss: 1.000028]\n",
      "2911 [D loss: 0.999967] [G loss: 1.000032]\n",
      "2912 [D loss: 1.000044] [G loss: 0.999868]\n",
      "2913 [D loss: 0.999921] [G loss: 1.000056]\n",
      "2914 [D loss: 1.000005] [G loss: 0.999930]\n",
      "2915 [D loss: 0.999996] [G loss: 1.000007]\n",
      "2916 [D loss: 0.999944] [G loss: 0.999956]\n",
      "2917 [D loss: 0.999912] [G loss: 1.000065]\n",
      "2918 [D loss: 0.999955] [G loss: 1.000028]\n",
      "2919 [D loss: 0.999936] [G loss: 0.999991]\n",
      "2920 [D loss: 1.000010] [G loss: 1.000035]\n",
      "2921 [D loss: 0.999956] [G loss: 0.999911]\n",
      "2922 [D loss: 0.999969] [G loss: 0.999918]\n",
      "2923 [D loss: 0.999966] [G loss: 0.999940]\n",
      "2924 [D loss: 0.999953] [G loss: 0.999940]\n",
      "2925 [D loss: 1.000047] [G loss: 0.999932]\n",
      "2926 [D loss: 0.999974] [G loss: 0.999991]\n",
      "2927 [D loss: 0.999972] [G loss: 0.999982]\n",
      "2928 [D loss: 1.000017] [G loss: 0.999914]\n",
      "2929 [D loss: 0.999973] [G loss: 0.999998]\n",
      "2930 [D loss: 1.000030] [G loss: 0.999942]\n",
      "2931 [D loss: 0.999929] [G loss: 1.000047]\n",
      "2932 [D loss: 0.999989] [G loss: 0.999929]\n",
      "2933 [D loss: 0.999970] [G loss: 1.000008]\n",
      "2934 [D loss: 0.999991] [G loss: 0.999945]\n",
      "2935 [D loss: 0.999985] [G loss: 0.999911]\n",
      "2936 [D loss: 1.000060] [G loss: 0.999905]\n",
      "2937 [D loss: 1.000014] [G loss: 0.999863]\n",
      "2938 [D loss: 0.999953] [G loss: 0.999940]\n",
      "2939 [D loss: 0.999967] [G loss: 1.000041]\n",
      "2940 [D loss: 0.999946] [G loss: 1.000011]\n",
      "2941 [D loss: 1.000015] [G loss: 0.999928]\n",
      "2942 [D loss: 0.999995] [G loss: 0.999986]\n",
      "2943 [D loss: 0.999920] [G loss: 1.000117]\n",
      "2944 [D loss: 0.999951] [G loss: 0.999983]\n",
      "2945 [D loss: 1.000007] [G loss: 0.999921]\n",
      "2946 [D loss: 1.000121] [G loss: 0.999974]\n",
      "2947 [D loss: 1.000030] [G loss: 0.999914]\n",
      "2948 [D loss: 0.999871] [G loss: 0.999936]\n",
      "2949 [D loss: 0.999934] [G loss: 0.999970]\n",
      "2950 [D loss: 1.000003] [G loss: 0.999962]\n",
      "2951 [D loss: 0.999943] [G loss: 1.000079]\n",
      "2952 [D loss: 0.999980] [G loss: 0.999998]\n",
      "2953 [D loss: 0.999966] [G loss: 1.000043]\n",
      "2954 [D loss: 0.999954] [G loss: 0.999997]\n",
      "2955 [D loss: 0.999964] [G loss: 1.000020]\n",
      "2956 [D loss: 0.999895] [G loss: 1.000081]\n",
      "2957 [D loss: 0.999943] [G loss: 1.000004]\n",
      "2958 [D loss: 0.999940] [G loss: 0.999986]\n",
      "2959 [D loss: 1.000060] [G loss: 0.999949]\n",
      "2960 [D loss: 0.999918] [G loss: 1.000054]\n",
      "2961 [D loss: 0.999939] [G loss: 0.999932]\n",
      "2962 [D loss: 1.000020] [G loss: 0.999945]\n",
      "2963 [D loss: 1.000038] [G loss: 0.999918]\n",
      "2964 [D loss: 0.999956] [G loss: 1.000032]\n",
      "2965 [D loss: 1.000058] [G loss: 0.999905]\n",
      "2966 [D loss: 0.999968] [G loss: 0.999925]\n",
      "2967 [D loss: 1.000012] [G loss: 0.999966]\n",
      "2968 [D loss: 0.999976] [G loss: 0.999991]\n",
      "2969 [D loss: 1.000001] [G loss: 1.000052]\n",
      "2970 [D loss: 0.999994] [G loss: 0.999941]\n",
      "2971 [D loss: 0.999996] [G loss: 1.000043]\n",
      "2972 [D loss: 0.999981] [G loss: 1.000072]\n",
      "2973 [D loss: 0.999956] [G loss: 0.999999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2974 [D loss: 0.999942] [G loss: 0.999975]\n",
      "2975 [D loss: 1.000012] [G loss: 0.999890]\n",
      "2976 [D loss: 0.999990] [G loss: 0.999950]\n",
      "2977 [D loss: 0.999987] [G loss: 1.000011]\n",
      "2978 [D loss: 1.000001] [G loss: 0.999978]\n",
      "2979 [D loss: 0.999995] [G loss: 0.999974]\n",
      "2980 [D loss: 1.000008] [G loss: 0.999962]\n",
      "2981 [D loss: 0.999992] [G loss: 0.999936]\n",
      "2982 [D loss: 1.000044] [G loss: 0.999877]\n",
      "2983 [D loss: 1.000023] [G loss: 0.999922]\n",
      "2984 [D loss: 0.999969] [G loss: 1.000023]\n",
      "2985 [D loss: 0.999959] [G loss: 0.999936]\n",
      "2986 [D loss: 0.999959] [G loss: 0.999937]\n",
      "2987 [D loss: 0.999994] [G loss: 0.999978]\n",
      "2988 [D loss: 1.000015] [G loss: 0.999998]\n",
      "2989 [D loss: 0.999986] [G loss: 0.999913]\n",
      "2990 [D loss: 1.000025] [G loss: 0.999965]\n",
      "2991 [D loss: 1.000022] [G loss: 0.999993]\n",
      "2992 [D loss: 1.000038] [G loss: 1.000010]\n",
      "2993 [D loss: 0.999968] [G loss: 1.000039]\n",
      "2994 [D loss: 0.999954] [G loss: 1.000053]\n",
      "2995 [D loss: 0.999961] [G loss: 0.999992]\n",
      "2996 [D loss: 0.999967] [G loss: 0.999958]\n",
      "2997 [D loss: 0.999927] [G loss: 1.000035]\n",
      "2998 [D loss: 0.999961] [G loss: 0.999967]\n",
      "2999 [D loss: 0.999976] [G loss: 1.000052]\n",
      "3000 [D loss: 0.999990] [G loss: 1.000055]\n",
      "3001 [D loss: 0.999968] [G loss: 0.999940]\n",
      "3002 [D loss: 0.999973] [G loss: 1.000035]\n",
      "3003 [D loss: 1.000012] [G loss: 0.999954]\n",
      "3004 [D loss: 1.000011] [G loss: 0.999956]\n",
      "3005 [D loss: 0.999964] [G loss: 1.000033]\n",
      "3006 [D loss: 1.000025] [G loss: 1.000002]\n",
      "3007 [D loss: 1.000044] [G loss: 0.999964]\n",
      "3008 [D loss: 1.000018] [G loss: 0.999890]\n",
      "3009 [D loss: 0.999930] [G loss: 1.000061]\n",
      "3010 [D loss: 0.999992] [G loss: 0.999849]\n",
      "3011 [D loss: 0.999960] [G loss: 0.999990]\n",
      "3012 [D loss: 0.999978] [G loss: 0.999953]\n",
      "3013 [D loss: 1.000016] [G loss: 0.999987]\n",
      "3014 [D loss: 1.000009] [G loss: 0.999929]\n",
      "3015 [D loss: 0.999961] [G loss: 1.000013]\n",
      "3016 [D loss: 0.999974] [G loss: 1.000017]\n",
      "3017 [D loss: 1.000004] [G loss: 0.999953]\n",
      "3018 [D loss: 0.999991] [G loss: 1.000130]\n",
      "3019 [D loss: 1.000009] [G loss: 0.999882]\n",
      "3020 [D loss: 0.999967] [G loss: 0.999975]\n",
      "3021 [D loss: 0.999971] [G loss: 1.000045]\n",
      "3022 [D loss: 1.000003] [G loss: 0.999970]\n",
      "3023 [D loss: 0.999967] [G loss: 0.999920]\n",
      "3024 [D loss: 0.999966] [G loss: 0.999997]\n",
      "3025 [D loss: 0.999993] [G loss: 0.999986]\n",
      "3026 [D loss: 0.999917] [G loss: 0.999984]\n",
      "3027 [D loss: 1.000000] [G loss: 0.999968]\n",
      "3028 [D loss: 1.000016] [G loss: 0.999927]\n",
      "3029 [D loss: 0.999989] [G loss: 0.999983]\n",
      "3030 [D loss: 1.000024] [G loss: 0.999912]\n",
      "3031 [D loss: 0.999913] [G loss: 0.999969]\n",
      "3032 [D loss: 0.999970] [G loss: 0.999950]\n",
      "3033 [D loss: 0.999972] [G loss: 0.999964]\n",
      "3034 [D loss: 0.999966] [G loss: 1.000035]\n",
      "3035 [D loss: 1.000042] [G loss: 0.999881]\n",
      "3036 [D loss: 0.999929] [G loss: 0.999943]\n",
      "3037 [D loss: 0.999941] [G loss: 0.999976]\n",
      "3038 [D loss: 1.000047] [G loss: 0.999908]\n",
      "3039 [D loss: 1.000051] [G loss: 0.999873]\n",
      "3040 [D loss: 1.000034] [G loss: 1.000043]\n",
      "3041 [D loss: 0.999971] [G loss: 1.000008]\n",
      "3042 [D loss: 0.999986] [G loss: 0.999965]\n",
      "3043 [D loss: 0.999976] [G loss: 1.000031]\n",
      "3044 [D loss: 0.999983] [G loss: 1.000089]\n",
      "3045 [D loss: 1.000002] [G loss: 0.999965]\n",
      "3046 [D loss: 0.999985] [G loss: 0.999959]\n",
      "3047 [D loss: 1.000004] [G loss: 0.999993]\n",
      "3048 [D loss: 0.999995] [G loss: 1.000076]\n",
      "3049 [D loss: 0.999972] [G loss: 0.999851]\n",
      "3050 [D loss: 0.999993] [G loss: 0.999861]\n",
      "3051 [D loss: 0.999992] [G loss: 0.999957]\n",
      "3052 [D loss: 1.000004] [G loss: 1.000011]\n",
      "3053 [D loss: 1.000021] [G loss: 0.999992]\n",
      "3054 [D loss: 0.999991] [G loss: 0.999956]\n",
      "3055 [D loss: 1.000026] [G loss: 0.999963]\n",
      "3056 [D loss: 0.999914] [G loss: 0.999992]\n",
      "3057 [D loss: 0.999937] [G loss: 1.000009]\n",
      "3058 [D loss: 0.999938] [G loss: 0.999963]\n",
      "3059 [D loss: 0.999939] [G loss: 0.999918]\n",
      "3060 [D loss: 0.999997] [G loss: 0.999992]\n",
      "3061 [D loss: 1.000021] [G loss: 0.999965]\n",
      "3062 [D loss: 0.999991] [G loss: 0.999956]\n",
      "3063 [D loss: 1.000003] [G loss: 0.999971]\n",
      "3064 [D loss: 1.000023] [G loss: 0.999889]\n",
      "3065 [D loss: 1.000081] [G loss: 0.999918]\n",
      "3066 [D loss: 1.000001] [G loss: 0.999948]\n",
      "3067 [D loss: 0.999989] [G loss: 0.999951]\n",
      "3068 [D loss: 0.999977] [G loss: 1.000026]\n",
      "3069 [D loss: 1.000010] [G loss: 0.999971]\n",
      "3070 [D loss: 0.999982] [G loss: 1.000130]\n",
      "3071 [D loss: 0.999915] [G loss: 1.000083]\n",
      "3072 [D loss: 1.000004] [G loss: 1.000083]\n",
      "3073 [D loss: 0.999969] [G loss: 0.999982]\n",
      "3074 [D loss: 0.999996] [G loss: 0.999952]\n",
      "3075 [D loss: 0.999990] [G loss: 1.000032]\n",
      "3076 [D loss: 0.999923] [G loss: 1.000074]\n",
      "3077 [D loss: 1.000006] [G loss: 1.000024]\n",
      "3078 [D loss: 1.000044] [G loss: 0.999924]\n",
      "3079 [D loss: 0.999997] [G loss: 0.999987]\n",
      "3080 [D loss: 1.000123] [G loss: 0.999669]\n",
      "3081 [D loss: 1.000028] [G loss: 0.999841]\n",
      "3082 [D loss: 1.000011] [G loss: 0.999983]\n",
      "3083 [D loss: 0.999950] [G loss: 0.999965]\n",
      "3084 [D loss: 0.999939] [G loss: 0.999973]\n",
      "3085 [D loss: 0.999933] [G loss: 1.000012]\n",
      "3086 [D loss: 0.999972] [G loss: 0.999964]\n",
      "3087 [D loss: 1.000001] [G loss: 0.999874]\n",
      "3088 [D loss: 0.999974] [G loss: 0.999912]\n",
      "3089 [D loss: 0.999998] [G loss: 0.999842]\n",
      "3090 [D loss: 0.999967] [G loss: 1.000002]\n",
      "3091 [D loss: 0.999970] [G loss: 0.999890]\n",
      "3092 [D loss: 0.999940] [G loss: 1.000044]\n",
      "3093 [D loss: 0.999943] [G loss: 1.000020]\n",
      "3094 [D loss: 0.999946] [G loss: 0.999977]\n",
      "3095 [D loss: 0.999974] [G loss: 0.999915]\n",
      "3096 [D loss: 1.000053] [G loss: 0.999947]\n",
      "3097 [D loss: 0.999960] [G loss: 0.999976]\n",
      "3098 [D loss: 1.000002] [G loss: 0.999872]\n",
      "3099 [D loss: 0.999948] [G loss: 0.999958]\n",
      "3100 [D loss: 0.999978] [G loss: 0.999992]\n",
      "3101 [D loss: 0.999942] [G loss: 0.999963]\n",
      "3102 [D loss: 1.000009] [G loss: 0.999984]\n",
      "3103 [D loss: 0.999932] [G loss: 1.000019]\n",
      "3104 [D loss: 1.000010] [G loss: 0.999923]\n",
      "3105 [D loss: 1.000029] [G loss: 0.999961]\n",
      "3106 [D loss: 0.999834] [G loss: 1.000118]\n",
      "3107 [D loss: 0.999990] [G loss: 1.000018]\n",
      "3108 [D loss: 1.000024] [G loss: 0.999976]\n",
      "3109 [D loss: 0.999964] [G loss: 1.000043]\n",
      "3110 [D loss: 0.999998] [G loss: 0.999988]\n",
      "3111 [D loss: 1.000012] [G loss: 0.999945]\n",
      "3112 [D loss: 1.000022] [G loss: 0.999907]\n",
      "3113 [D loss: 0.999918] [G loss: 1.000046]\n",
      "3114 [D loss: 0.999936] [G loss: 0.999922]\n",
      "3115 [D loss: 0.999921] [G loss: 0.999939]\n",
      "3116 [D loss: 0.999955] [G loss: 0.999994]\n",
      "3117 [D loss: 1.000009] [G loss: 0.999981]\n",
      "3118 [D loss: 0.999951] [G loss: 0.999966]\n",
      "3119 [D loss: 0.999993] [G loss: 0.999952]\n",
      "3120 [D loss: 0.999981] [G loss: 0.999855]\n",
      "3121 [D loss: 1.000004] [G loss: 1.000009]\n",
      "3122 [D loss: 0.999958] [G loss: 0.999942]\n",
      "3123 [D loss: 0.999960] [G loss: 0.999909]\n",
      "3124 [D loss: 0.999976] [G loss: 0.999979]\n",
      "3125 [D loss: 0.999990] [G loss: 0.999915]\n",
      "3126 [D loss: 1.000003] [G loss: 0.999950]\n",
      "3127 [D loss: 0.999913] [G loss: 0.999986]\n",
      "3128 [D loss: 0.999993] [G loss: 0.999970]\n",
      "3129 [D loss: 0.999967] [G loss: 0.999998]\n",
      "3130 [D loss: 0.999973] [G loss: 0.999977]\n",
      "3131 [D loss: 1.000015] [G loss: 0.999913]\n",
      "3132 [D loss: 0.999955] [G loss: 1.000016]\n",
      "3133 [D loss: 0.999984] [G loss: 1.000009]\n",
      "3134 [D loss: 0.999991] [G loss: 1.000036]\n",
      "3135 [D loss: 0.999986] [G loss: 1.000051]\n",
      "3136 [D loss: 0.999963] [G loss: 1.000085]\n",
      "3137 [D loss: 0.999975] [G loss: 0.999994]\n",
      "3138 [D loss: 0.999941] [G loss: 0.999998]\n",
      "3139 [D loss: 1.000026] [G loss: 1.000039]\n",
      "3140 [D loss: 0.999987] [G loss: 0.999951]\n",
      "3141 [D loss: 1.000028] [G loss: 0.999973]\n",
      "3142 [D loss: 0.999986] [G loss: 0.999857]\n",
      "3143 [D loss: 0.999946] [G loss: 1.000083]\n",
      "3144 [D loss: 1.000068] [G loss: 0.999978]\n",
      "3145 [D loss: 1.000039] [G loss: 0.999937]\n",
      "3146 [D loss: 0.999970] [G loss: 0.999938]\n",
      "3147 [D loss: 0.999974] [G loss: 0.999912]\n",
      "3148 [D loss: 1.000004] [G loss: 0.999949]\n",
      "3149 [D loss: 1.000018] [G loss: 0.999797]\n",
      "3150 [D loss: 1.000009] [G loss: 0.999735]\n",
      "3151 [D loss: 1.000075] [G loss: 0.999863]\n",
      "3152 [D loss: 0.999909] [G loss: 0.999931]\n",
      "3153 [D loss: 0.999987] [G loss: 1.000007]\n",
      "3154 [D loss: 1.000025] [G loss: 0.999896]\n",
      "3155 [D loss: 1.000128] [G loss: 0.999758]\n",
      "3156 [D loss: 0.999875] [G loss: 0.999999]\n",
      "3157 [D loss: 0.999999] [G loss: 1.000019]\n",
      "3158 [D loss: 0.999968] [G loss: 0.999900]\n",
      "3159 [D loss: 0.999940] [G loss: 0.999925]\n",
      "3160 [D loss: 0.999945] [G loss: 0.999990]\n",
      "3161 [D loss: 0.999927] [G loss: 1.000001]\n",
      "3162 [D loss: 0.999973] [G loss: 1.000003]\n",
      "3163 [D loss: 0.999917] [G loss: 1.000025]\n",
      "3164 [D loss: 1.000022] [G loss: 0.999957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3165 [D loss: 0.999878] [G loss: 1.000057]\n",
      "3166 [D loss: 0.999937] [G loss: 1.000051]\n",
      "3167 [D loss: 0.999963] [G loss: 0.999972]\n",
      "3168 [D loss: 0.999978] [G loss: 0.999989]\n",
      "3169 [D loss: 0.999999] [G loss: 0.999987]\n",
      "3170 [D loss: 0.999923] [G loss: 1.000029]\n",
      "3171 [D loss: 0.999963] [G loss: 0.999999]\n",
      "3172 [D loss: 1.000007] [G loss: 0.999973]\n",
      "3173 [D loss: 0.999937] [G loss: 1.000037]\n",
      "3174 [D loss: 0.999982] [G loss: 0.999998]\n",
      "3175 [D loss: 1.000038] [G loss: 0.999959]\n",
      "3176 [D loss: 0.999970] [G loss: 1.000000]\n",
      "3177 [D loss: 1.000017] [G loss: 0.999967]\n",
      "3178 [D loss: 0.999954] [G loss: 0.999955]\n",
      "3179 [D loss: 0.999947] [G loss: 1.000050]\n",
      "3180 [D loss: 0.999962] [G loss: 0.999954]\n",
      "3181 [D loss: 0.999941] [G loss: 0.999993]\n",
      "3182 [D loss: 0.999944] [G loss: 1.000036]\n",
      "3183 [D loss: 0.999909] [G loss: 0.999924]\n",
      "3184 [D loss: 0.999994] [G loss: 0.999921]\n",
      "3185 [D loss: 1.000010] [G loss: 0.999982]\n",
      "3186 [D loss: 0.999986] [G loss: 1.000110]\n",
      "3187 [D loss: 0.999970] [G loss: 1.000105]\n",
      "3188 [D loss: 0.999988] [G loss: 1.000049]\n",
      "3189 [D loss: 0.999929] [G loss: 0.999957]\n",
      "3190 [D loss: 0.999910] [G loss: 1.000045]\n",
      "3191 [D loss: 0.999959] [G loss: 0.999910]\n",
      "3192 [D loss: 1.000013] [G loss: 0.999947]\n",
      "3193 [D loss: 0.999971] [G loss: 0.999934]\n",
      "3194 [D loss: 0.999919] [G loss: 1.000029]\n",
      "3195 [D loss: 0.999921] [G loss: 0.999964]\n",
      "3196 [D loss: 1.000039] [G loss: 0.999945]\n",
      "3197 [D loss: 1.000018] [G loss: 0.999971]\n",
      "3198 [D loss: 0.999976] [G loss: 0.999871]\n",
      "3199 [D loss: 1.000000] [G loss: 1.000022]\n",
      "3200 [D loss: 0.999955] [G loss: 0.999976]\n",
      "3201 [D loss: 0.999958] [G loss: 1.000019]\n",
      "3202 [D loss: 0.999996] [G loss: 0.999940]\n",
      "3203 [D loss: 1.000008] [G loss: 0.999939]\n",
      "3204 [D loss: 0.999922] [G loss: 1.000139]\n",
      "3205 [D loss: 0.999979] [G loss: 0.999967]\n",
      "3206 [D loss: 1.000008] [G loss: 0.999936]\n",
      "3207 [D loss: 1.000012] [G loss: 0.999924]\n",
      "3208 [D loss: 1.000067] [G loss: 1.000028]\n",
      "3209 [D loss: 0.999954] [G loss: 1.000010]\n",
      "3210 [D loss: 0.999942] [G loss: 1.000022]\n",
      "3211 [D loss: 1.000008] [G loss: 0.999933]\n",
      "3212 [D loss: 1.000017] [G loss: 0.999967]\n",
      "3213 [D loss: 0.999942] [G loss: 0.999957]\n",
      "3214 [D loss: 0.999958] [G loss: 0.999975]\n",
      "3215 [D loss: 1.000007] [G loss: 0.999831]\n",
      "3216 [D loss: 1.000017] [G loss: 0.999889]\n",
      "3217 [D loss: 0.999991] [G loss: 0.999904]\n",
      "3218 [D loss: 1.000006] [G loss: 0.999973]\n",
      "3219 [D loss: 1.000000] [G loss: 0.999992]\n",
      "3220 [D loss: 1.000095] [G loss: 0.999800]\n",
      "3221 [D loss: 1.000018] [G loss: 0.999910]\n",
      "3222 [D loss: 1.000008] [G loss: 0.999979]\n",
      "3223 [D loss: 0.999974] [G loss: 0.999927]\n",
      "3224 [D loss: 1.000029] [G loss: 0.999892]\n",
      "3225 [D loss: 0.999987] [G loss: 0.999918]\n",
      "3226 [D loss: 0.999998] [G loss: 0.999962]\n",
      "3227 [D loss: 0.999876] [G loss: 1.000086]\n",
      "3228 [D loss: 0.999979] [G loss: 0.999929]\n",
      "3229 [D loss: 1.000007] [G loss: 0.999912]\n",
      "3230 [D loss: 0.999905] [G loss: 0.999923]\n",
      "3231 [D loss: 0.999999] [G loss: 0.999980]\n",
      "3232 [D loss: 1.000006] [G loss: 1.000043]\n",
      "3233 [D loss: 1.000002] [G loss: 0.999945]\n",
      "3234 [D loss: 0.999917] [G loss: 1.000016]\n",
      "3235 [D loss: 0.999937] [G loss: 1.000030]\n",
      "3236 [D loss: 0.999975] [G loss: 1.000041]\n",
      "3237 [D loss: 0.999984] [G loss: 1.000013]\n",
      "3238 [D loss: 0.999972] [G loss: 1.000007]\n",
      "3239 [D loss: 0.999982] [G loss: 0.999993]\n",
      "3240 [D loss: 0.999937] [G loss: 0.999866]\n",
      "3241 [D loss: 0.999968] [G loss: 0.999915]\n",
      "3242 [D loss: 0.999970] [G loss: 0.999892]\n",
      "3243 [D loss: 0.999918] [G loss: 1.000000]\n",
      "3244 [D loss: 0.999978] [G loss: 0.999932]\n",
      "3245 [D loss: 1.000045] [G loss: 0.999884]\n",
      "3246 [D loss: 0.999968] [G loss: 0.999949]\n",
      "3247 [D loss: 0.999948] [G loss: 0.999975]\n",
      "3248 [D loss: 0.999984] [G loss: 0.999954]\n",
      "3249 [D loss: 0.999948] [G loss: 1.000035]\n",
      "3250 [D loss: 0.999984] [G loss: 0.999937]\n",
      "3251 [D loss: 0.999943] [G loss: 1.000043]\n",
      "3252 [D loss: 0.999924] [G loss: 1.000024]\n",
      "3253 [D loss: 0.999910] [G loss: 1.000133]\n",
      "3254 [D loss: 0.999949] [G loss: 0.999975]\n",
      "3255 [D loss: 0.999968] [G loss: 0.999975]\n",
      "3256 [D loss: 0.999977] [G loss: 1.000029]\n",
      "3257 [D loss: 0.999990] [G loss: 0.999928]\n",
      "3258 [D loss: 0.999986] [G loss: 0.999937]\n",
      "3259 [D loss: 0.999970] [G loss: 0.999937]\n",
      "3260 [D loss: 1.000015] [G loss: 0.999947]\n",
      "3261 [D loss: 1.000029] [G loss: 0.999874]\n",
      "3262 [D loss: 0.999972] [G loss: 0.999932]\n",
      "3263 [D loss: 1.000009] [G loss: 0.999818]\n",
      "3264 [D loss: 0.999954] [G loss: 0.999936]\n",
      "3265 [D loss: 0.999953] [G loss: 0.999849]\n",
      "3266 [D loss: 0.999923] [G loss: 1.000033]\n",
      "3267 [D loss: 0.999966] [G loss: 0.999939]\n",
      "3268 [D loss: 0.999962] [G loss: 1.000038]\n",
      "3269 [D loss: 0.999945] [G loss: 0.999982]\n",
      "3270 [D loss: 1.000006] [G loss: 0.999993]\n",
      "3271 [D loss: 0.999988] [G loss: 0.999967]\n",
      "3272 [D loss: 0.999935] [G loss: 1.000015]\n",
      "3273 [D loss: 0.999998] [G loss: 0.999970]\n",
      "3274 [D loss: 1.000020] [G loss: 0.999984]\n",
      "3275 [D loss: 1.000008] [G loss: 0.999933]\n",
      "3276 [D loss: 0.999985] [G loss: 0.999997]\n",
      "3277 [D loss: 1.000010] [G loss: 0.999869]\n",
      "3278 [D loss: 1.000028] [G loss: 0.999951]\n",
      "3279 [D loss: 0.999946] [G loss: 1.000008]\n",
      "3280 [D loss: 0.999973] [G loss: 0.999956]\n",
      "3281 [D loss: 1.000052] [G loss: 1.000032]\n",
      "3282 [D loss: 0.999912] [G loss: 1.000085]\n",
      "3283 [D loss: 1.000023] [G loss: 1.000074]\n",
      "3284 [D loss: 0.999974] [G loss: 1.000041]\n",
      "3285 [D loss: 0.999999] [G loss: 0.999994]\n",
      "3286 [D loss: 0.999976] [G loss: 1.000028]\n",
      "3287 [D loss: 0.999919] [G loss: 0.999977]\n",
      "3288 [D loss: 1.000005] [G loss: 0.999950]\n",
      "3289 [D loss: 0.999944] [G loss: 1.000019]\n",
      "3290 [D loss: 0.999963] [G loss: 1.000021]\n",
      "3291 [D loss: 1.000000] [G loss: 0.999906]\n",
      "3292 [D loss: 0.999972] [G loss: 0.999932]\n",
      "3293 [D loss: 1.000008] [G loss: 0.999984]\n",
      "3294 [D loss: 0.999957] [G loss: 1.000076]\n",
      "3295 [D loss: 0.999957] [G loss: 1.000021]\n",
      "3296 [D loss: 1.000002] [G loss: 0.999968]\n",
      "3297 [D loss: 0.999995] [G loss: 1.000040]\n",
      "3298 [D loss: 1.000053] [G loss: 0.999894]\n",
      "3299 [D loss: 0.999992] [G loss: 0.999953]\n",
      "3300 [D loss: 0.999957] [G loss: 0.999924]\n",
      "3301 [D loss: 1.000040] [G loss: 0.999945]\n",
      "3302 [D loss: 0.999999] [G loss: 1.000015]\n",
      "3303 [D loss: 1.000019] [G loss: 0.999908]\n",
      "3304 [D loss: 0.999983] [G loss: 0.999934]\n",
      "3305 [D loss: 0.999966] [G loss: 0.999958]\n",
      "3306 [D loss: 0.999999] [G loss: 0.999942]\n",
      "3307 [D loss: 0.999976] [G loss: 0.999959]\n",
      "3308 [D loss: 0.999950] [G loss: 1.000017]\n",
      "3309 [D loss: 0.999956] [G loss: 0.999992]\n",
      "3310 [D loss: 1.000036] [G loss: 0.999994]\n",
      "3311 [D loss: 1.000011] [G loss: 0.999951]\n",
      "3312 [D loss: 0.999942] [G loss: 1.000034]\n",
      "3313 [D loss: 1.000024] [G loss: 1.000024]\n",
      "3314 [D loss: 0.999947] [G loss: 0.999999]\n",
      "3315 [D loss: 1.000035] [G loss: 1.000012]\n",
      "3316 [D loss: 1.000027] [G loss: 1.000012]\n",
      "3317 [D loss: 0.999950] [G loss: 1.000004]\n",
      "3318 [D loss: 0.999984] [G loss: 1.000016]\n",
      "3319 [D loss: 1.000026] [G loss: 0.999896]\n",
      "3320 [D loss: 0.999978] [G loss: 1.000075]\n",
      "3321 [D loss: 1.000006] [G loss: 1.000085]\n",
      "3322 [D loss: 0.999918] [G loss: 1.000006]\n",
      "3323 [D loss: 0.999960] [G loss: 1.000069]\n",
      "3324 [D loss: 0.999964] [G loss: 0.999993]\n",
      "3325 [D loss: 1.000067] [G loss: 0.999892]\n",
      "3326 [D loss: 0.999978] [G loss: 0.999933]\n",
      "3327 [D loss: 0.999986] [G loss: 0.999918]\n",
      "3328 [D loss: 1.000011] [G loss: 0.999976]\n",
      "3329 [D loss: 0.999956] [G loss: 0.999956]\n",
      "3330 [D loss: 0.999979] [G loss: 0.999992]\n",
      "3331 [D loss: 0.999999] [G loss: 1.000000]\n",
      "3332 [D loss: 0.999993] [G loss: 0.999985]\n",
      "3333 [D loss: 0.999964] [G loss: 1.000004]\n",
      "3334 [D loss: 0.999944] [G loss: 1.000062]\n",
      "3335 [D loss: 0.999945] [G loss: 1.000026]\n",
      "3336 [D loss: 0.999993] [G loss: 0.999995]\n",
      "3337 [D loss: 0.999938] [G loss: 1.000020]\n",
      "3338 [D loss: 0.999908] [G loss: 0.999920]\n",
      "3339 [D loss: 0.999949] [G loss: 1.000006]\n",
      "3340 [D loss: 1.000039] [G loss: 1.000003]\n",
      "3341 [D loss: 1.000025] [G loss: 0.999913]\n",
      "3342 [D loss: 0.999951] [G loss: 1.000079]\n",
      "3343 [D loss: 0.999968] [G loss: 1.000009]\n",
      "3344 [D loss: 0.999949] [G loss: 1.000033]\n",
      "3345 [D loss: 0.999941] [G loss: 0.999910]\n",
      "3346 [D loss: 1.000060] [G loss: 0.999901]\n",
      "3347 [D loss: 1.000007] [G loss: 0.999978]\n",
      "3348 [D loss: 0.999958] [G loss: 0.999925]\n",
      "3349 [D loss: 0.999986] [G loss: 1.000049]\n",
      "3350 [D loss: 0.999965] [G loss: 0.999873]\n",
      "3351 [D loss: 0.999930] [G loss: 1.000006]\n",
      "3352 [D loss: 0.999995] [G loss: 0.999938]\n",
      "3353 [D loss: 0.999972] [G loss: 0.999930]\n",
      "3354 [D loss: 1.000022] [G loss: 1.000007]\n",
      "3355 [D loss: 0.999964] [G loss: 1.000049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3356 [D loss: 0.999977] [G loss: 0.999925]\n",
      "3357 [D loss: 0.999956] [G loss: 0.999978]\n",
      "3358 [D loss: 1.000041] [G loss: 0.999886]\n",
      "3359 [D loss: 0.999952] [G loss: 1.000040]\n",
      "3360 [D loss: 0.999926] [G loss: 0.999995]\n",
      "3361 [D loss: 0.999958] [G loss: 0.999925]\n",
      "3362 [D loss: 0.999966] [G loss: 0.999978]\n",
      "3363 [D loss: 0.999934] [G loss: 0.999958]\n",
      "3364 [D loss: 0.999959] [G loss: 0.999933]\n",
      "3365 [D loss: 1.000054] [G loss: 0.999915]\n",
      "3366 [D loss: 1.000024] [G loss: 0.999929]\n",
      "3367 [D loss: 0.999969] [G loss: 0.999999]\n",
      "3368 [D loss: 1.000001] [G loss: 0.999958]\n",
      "3369 [D loss: 0.999978] [G loss: 0.999962]\n",
      "3370 [D loss: 0.999949] [G loss: 0.999978]\n",
      "3371 [D loss: 0.999991] [G loss: 0.999850]\n",
      "3372 [D loss: 0.999937] [G loss: 1.000092]\n",
      "3373 [D loss: 0.999979] [G loss: 1.000057]\n",
      "3374 [D loss: 0.999925] [G loss: 1.000080]\n",
      "3375 [D loss: 1.000019] [G loss: 1.000018]\n",
      "3376 [D loss: 0.999980] [G loss: 1.000022]\n",
      "3377 [D loss: 0.999898] [G loss: 1.000025]\n",
      "3378 [D loss: 0.999949] [G loss: 1.000063]\n",
      "3379 [D loss: 0.999988] [G loss: 0.999989]\n",
      "3380 [D loss: 1.000042] [G loss: 0.999960]\n",
      "3381 [D loss: 0.999975] [G loss: 0.999940]\n",
      "3382 [D loss: 0.999992] [G loss: 0.999978]\n",
      "3383 [D loss: 0.999947] [G loss: 0.999981]\n",
      "3384 [D loss: 0.999977] [G loss: 0.999924]\n",
      "3385 [D loss: 0.999981] [G loss: 0.999955]\n",
      "3386 [D loss: 1.000003] [G loss: 0.999992]\n",
      "3387 [D loss: 1.000023] [G loss: 0.999886]\n",
      "3388 [D loss: 1.000058] [G loss: 0.999880]\n",
      "3389 [D loss: 0.999984] [G loss: 0.999993]\n",
      "3390 [D loss: 0.999934] [G loss: 1.000060]\n",
      "3391 [D loss: 0.999969] [G loss: 1.000027]\n",
      "3392 [D loss: 1.000016] [G loss: 0.999963]\n",
      "3393 [D loss: 0.999973] [G loss: 1.000007]\n",
      "3394 [D loss: 1.000049] [G loss: 0.999947]\n",
      "3395 [D loss: 0.999919] [G loss: 0.999930]\n",
      "3396 [D loss: 0.999961] [G loss: 1.000088]\n",
      "3397 [D loss: 0.999985] [G loss: 1.000065]\n",
      "3398 [D loss: 1.000018] [G loss: 0.999995]\n",
      "3399 [D loss: 0.999986] [G loss: 0.999995]\n",
      "3400 [D loss: 0.999988] [G loss: 0.999974]\n",
      "3401 [D loss: 0.999949] [G loss: 1.000019]\n",
      "3402 [D loss: 1.000004] [G loss: 0.999909]\n",
      "3403 [D loss: 0.999975] [G loss: 0.999997]\n",
      "3404 [D loss: 0.999999] [G loss: 0.999943]\n",
      "3405 [D loss: 1.000000] [G loss: 0.999988]\n",
      "3406 [D loss: 0.999963] [G loss: 1.000067]\n",
      "3407 [D loss: 1.000036] [G loss: 1.000000]\n",
      "3408 [D loss: 0.999983] [G loss: 0.999917]\n",
      "3409 [D loss: 0.999935] [G loss: 0.999911]\n",
      "3410 [D loss: 0.999999] [G loss: 0.999883]\n",
      "3411 [D loss: 0.999929] [G loss: 1.000112]\n",
      "3412 [D loss: 0.999994] [G loss: 0.999856]\n",
      "3413 [D loss: 0.999953] [G loss: 1.000083]\n",
      "3414 [D loss: 0.999994] [G loss: 1.000059]\n",
      "3415 [D loss: 0.999909] [G loss: 1.000011]\n",
      "3416 [D loss: 0.999918] [G loss: 1.000025]\n",
      "3417 [D loss: 1.000006] [G loss: 0.999962]\n",
      "3418 [D loss: 0.999932] [G loss: 1.000068]\n",
      "3419 [D loss: 0.999988] [G loss: 1.000067]\n",
      "3420 [D loss: 0.999963] [G loss: 1.000014]\n",
      "3421 [D loss: 1.000031] [G loss: 1.000036]\n",
      "3422 [D loss: 1.000003] [G loss: 1.000021]\n",
      "3423 [D loss: 0.999959] [G loss: 1.000024]\n",
      "3424 [D loss: 1.000043] [G loss: 0.999918]\n",
      "3425 [D loss: 0.999973] [G loss: 1.000086]\n",
      "3426 [D loss: 0.999972] [G loss: 1.000054]\n",
      "3427 [D loss: 0.999923] [G loss: 1.000031]\n",
      "3428 [D loss: 0.999961] [G loss: 1.000042]\n",
      "3429 [D loss: 1.000003] [G loss: 1.000029]\n",
      "3430 [D loss: 0.999980] [G loss: 0.999972]\n",
      "3431 [D loss: 0.999990] [G loss: 0.999974]\n",
      "3432 [D loss: 1.000012] [G loss: 0.999978]\n",
      "3433 [D loss: 0.999996] [G loss: 0.999928]\n",
      "3434 [D loss: 1.000014] [G loss: 1.000037]\n",
      "3435 [D loss: 0.999938] [G loss: 0.999958]\n",
      "3436 [D loss: 0.999991] [G loss: 1.000066]\n",
      "3437 [D loss: 0.999926] [G loss: 1.000016]\n",
      "3438 [D loss: 0.999962] [G loss: 0.999939]\n",
      "3439 [D loss: 0.999998] [G loss: 0.999977]\n",
      "3440 [D loss: 0.999962] [G loss: 0.999908]\n",
      "3441 [D loss: 1.000040] [G loss: 0.999931]\n",
      "3442 [D loss: 1.000019] [G loss: 0.999939]\n",
      "3443 [D loss: 0.999919] [G loss: 1.000057]\n",
      "3444 [D loss: 1.000010] [G loss: 0.999917]\n",
      "3445 [D loss: 0.999991] [G loss: 0.999949]\n",
      "3446 [D loss: 0.999996] [G loss: 0.999928]\n",
      "3447 [D loss: 1.000068] [G loss: 0.999942]\n",
      "3448 [D loss: 0.999988] [G loss: 0.999897]\n",
      "3449 [D loss: 0.999971] [G loss: 1.000009]\n",
      "3450 [D loss: 1.000016] [G loss: 1.000013]\n",
      "3451 [D loss: 1.000034] [G loss: 1.000001]\n",
      "3452 [D loss: 0.999901] [G loss: 1.000029]\n",
      "3453 [D loss: 1.000047] [G loss: 1.000035]\n",
      "3454 [D loss: 0.999950] [G loss: 1.000098]\n",
      "3455 [D loss: 0.999979] [G loss: 0.999995]\n",
      "3456 [D loss: 0.999904] [G loss: 1.000049]\n",
      "3457 [D loss: 0.999979] [G loss: 1.000065]\n",
      "3458 [D loss: 0.999997] [G loss: 1.000019]\n",
      "3459 [D loss: 0.999939] [G loss: 0.999980]\n",
      "3460 [D loss: 0.999942] [G loss: 1.000006]\n",
      "3461 [D loss: 0.999953] [G loss: 0.999952]\n",
      "3462 [D loss: 0.999994] [G loss: 0.999999]\n",
      "3463 [D loss: 1.000042] [G loss: 0.999919]\n",
      "3464 [D loss: 0.999902] [G loss: 1.000051]\n",
      "3465 [D loss: 1.000054] [G loss: 0.999887]\n",
      "3466 [D loss: 0.999997] [G loss: 0.999979]\n",
      "3467 [D loss: 0.999999] [G loss: 0.999950]\n",
      "3468 [D loss: 0.999876] [G loss: 1.000099]\n",
      "3469 [D loss: 0.999921] [G loss: 1.000012]\n",
      "3470 [D loss: 0.999939] [G loss: 1.000009]\n",
      "3471 [D loss: 0.999990] [G loss: 1.000006]\n",
      "3472 [D loss: 0.999948] [G loss: 1.000115]\n",
      "3473 [D loss: 0.999998] [G loss: 0.999927]\n",
      "3474 [D loss: 1.000033] [G loss: 1.000003]\n",
      "3475 [D loss: 1.000025] [G loss: 0.999941]\n",
      "3476 [D loss: 0.999956] [G loss: 1.000002]\n",
      "3477 [D loss: 0.999935] [G loss: 0.999999]\n",
      "3478 [D loss: 1.000018] [G loss: 0.999908]\n",
      "3479 [D loss: 0.999924] [G loss: 1.000094]\n",
      "3480 [D loss: 0.999937] [G loss: 0.999983]\n",
      "3481 [D loss: 0.999982] [G loss: 1.000013]\n",
      "3482 [D loss: 0.999950] [G loss: 1.000038]\n",
      "3483 [D loss: 0.999929] [G loss: 1.000053]\n",
      "3484 [D loss: 0.999988] [G loss: 1.000049]\n",
      "3485 [D loss: 0.999988] [G loss: 0.999988]\n",
      "3486 [D loss: 0.999964] [G loss: 1.000036]\n",
      "3487 [D loss: 0.999932] [G loss: 0.999943]\n",
      "3488 [D loss: 0.999970] [G loss: 1.000017]\n",
      "3489 [D loss: 0.999920] [G loss: 1.000052]\n",
      "3490 [D loss: 1.000003] [G loss: 0.999965]\n",
      "3491 [D loss: 1.000009] [G loss: 0.999974]\n",
      "3492 [D loss: 0.999962] [G loss: 0.999982]\n",
      "3493 [D loss: 1.000036] [G loss: 0.999833]\n",
      "3494 [D loss: 1.000039] [G loss: 0.999964]\n",
      "3495 [D loss: 0.999944] [G loss: 1.000097]\n",
      "3496 [D loss: 0.999970] [G loss: 0.999982]\n",
      "3497 [D loss: 0.999963] [G loss: 0.999960]\n",
      "3498 [D loss: 0.999968] [G loss: 1.000029]\n",
      "3499 [D loss: 1.000002] [G loss: 0.999913]\n",
      "3500 [D loss: 1.000002] [G loss: 0.999861]\n",
      "3501 [D loss: 0.999919] [G loss: 1.000057]\n",
      "3502 [D loss: 0.999949] [G loss: 0.999960]\n",
      "3503 [D loss: 0.999979] [G loss: 1.000003]\n",
      "3504 [D loss: 0.999934] [G loss: 0.999940]\n",
      "3505 [D loss: 0.999968] [G loss: 0.999921]\n",
      "3506 [D loss: 0.999968] [G loss: 0.999982]\n",
      "3507 [D loss: 1.000013] [G loss: 0.999895]\n",
      "3508 [D loss: 1.000021] [G loss: 0.999886]\n",
      "3509 [D loss: 0.999915] [G loss: 0.999993]\n",
      "3510 [D loss: 0.999973] [G loss: 0.999979]\n",
      "3511 [D loss: 1.000007] [G loss: 0.999942]\n",
      "3512 [D loss: 0.999957] [G loss: 0.999975]\n",
      "3513 [D loss: 0.999972] [G loss: 1.000000]\n",
      "3514 [D loss: 0.999956] [G loss: 1.000042]\n",
      "3515 [D loss: 0.999985] [G loss: 1.000001]\n",
      "3516 [D loss: 0.999942] [G loss: 1.000027]\n",
      "3517 [D loss: 0.999966] [G loss: 0.999944]\n",
      "3518 [D loss: 1.000006] [G loss: 0.999976]\n",
      "3519 [D loss: 0.999954] [G loss: 0.999997]\n",
      "3520 [D loss: 1.000009] [G loss: 0.999980]\n",
      "3521 [D loss: 0.999965] [G loss: 0.999983]\n",
      "3522 [D loss: 0.999979] [G loss: 0.999984]\n",
      "3523 [D loss: 0.999969] [G loss: 0.999956]\n",
      "3524 [D loss: 0.999936] [G loss: 1.000005]\n",
      "3525 [D loss: 0.999994] [G loss: 1.000043]\n",
      "3526 [D loss: 0.999956] [G loss: 1.000039]\n",
      "3527 [D loss: 1.000028] [G loss: 0.999937]\n",
      "3528 [D loss: 0.999974] [G loss: 0.999968]\n",
      "3529 [D loss: 0.999993] [G loss: 1.000020]\n",
      "3530 [D loss: 0.999978] [G loss: 1.000102]\n",
      "3531 [D loss: 0.999947] [G loss: 1.000042]\n",
      "3532 [D loss: 0.999980] [G loss: 1.000054]\n",
      "3533 [D loss: 0.999985] [G loss: 1.000004]\n",
      "3534 [D loss: 0.999953] [G loss: 1.000009]\n",
      "3535 [D loss: 0.999921] [G loss: 0.999943]\n",
      "3536 [D loss: 0.999988] [G loss: 0.999901]\n",
      "3537 [D loss: 0.999974] [G loss: 1.000005]\n",
      "3538 [D loss: 1.000014] [G loss: 1.000044]\n",
      "3539 [D loss: 0.999945] [G loss: 1.000044]\n",
      "3540 [D loss: 0.999946] [G loss: 1.000009]\n",
      "3541 [D loss: 0.999961] [G loss: 0.999991]\n",
      "3542 [D loss: 0.999928] [G loss: 1.000020]\n",
      "3543 [D loss: 0.999994] [G loss: 0.999953]\n",
      "3544 [D loss: 1.000004] [G loss: 0.999956]\n",
      "3545 [D loss: 0.999912] [G loss: 1.000027]\n",
      "3546 [D loss: 0.999941] [G loss: 1.000018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547 [D loss: 1.000000] [G loss: 1.000041]\n",
      "3548 [D loss: 0.999936] [G loss: 1.000039]\n",
      "3549 [D loss: 1.000037] [G loss: 0.999983]\n",
      "3550 [D loss: 0.999968] [G loss: 1.000043]\n",
      "3551 [D loss: 0.999956] [G loss: 1.000006]\n",
      "3552 [D loss: 0.999994] [G loss: 1.000007]\n",
      "3553 [D loss: 0.999967] [G loss: 1.000069]\n",
      "3554 [D loss: 0.999954] [G loss: 0.999952]\n",
      "3555 [D loss: 1.000026] [G loss: 0.999974]\n",
      "3556 [D loss: 0.999953] [G loss: 1.000021]\n",
      "3557 [D loss: 0.999999] [G loss: 0.999971]\n",
      "3558 [D loss: 0.999908] [G loss: 1.000012]\n",
      "3559 [D loss: 0.999960] [G loss: 1.000020]\n",
      "3560 [D loss: 0.999997] [G loss: 0.999964]\n",
      "3561 [D loss: 0.999959] [G loss: 0.999997]\n",
      "3562 [D loss: 0.999988] [G loss: 0.999978]\n",
      "3563 [D loss: 1.000007] [G loss: 0.999873]\n",
      "3564 [D loss: 0.999968] [G loss: 0.999959]\n",
      "3565 [D loss: 1.000004] [G loss: 1.000000]\n",
      "3566 [D loss: 0.999983] [G loss: 1.000005]\n",
      "3567 [D loss: 0.999992] [G loss: 0.999985]\n",
      "3568 [D loss: 0.999949] [G loss: 0.999980]\n",
      "3569 [D loss: 0.999957] [G loss: 1.000022]\n",
      "3570 [D loss: 1.000072] [G loss: 0.999972]\n",
      "3571 [D loss: 0.999964] [G loss: 1.000060]\n",
      "3572 [D loss: 1.000027] [G loss: 0.999982]\n",
      "3573 [D loss: 1.000014] [G loss: 1.000013]\n",
      "3574 [D loss: 0.999944] [G loss: 1.000028]\n",
      "3575 [D loss: 0.999987] [G loss: 0.999969]\n",
      "3576 [D loss: 1.000055] [G loss: 0.999914]\n",
      "3577 [D loss: 0.999996] [G loss: 0.999966]\n",
      "3578 [D loss: 0.999968] [G loss: 1.000020]\n",
      "3579 [D loss: 0.999982] [G loss: 1.000095]\n",
      "3580 [D loss: 0.999993] [G loss: 1.000005]\n",
      "3581 [D loss: 0.999911] [G loss: 1.000023]\n",
      "3582 [D loss: 0.999990] [G loss: 0.999946]\n",
      "3583 [D loss: 0.999983] [G loss: 1.000051]\n",
      "3584 [D loss: 1.000003] [G loss: 1.000061]\n",
      "3585 [D loss: 0.999999] [G loss: 1.000000]\n",
      "3586 [D loss: 0.999962] [G loss: 1.000035]\n",
      "3587 [D loss: 0.999990] [G loss: 1.000000]\n",
      "3588 [D loss: 0.999937] [G loss: 1.000070]\n",
      "3589 [D loss: 0.999922] [G loss: 0.999951]\n",
      "3590 [D loss: 0.999961] [G loss: 1.000080]\n",
      "3591 [D loss: 0.999960] [G loss: 1.000038]\n",
      "3592 [D loss: 0.999920] [G loss: 1.000033]\n",
      "3593 [D loss: 1.000010] [G loss: 0.999930]\n",
      "3594 [D loss: 0.999944] [G loss: 1.000016]\n",
      "3595 [D loss: 0.999937] [G loss: 1.000038]\n",
      "3596 [D loss: 0.999999] [G loss: 1.000008]\n",
      "3597 [D loss: 0.999968] [G loss: 0.999893]\n",
      "3598 [D loss: 0.999908] [G loss: 0.999953]\n",
      "3599 [D loss: 0.999966] [G loss: 0.999976]\n",
      "3600 [D loss: 1.000044] [G loss: 0.999966]\n",
      "3601 [D loss: 0.999961] [G loss: 0.999993]\n",
      "3602 [D loss: 0.999984] [G loss: 0.999968]\n",
      "3603 [D loss: 0.999984] [G loss: 0.999959]\n",
      "3604 [D loss: 0.999976] [G loss: 1.000016]\n",
      "3605 [D loss: 0.999931] [G loss: 1.000045]\n",
      "3606 [D loss: 0.999933] [G loss: 0.999996]\n",
      "3607 [D loss: 0.999944] [G loss: 0.999979]\n",
      "3608 [D loss: 0.999985] [G loss: 1.000031]\n",
      "3609 [D loss: 0.999970] [G loss: 0.999955]\n",
      "3610 [D loss: 0.999949] [G loss: 1.000034]\n",
      "3611 [D loss: 0.999939] [G loss: 0.999975]\n",
      "3612 [D loss: 0.999938] [G loss: 1.000044]\n",
      "3613 [D loss: 0.999970] [G loss: 1.000038]\n",
      "3614 [D loss: 1.000009] [G loss: 0.999986]\n",
      "3615 [D loss: 0.999984] [G loss: 0.999994]\n",
      "3616 [D loss: 0.999987] [G loss: 0.999990]\n",
      "3617 [D loss: 1.000024] [G loss: 1.000006]\n",
      "3618 [D loss: 0.999963] [G loss: 0.999997]\n",
      "3619 [D loss: 1.000047] [G loss: 0.999901]\n",
      "3620 [D loss: 0.999975] [G loss: 0.999917]\n",
      "3621 [D loss: 1.000032] [G loss: 0.999975]\n",
      "3622 [D loss: 0.999981] [G loss: 1.000054]\n",
      "3623 [D loss: 0.999989] [G loss: 0.999973]\n",
      "3624 [D loss: 0.999998] [G loss: 0.999937]\n",
      "3625 [D loss: 0.999959] [G loss: 1.000022]\n",
      "3626 [D loss: 0.999954] [G loss: 1.000050]\n",
      "3627 [D loss: 0.999955] [G loss: 0.999946]\n",
      "3628 [D loss: 0.999995] [G loss: 0.999949]\n",
      "3629 [D loss: 0.999956] [G loss: 1.000004]\n",
      "3630 [D loss: 0.999964] [G loss: 1.000042]\n",
      "3631 [D loss: 1.000082] [G loss: 0.999915]\n",
      "3632 [D loss: 0.999992] [G loss: 0.999952]\n",
      "3633 [D loss: 0.999913] [G loss: 1.000097]\n",
      "3634 [D loss: 0.999981] [G loss: 1.000008]\n",
      "3635 [D loss: 0.999990] [G loss: 0.999964]\n",
      "3636 [D loss: 1.000024] [G loss: 0.999983]\n",
      "3637 [D loss: 0.999944] [G loss: 1.000013]\n",
      "3638 [D loss: 1.000007] [G loss: 1.000007]\n",
      "3639 [D loss: 0.999977] [G loss: 1.000034]\n",
      "3640 [D loss: 0.999978] [G loss: 1.000007]\n",
      "3641 [D loss: 0.999989] [G loss: 0.999968]\n",
      "3642 [D loss: 0.999985] [G loss: 0.999940]\n",
      "3643 [D loss: 0.999973] [G loss: 0.999970]\n",
      "3644 [D loss: 1.000031] [G loss: 0.999946]\n",
      "3645 [D loss: 0.999974] [G loss: 0.999922]\n",
      "3646 [D loss: 1.000059] [G loss: 1.000007]\n",
      "3647 [D loss: 0.999984] [G loss: 0.999957]\n",
      "3648 [D loss: 0.999926] [G loss: 1.000026]\n",
      "3649 [D loss: 0.999988] [G loss: 0.999939]\n",
      "3650 [D loss: 0.999988] [G loss: 0.999941]\n",
      "3651 [D loss: 0.999974] [G loss: 0.999980]\n",
      "3652 [D loss: 0.999960] [G loss: 0.999980]\n",
      "3653 [D loss: 0.999945] [G loss: 0.999947]\n",
      "3654 [D loss: 0.999966] [G loss: 1.000017]\n",
      "3655 [D loss: 1.000002] [G loss: 1.000038]\n",
      "3656 [D loss: 0.999963] [G loss: 1.000043]\n",
      "3657 [D loss: 0.999948] [G loss: 1.000104]\n",
      "3658 [D loss: 0.999981] [G loss: 1.000019]\n",
      "3659 [D loss: 1.000033] [G loss: 0.999966]\n",
      "3660 [D loss: 0.999982] [G loss: 1.000049]\n",
      "3661 [D loss: 0.999971] [G loss: 1.000036]\n",
      "3662 [D loss: 0.999932] [G loss: 1.000074]\n",
      "3663 [D loss: 0.999967] [G loss: 1.000070]\n",
      "3664 [D loss: 0.999931] [G loss: 1.000062]\n",
      "3665 [D loss: 0.999943] [G loss: 1.000014]\n",
      "3666 [D loss: 0.999958] [G loss: 1.000018]\n",
      "3667 [D loss: 0.999950] [G loss: 1.000020]\n",
      "3668 [D loss: 0.999992] [G loss: 0.999990]\n",
      "3669 [D loss: 0.999898] [G loss: 0.999972]\n",
      "3670 [D loss: 1.000012] [G loss: 1.000043]\n",
      "3671 [D loss: 1.000049] [G loss: 0.999961]\n",
      "3672 [D loss: 0.999990] [G loss: 1.000046]\n",
      "3673 [D loss: 0.999912] [G loss: 1.000103]\n",
      "3674 [D loss: 0.999976] [G loss: 1.000072]\n",
      "3675 [D loss: 0.999948] [G loss: 0.999983]\n",
      "3676 [D loss: 0.999963] [G loss: 0.999984]\n",
      "3677 [D loss: 0.999967] [G loss: 0.999994]\n",
      "3678 [D loss: 0.999957] [G loss: 1.000048]\n",
      "3679 [D loss: 1.000064] [G loss: 0.999893]\n",
      "3680 [D loss: 0.999969] [G loss: 1.000056]\n",
      "3681 [D loss: 0.999960] [G loss: 0.999961]\n",
      "3682 [D loss: 0.999982] [G loss: 0.999964]\n",
      "3683 [D loss: 0.999996] [G loss: 0.999976]\n",
      "3684 [D loss: 0.999964] [G loss: 1.000067]\n",
      "3685 [D loss: 0.999990] [G loss: 0.999947]\n",
      "3686 [D loss: 1.000010] [G loss: 0.999965]\n",
      "3687 [D loss: 1.000012] [G loss: 1.000026]\n",
      "3688 [D loss: 0.999968] [G loss: 1.000051]\n",
      "3689 [D loss: 0.999930] [G loss: 0.999996]\n",
      "3690 [D loss: 0.999991] [G loss: 1.000019]\n",
      "3691 [D loss: 0.999915] [G loss: 0.999996]\n",
      "3692 [D loss: 0.999927] [G loss: 0.999996]\n",
      "3693 [D loss: 0.999992] [G loss: 1.000001]\n",
      "3694 [D loss: 0.999969] [G loss: 1.000040]\n",
      "3695 [D loss: 0.999962] [G loss: 1.000038]\n",
      "3696 [D loss: 1.000002] [G loss: 0.999976]\n",
      "3697 [D loss: 0.999983] [G loss: 0.999994]\n",
      "3698 [D loss: 0.999958] [G loss: 1.000017]\n",
      "3699 [D loss: 0.999954] [G loss: 1.000008]\n",
      "3700 [D loss: 0.999957] [G loss: 1.000069]\n",
      "3701 [D loss: 1.000021] [G loss: 0.999881]\n",
      "3702 [D loss: 0.999954] [G loss: 0.999995]\n",
      "3703 [D loss: 0.999971] [G loss: 1.000025]\n",
      "3704 [D loss: 0.999991] [G loss: 1.000024]\n",
      "3705 [D loss: 1.000034] [G loss: 1.000026]\n",
      "3706 [D loss: 0.999969] [G loss: 1.000019]\n",
      "3707 [D loss: 0.999972] [G loss: 0.999908]\n",
      "3708 [D loss: 0.999988] [G loss: 1.000029]\n",
      "3709 [D loss: 1.000070] [G loss: 0.999968]\n",
      "3710 [D loss: 0.999979] [G loss: 1.000018]\n",
      "3711 [D loss: 0.999970] [G loss: 1.000002]\n",
      "3712 [D loss: 1.000010] [G loss: 1.000014]\n",
      "3713 [D loss: 0.999962] [G loss: 1.000022]\n",
      "3714 [D loss: 0.999965] [G loss: 1.000059]\n",
      "3715 [D loss: 1.000015] [G loss: 1.000042]\n",
      "3716 [D loss: 1.000020] [G loss: 1.000011]\n",
      "3717 [D loss: 0.999975] [G loss: 1.000076]\n",
      "3718 [D loss: 1.000007] [G loss: 0.999986]\n",
      "3719 [D loss: 0.999968] [G loss: 1.000003]\n",
      "3720 [D loss: 0.999945] [G loss: 1.000080]\n",
      "3721 [D loss: 0.999980] [G loss: 1.000059]\n",
      "3722 [D loss: 0.999974] [G loss: 0.999971]\n",
      "3723 [D loss: 0.999980] [G loss: 0.999995]\n",
      "3724 [D loss: 0.999986] [G loss: 1.000013]\n",
      "3725 [D loss: 0.999988] [G loss: 0.999959]\n",
      "3726 [D loss: 0.999990] [G loss: 0.999973]\n",
      "3727 [D loss: 0.999915] [G loss: 1.000047]\n",
      "3728 [D loss: 0.999991] [G loss: 1.000051]\n",
      "3729 [D loss: 0.999962] [G loss: 1.000046]\n",
      "3730 [D loss: 0.999980] [G loss: 1.000044]\n",
      "3731 [D loss: 0.999962] [G loss: 1.000039]\n",
      "3732 [D loss: 0.999930] [G loss: 0.999987]\n",
      "3733 [D loss: 0.999948] [G loss: 1.000071]\n",
      "3734 [D loss: 0.999974] [G loss: 0.999927]\n",
      "3735 [D loss: 0.999974] [G loss: 1.000075]\n",
      "3736 [D loss: 1.000001] [G loss: 0.999993]\n",
      "3737 [D loss: 1.000013] [G loss: 0.999955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3738 [D loss: 0.999933] [G loss: 0.999919]\n",
      "3739 [D loss: 0.999943] [G loss: 1.000022]\n",
      "3740 [D loss: 1.000048] [G loss: 1.000051]\n",
      "3741 [D loss: 0.999906] [G loss: 1.000056]\n",
      "3742 [D loss: 0.999911] [G loss: 1.000029]\n",
      "3743 [D loss: 1.000033] [G loss: 0.999992]\n",
      "3744 [D loss: 1.000015] [G loss: 0.999998]\n",
      "3745 [D loss: 1.000021] [G loss: 0.999937]\n",
      "3746 [D loss: 0.999980] [G loss: 0.999977]\n",
      "3747 [D loss: 1.000061] [G loss: 0.999920]\n",
      "3748 [D loss: 0.999900] [G loss: 1.000045]\n",
      "3749 [D loss: 0.999964] [G loss: 1.000013]\n",
      "3750 [D loss: 1.000005] [G loss: 1.000048]\n",
      "3751 [D loss: 1.000005] [G loss: 0.999924]\n",
      "3752 [D loss: 1.000008] [G loss: 1.000050]\n",
      "3753 [D loss: 1.000054] [G loss: 0.999892]\n",
      "3754 [D loss: 0.999998] [G loss: 0.999910]\n",
      "3755 [D loss: 0.999947] [G loss: 0.999949]\n",
      "3756 [D loss: 1.000011] [G loss: 0.999830]\n",
      "3757 [D loss: 0.999979] [G loss: 0.999864]\n",
      "3758 [D loss: 0.999861] [G loss: 1.000025]\n",
      "3759 [D loss: 0.999982] [G loss: 0.999993]\n",
      "3760 [D loss: 0.999994] [G loss: 0.999909]\n",
      "3761 [D loss: 0.999971] [G loss: 0.999997]\n",
      "3762 [D loss: 0.999977] [G loss: 0.999958]\n",
      "3763 [D loss: 1.000009] [G loss: 0.999999]\n",
      "3764 [D loss: 0.999951] [G loss: 0.999966]\n",
      "3765 [D loss: 0.999985] [G loss: 0.999924]\n",
      "3766 [D loss: 0.999936] [G loss: 0.999953]\n",
      "3767 [D loss: 0.999983] [G loss: 0.999936]\n",
      "3768 [D loss: 0.999933] [G loss: 0.999973]\n",
      "3769 [D loss: 0.999894] [G loss: 1.000026]\n",
      "3770 [D loss: 0.999946] [G loss: 1.000028]\n",
      "3771 [D loss: 1.000043] [G loss: 0.999971]\n",
      "3772 [D loss: 0.999968] [G loss: 0.999980]\n",
      "3773 [D loss: 1.000001] [G loss: 0.999919]\n",
      "3774 [D loss: 0.999994] [G loss: 1.000052]\n",
      "3775 [D loss: 0.999978] [G loss: 1.000027]\n",
      "3776 [D loss: 0.999913] [G loss: 1.000086]\n",
      "3777 [D loss: 0.999944] [G loss: 0.999978]\n",
      "3778 [D loss: 1.000016] [G loss: 0.999973]\n",
      "3779 [D loss: 0.999984] [G loss: 0.999962]\n",
      "3780 [D loss: 1.000002] [G loss: 0.999897]\n",
      "3781 [D loss: 0.999990] [G loss: 0.999964]\n",
      "3782 [D loss: 1.000028] [G loss: 0.999939]\n",
      "3783 [D loss: 1.000021] [G loss: 0.999911]\n",
      "3784 [D loss: 0.999959] [G loss: 0.999951]\n",
      "3785 [D loss: 0.999938] [G loss: 1.000029]\n",
      "3786 [D loss: 0.999944] [G loss: 1.000002]\n",
      "3787 [D loss: 0.999983] [G loss: 1.000015]\n",
      "3788 [D loss: 1.000004] [G loss: 0.999949]\n",
      "3789 [D loss: 0.999997] [G loss: 0.999878]\n",
      "3790 [D loss: 0.999990] [G loss: 0.999983]\n",
      "3791 [D loss: 0.999927] [G loss: 0.999984]\n",
      "3792 [D loss: 1.000005] [G loss: 0.999937]\n",
      "3793 [D loss: 0.999916] [G loss: 1.000077]\n",
      "3794 [D loss: 0.999955] [G loss: 0.999905]\n",
      "3795 [D loss: 0.999974] [G loss: 0.999930]\n",
      "3796 [D loss: 0.999971] [G loss: 0.999997]\n",
      "3797 [D loss: 1.000050] [G loss: 0.999881]\n",
      "3798 [D loss: 1.000004] [G loss: 1.000021]\n",
      "3799 [D loss: 0.999912] [G loss: 0.999951]\n",
      "3800 [D loss: 0.999948] [G loss: 0.999971]\n",
      "3801 [D loss: 0.999943] [G loss: 1.000053]\n",
      "3802 [D loss: 0.999911] [G loss: 0.999919]\n",
      "3803 [D loss: 1.000011] [G loss: 0.999994]\n",
      "3804 [D loss: 0.999955] [G loss: 1.000013]\n",
      "3805 [D loss: 0.999991] [G loss: 1.000063]\n",
      "3806 [D loss: 0.999991] [G loss: 1.000002]\n",
      "3807 [D loss: 0.999954] [G loss: 1.000009]\n",
      "3808 [D loss: 0.999958] [G loss: 1.000062]\n",
      "3809 [D loss: 0.999927] [G loss: 1.000082]\n",
      "3810 [D loss: 1.000009] [G loss: 0.999981]\n",
      "3811 [D loss: 1.000099] [G loss: 0.999884]\n",
      "3812 [D loss: 0.999949] [G loss: 1.000024]\n",
      "3813 [D loss: 0.999969] [G loss: 1.000065]\n",
      "3814 [D loss: 0.999946] [G loss: 1.000046]\n",
      "3815 [D loss: 1.000013] [G loss: 1.000006]\n",
      "3816 [D loss: 0.999997] [G loss: 1.000003]\n",
      "3817 [D loss: 0.999996] [G loss: 0.999976]\n",
      "3818 [D loss: 0.999949] [G loss: 1.000055]\n",
      "3819 [D loss: 0.999939] [G loss: 1.000018]\n",
      "3820 [D loss: 1.000014] [G loss: 0.999980]\n",
      "3821 [D loss: 0.999924] [G loss: 0.999996]\n",
      "3822 [D loss: 0.999973] [G loss: 1.000028]\n",
      "3823 [D loss: 1.000009] [G loss: 0.999978]\n",
      "3824 [D loss: 0.999936] [G loss: 1.000022]\n",
      "3825 [D loss: 0.999987] [G loss: 0.999976]\n",
      "3826 [D loss: 0.999991] [G loss: 0.999878]\n",
      "3827 [D loss: 0.999975] [G loss: 1.000016]\n",
      "3828 [D loss: 0.999983] [G loss: 1.000048]\n",
      "3829 [D loss: 0.999970] [G loss: 0.999987]\n",
      "3830 [D loss: 0.999880] [G loss: 0.999975]\n",
      "3831 [D loss: 0.999890] [G loss: 1.000036]\n",
      "3832 [D loss: 0.999991] [G loss: 0.999985]\n",
      "3833 [D loss: 0.999933] [G loss: 0.999970]\n",
      "3834 [D loss: 0.999959] [G loss: 1.000035]\n",
      "3835 [D loss: 1.000019] [G loss: 0.999918]\n",
      "3836 [D loss: 1.000049] [G loss: 0.999917]\n",
      "3837 [D loss: 0.999991] [G loss: 0.999941]\n",
      "3838 [D loss: 0.999972] [G loss: 0.999984]\n",
      "3839 [D loss: 1.000023] [G loss: 0.999988]\n",
      "3840 [D loss: 0.999992] [G loss: 0.999964]\n",
      "3841 [D loss: 0.999945] [G loss: 1.000033]\n",
      "3842 [D loss: 1.000039] [G loss: 0.999923]\n",
      "3843 [D loss: 1.000003] [G loss: 0.999991]\n",
      "3844 [D loss: 1.000012] [G loss: 0.999933]\n",
      "3845 [D loss: 0.999987] [G loss: 1.000000]\n",
      "3846 [D loss: 1.000001] [G loss: 1.000021]\n",
      "3847 [D loss: 0.999939] [G loss: 0.999980]\n",
      "3848 [D loss: 0.999946] [G loss: 1.000043]\n",
      "3849 [D loss: 1.000024] [G loss: 0.999927]\n",
      "3850 [D loss: 0.999964] [G loss: 1.000010]\n",
      "3851 [D loss: 0.999985] [G loss: 1.000013]\n",
      "3852 [D loss: 0.999975] [G loss: 1.000010]\n",
      "3853 [D loss: 0.999985] [G loss: 1.000018]\n",
      "3854 [D loss: 1.000003] [G loss: 0.999978]\n",
      "3855 [D loss: 0.999977] [G loss: 0.999985]\n",
      "3856 [D loss: 0.999973] [G loss: 0.999848]\n",
      "3857 [D loss: 0.999997] [G loss: 0.999896]\n",
      "3858 [D loss: 1.000014] [G loss: 0.999950]\n",
      "3859 [D loss: 0.999964] [G loss: 0.999985]\n",
      "3860 [D loss: 0.999956] [G loss: 0.999972]\n",
      "3861 [D loss: 0.999998] [G loss: 0.999973]\n",
      "3862 [D loss: 0.999942] [G loss: 0.999973]\n",
      "3863 [D loss: 1.000069] [G loss: 0.999955]\n",
      "3864 [D loss: 0.999930] [G loss: 1.000019]\n",
      "3865 [D loss: 0.999996] [G loss: 0.999963]\n",
      "3866 [D loss: 1.000008] [G loss: 0.999976]\n",
      "3867 [D loss: 0.999951] [G loss: 0.999989]\n",
      "3868 [D loss: 0.999965] [G loss: 0.999975]\n",
      "3869 [D loss: 0.999975] [G loss: 1.000046]\n",
      "3870 [D loss: 0.999960] [G loss: 1.000031]\n",
      "3871 [D loss: 0.999913] [G loss: 1.000035]\n",
      "3872 [D loss: 1.000002] [G loss: 1.000030]\n",
      "3873 [D loss: 0.999958] [G loss: 0.999993]\n",
      "3874 [D loss: 0.999956] [G loss: 0.999958]\n",
      "3875 [D loss: 0.999997] [G loss: 0.999966]\n",
      "3876 [D loss: 0.999967] [G loss: 0.999983]\n",
      "3877 [D loss: 0.999960] [G loss: 1.000031]\n",
      "3878 [D loss: 0.999982] [G loss: 0.999976]\n",
      "3879 [D loss: 0.999931] [G loss: 0.999955]\n",
      "3880 [D loss: 0.999950] [G loss: 0.999954]\n",
      "3881 [D loss: 1.000028] [G loss: 0.999923]\n",
      "3882 [D loss: 0.999976] [G loss: 1.000008]\n",
      "3883 [D loss: 0.999951] [G loss: 1.000034]\n",
      "3884 [D loss: 0.999994] [G loss: 0.999927]\n",
      "3885 [D loss: 0.999955] [G loss: 0.999957]\n",
      "3886 [D loss: 0.999976] [G loss: 0.999989]\n",
      "3887 [D loss: 1.000014] [G loss: 0.999943]\n",
      "3888 [D loss: 1.000003] [G loss: 0.999983]\n",
      "3889 [D loss: 0.999960] [G loss: 1.000078]\n",
      "3890 [D loss: 1.000011] [G loss: 1.000017]\n",
      "3891 [D loss: 0.999954] [G loss: 1.000021]\n",
      "3892 [D loss: 1.000016] [G loss: 1.000049]\n",
      "3893 [D loss: 0.999976] [G loss: 0.999979]\n",
      "3894 [D loss: 0.999947] [G loss: 0.999996]\n",
      "3895 [D loss: 1.000046] [G loss: 1.000006]\n",
      "3896 [D loss: 0.999986] [G loss: 0.999931]\n",
      "3897 [D loss: 0.999971] [G loss: 0.999966]\n",
      "3898 [D loss: 1.000012] [G loss: 0.999923]\n",
      "3899 [D loss: 0.999918] [G loss: 1.000112]\n",
      "3900 [D loss: 0.999949] [G loss: 0.999968]\n",
      "3901 [D loss: 1.000006] [G loss: 0.999979]\n",
      "3902 [D loss: 1.000000] [G loss: 0.999965]\n",
      "3903 [D loss: 0.999965] [G loss: 0.999965]\n",
      "3904 [D loss: 1.000004] [G loss: 0.999952]\n",
      "3905 [D loss: 1.000047] [G loss: 0.999977]\n",
      "3906 [D loss: 0.999942] [G loss: 0.999994]\n",
      "3907 [D loss: 0.999954] [G loss: 1.000015]\n",
      "3908 [D loss: 0.999995] [G loss: 0.999975]\n",
      "3909 [D loss: 0.999949] [G loss: 0.999989]\n",
      "3910 [D loss: 1.000011] [G loss: 0.999935]\n",
      "3911 [D loss: 0.999945] [G loss: 0.999924]\n",
      "3912 [D loss: 1.000025] [G loss: 0.999917]\n",
      "3913 [D loss: 0.999896] [G loss: 1.000002]\n",
      "3914 [D loss: 1.000004] [G loss: 0.999887]\n",
      "3915 [D loss: 1.000031] [G loss: 0.999846]\n",
      "3916 [D loss: 0.999946] [G loss: 0.999947]\n",
      "3917 [D loss: 0.999988] [G loss: 0.999953]\n",
      "3918 [D loss: 0.999978] [G loss: 0.999901]\n",
      "3919 [D loss: 0.999989] [G loss: 0.999935]\n",
      "3920 [D loss: 0.999973] [G loss: 0.999969]\n",
      "3921 [D loss: 0.999958] [G loss: 0.999978]\n",
      "3922 [D loss: 0.999959] [G loss: 1.000021]\n",
      "3923 [D loss: 0.999945] [G loss: 1.000047]\n",
      "3924 [D loss: 0.999936] [G loss: 0.999990]\n",
      "3925 [D loss: 0.999971] [G loss: 0.999972]\n",
      "3926 [D loss: 0.999993] [G loss: 1.000049]\n",
      "3927 [D loss: 0.999932] [G loss: 1.000012]\n",
      "3928 [D loss: 0.999974] [G loss: 0.999990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3929 [D loss: 0.999971] [G loss: 0.999991]\n",
      "3930 [D loss: 0.999940] [G loss: 1.000053]\n",
      "3931 [D loss: 0.999980] [G loss: 0.999978]\n",
      "3932 [D loss: 0.999994] [G loss: 0.999977]\n",
      "3933 [D loss: 0.999969] [G loss: 0.999929]\n",
      "3934 [D loss: 0.999944] [G loss: 1.000047]\n",
      "3935 [D loss: 0.999980] [G loss: 1.000028]\n",
      "3936 [D loss: 0.999962] [G loss: 1.000008]\n",
      "3937 [D loss: 0.999952] [G loss: 0.999947]\n",
      "3938 [D loss: 0.999968] [G loss: 1.000017]\n",
      "3939 [D loss: 1.000012] [G loss: 0.999966]\n",
      "3940 [D loss: 0.999968] [G loss: 0.999986]\n",
      "3941 [D loss: 0.999942] [G loss: 1.000012]\n",
      "3942 [D loss: 0.999945] [G loss: 0.999945]\n",
      "3943 [D loss: 0.999968] [G loss: 0.999988]\n",
      "3944 [D loss: 0.999944] [G loss: 1.000019]\n",
      "3945 [D loss: 0.999951] [G loss: 1.000012]\n",
      "3946 [D loss: 0.999964] [G loss: 1.000048]\n",
      "3947 [D loss: 1.000019] [G loss: 0.999991]\n",
      "3948 [D loss: 0.999995] [G loss: 0.999977]\n",
      "3949 [D loss: 0.999876] [G loss: 0.999987]\n",
      "3950 [D loss: 1.000001] [G loss: 0.999965]\n",
      "3951 [D loss: 1.000024] [G loss: 0.999919]\n",
      "3952 [D loss: 1.000012] [G loss: 0.999996]\n",
      "3953 [D loss: 0.999999] [G loss: 1.000003]\n",
      "3954 [D loss: 0.999924] [G loss: 1.000011]\n",
      "3955 [D loss: 0.999993] [G loss: 0.999986]\n",
      "3956 [D loss: 0.999899] [G loss: 0.999978]\n",
      "3957 [D loss: 0.999973] [G loss: 1.000093]\n",
      "3958 [D loss: 1.000036] [G loss: 0.999992]\n",
      "3959 [D loss: 0.999972] [G loss: 0.999951]\n",
      "3960 [D loss: 0.999968] [G loss: 1.000016]\n",
      "3961 [D loss: 0.999928] [G loss: 0.999973]\n",
      "3962 [D loss: 0.999994] [G loss: 1.000033]\n",
      "3963 [D loss: 1.000006] [G loss: 1.000000]\n",
      "3964 [D loss: 0.999936] [G loss: 1.000036]\n",
      "3965 [D loss: 0.999973] [G loss: 0.999997]\n",
      "3966 [D loss: 0.999952] [G loss: 0.999985]\n",
      "3967 [D loss: 0.999966] [G loss: 0.999954]\n",
      "3968 [D loss: 0.999957] [G loss: 0.999962]\n",
      "3969 [D loss: 0.999964] [G loss: 1.000031]\n",
      "3970 [D loss: 0.999902] [G loss: 1.000010]\n",
      "3971 [D loss: 0.999980] [G loss: 1.000033]\n",
      "3972 [D loss: 0.999996] [G loss: 1.000007]\n",
      "3973 [D loss: 0.999965] [G loss: 1.000068]\n",
      "3974 [D loss: 0.999974] [G loss: 0.999958]\n",
      "3975 [D loss: 1.000010] [G loss: 0.999909]\n",
      "3976 [D loss: 0.999988] [G loss: 1.000096]\n",
      "3977 [D loss: 0.999984] [G loss: 1.000028]\n",
      "3978 [D loss: 0.999959] [G loss: 0.999981]\n",
      "3979 [D loss: 0.999948] [G loss: 0.999968]\n",
      "3980 [D loss: 0.999980] [G loss: 0.999903]\n",
      "3981 [D loss: 0.999970] [G loss: 1.000050]\n",
      "3982 [D loss: 0.999968] [G loss: 0.999919]\n",
      "3983 [D loss: 0.999957] [G loss: 0.999998]\n",
      "3984 [D loss: 0.999934] [G loss: 1.000004]\n",
      "3985 [D loss: 0.999960] [G loss: 1.000048]\n",
      "3986 [D loss: 1.000034] [G loss: 0.999963]\n",
      "3987 [D loss: 1.000031] [G loss: 1.000046]\n",
      "3988 [D loss: 0.999966] [G loss: 1.000047]\n",
      "3989 [D loss: 1.000040] [G loss: 0.999978]\n",
      "3990 [D loss: 1.000036] [G loss: 0.999927]\n",
      "3991 [D loss: 0.999938] [G loss: 1.000018]\n",
      "3992 [D loss: 1.000020] [G loss: 0.999929]\n",
      "3993 [D loss: 1.000024] [G loss: 0.999855]\n",
      "3994 [D loss: 0.999974] [G loss: 1.000020]\n",
      "3995 [D loss: 0.999993] [G loss: 1.000003]\n",
      "3996 [D loss: 0.999894] [G loss: 1.000027]\n",
      "3997 [D loss: 0.999973] [G loss: 1.000000]\n",
      "3998 [D loss: 0.999981] [G loss: 1.000012]\n",
      "3999 [D loss: 1.000005] [G loss: 1.000057]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clipping 0.005 dataset2\n",
    "#clipping 0.02 dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_130 (Conv2D)          (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_131 (Conv2D)          (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPaddi (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_132 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_133 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_18 (Reshape)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_36 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_37 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0 [D loss: 1.000025] [G loss: 1.000557]\n",
      "1 [D loss: 1.000160] [G loss: 1.000602]\n",
      "2 [D loss: 1.000207] [G loss: 1.000702]\n",
      "3 [D loss: 1.000176] [G loss: 1.000878]\n",
      "4 [D loss: 1.000174] [G loss: 1.000891]\n",
      "5 [D loss: 1.000187] [G loss: 1.001024]\n",
      "6 [D loss: 1.000222] [G loss: 1.001089]\n",
      "7 [D loss: 1.000157] [G loss: 1.001214]\n",
      "8 [D loss: 1.000140] [G loss: 1.001194]\n",
      "9 [D loss: 1.000138] [G loss: 1.001193]\n",
      "10 [D loss: 1.000091] [G loss: 1.001163]\n",
      "11 [D loss: 1.000198] [G loss: 1.001083]\n",
      "12 [D loss: 1.000191] [G loss: 1.001002]\n",
      "13 [D loss: 1.000374] [G loss: 1.000706]\n",
      "14 [D loss: 1.000601] [G loss: 1.000356]\n",
      "15 [D loss: 1.000866] [G loss: 1.000044]\n",
      "16 [D loss: 1.001054] [G loss: 0.999711]\n",
      "17 [D loss: 1.001312] [G loss: 0.999242]\n",
      "18 [D loss: 1.000863] [G loss: 0.999420]\n",
      "19 [D loss: 1.000778] [G loss: 1.000279]\n",
      "20 [D loss: 1.001049] [G loss: 1.000883]\n",
      "21 [D loss: 1.001298] [G loss: 1.001626]\n",
      "22 [D loss: 1.001863] [G loss: 1.002008]\n",
      "23 [D loss: 1.002476] [G loss: 1.002774]\n",
      "24 [D loss: 1.002722] [G loss: 1.003611]\n",
      "25 [D loss: 1.003962] [G loss: 1.003188]\n",
      "26 [D loss: 1.004028] [G loss: 1.003824]\n",
      "27 [D loss: 1.004998] [G loss: 1.003896]\n",
      "28 [D loss: 1.005317] [G loss: 1.004604]\n",
      "29 [D loss: 1.006367] [G loss: 1.004483]\n",
      "30 [D loss: 1.006876] [G loss: 1.004678]\n",
      "31 [D loss: 1.007412] [G loss: 1.005365]\n",
      "32 [D loss: 1.006655] [G loss: 1.006137]\n",
      "33 [D loss: 1.004722] [G loss: 1.007380]\n",
      "34 [D loss: 1.004529] [G loss: 1.007988]\n",
      "35 [D loss: 1.005885] [G loss: 1.010905]\n",
      "36 [D loss: 1.004857] [G loss: 1.012412]\n",
      "37 [D loss: 1.003332] [G loss: 1.014063]\n",
      "38 [D loss: 1.003674] [G loss: 1.015115]\n",
      "39 [D loss: 1.004835] [G loss: 1.014536]\n",
      "40 [D loss: 1.003426] [G loss: 1.015662]\n",
      "41 [D loss: 1.003795] [G loss: 1.016526]\n",
      "42 [D loss: 1.004513] [G loss: 1.017804]\n",
      "43 [D loss: 1.006059] [G loss: 1.020483]\n",
      "44 [D loss: 1.005935] [G loss: 1.019599]\n",
      "45 [D loss: 1.003538] [G loss: 1.021330]\n",
      "46 [D loss: 1.005902] [G loss: 1.022076]\n",
      "47 [D loss: 1.003899] [G loss: 1.023884]\n",
      "48 [D loss: 1.004541] [G loss: 1.024662]\n",
      "49 [D loss: 1.003127] [G loss: 1.026999]\n",
      "50 [D loss: 1.006180] [G loss: 1.025182]\n",
      "51 [D loss: 1.005962] [G loss: 1.026089]\n",
      "52 [D loss: 1.004287] [G loss: 1.028542]\n",
      "53 [D loss: 1.004325] [G loss: 1.029549]\n",
      "54 [D loss: 1.004158] [G loss: 1.028844]\n",
      "55 [D loss: 1.003499] [G loss: 1.030602]\n",
      "56 [D loss: 1.003345] [G loss: 1.029787]\n",
      "57 [D loss: 1.003303] [G loss: 1.031421]\n",
      "58 [D loss: 1.003750] [G loss: 1.031492]\n",
      "59 [D loss: 1.002858] [G loss: 1.032443]\n",
      "60 [D loss: 1.003566] [G loss: 1.032510]\n",
      "61 [D loss: 1.002296] [G loss: 1.033979]\n",
      "62 [D loss: 1.002978] [G loss: 1.034238]\n",
      "63 [D loss: 1.002498] [G loss: 1.034264]\n",
      "64 [D loss: 1.002446] [G loss: 1.035322]\n",
      "65 [D loss: 1.001974] [G loss: 1.033219]\n",
      "66 [D loss: 1.001944] [G loss: 1.034574]\n",
      "67 [D loss: 1.002397] [G loss: 1.034795]\n",
      "68 [D loss: 1.001745] [G loss: 1.034469]\n",
      "69 [D loss: 1.002530] [G loss: 1.033894]\n",
      "70 [D loss: 1.001841] [G loss: 1.035554]\n",
      "71 [D loss: 1.002402] [G loss: 1.034050]\n",
      "72 [D loss: 1.002545] [G loss: 1.033638]\n",
      "73 [D loss: 1.001869] [G loss: 1.033458]\n",
      "74 [D loss: 1.003680] [G loss: 1.033293]\n",
      "75 [D loss: 1.001883] [G loss: 1.035394]\n",
      "76 [D loss: 1.001675] [G loss: 1.033595]\n",
      "77 [D loss: 1.000563] [G loss: 1.034582]\n",
      "78 [D loss: 1.000937] [G loss: 1.033905]\n",
      "79 [D loss: 1.000856] [G loss: 1.033558]\n",
      "80 [D loss: 1.001536] [G loss: 1.035148]\n",
      "81 [D loss: 1.000980] [G loss: 1.035729]\n",
      "82 [D loss: 1.001584] [G loss: 1.034747]\n",
      "83 [D loss: 1.000396] [G loss: 1.033325]\n",
      "84 [D loss: 1.001646] [G loss: 1.033349]\n",
      "85 [D loss: 1.001981] [G loss: 1.032042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 [D loss: 0.999988] [G loss: 1.033873]\n",
      "87 [D loss: 1.000763] [G loss: 1.033461]\n",
      "88 [D loss: 1.002119] [G loss: 1.032621]\n",
      "89 [D loss: 1.000676] [G loss: 1.032386]\n",
      "90 [D loss: 1.000908] [G loss: 1.034000]\n",
      "91 [D loss: 1.001321] [G loss: 1.031486]\n",
      "92 [D loss: 1.000133] [G loss: 1.031248]\n",
      "93 [D loss: 1.001026] [G loss: 1.032875]\n",
      "94 [D loss: 1.000625] [G loss: 1.032281]\n",
      "95 [D loss: 1.000267] [G loss: 1.031750]\n",
      "96 [D loss: 1.000989] [G loss: 1.030526]\n",
      "97 [D loss: 1.000540] [G loss: 1.029212]\n",
      "98 [D loss: 1.001107] [G loss: 1.031436]\n",
      "99 [D loss: 0.999814] [G loss: 1.031231]\n",
      "100 [D loss: 1.001328] [G loss: 1.031086]\n",
      "101 [D loss: 1.000900] [G loss: 1.030717]\n",
      "102 [D loss: 1.001066] [G loss: 1.030372]\n",
      "103 [D loss: 0.999741] [G loss: 1.029724]\n",
      "104 [D loss: 1.001413] [G loss: 1.029781]\n",
      "105 [D loss: 1.000486] [G loss: 1.029769]\n",
      "106 [D loss: 1.001338] [G loss: 1.029174]\n",
      "107 [D loss: 1.000862] [G loss: 1.029824]\n",
      "108 [D loss: 1.002211] [G loss: 1.028785]\n",
      "109 [D loss: 1.001061] [G loss: 1.029047]\n",
      "110 [D loss: 1.000802] [G loss: 1.028913]\n",
      "111 [D loss: 1.001605] [G loss: 1.028893]\n",
      "112 [D loss: 1.001775] [G loss: 1.028996]\n",
      "113 [D loss: 1.000908] [G loss: 1.031062]\n",
      "114 [D loss: 1.000517] [G loss: 1.029011]\n",
      "115 [D loss: 1.002125] [G loss: 1.028966]\n",
      "116 [D loss: 1.001463] [G loss: 1.028744]\n",
      "117 [D loss: 0.999864] [G loss: 1.030039]\n",
      "118 [D loss: 1.001038] [G loss: 1.030117]\n",
      "119 [D loss: 1.002937] [G loss: 1.030079]\n",
      "120 [D loss: 1.001804] [G loss: 1.029815]\n",
      "121 [D loss: 1.001854] [G loss: 1.029405]\n",
      "122 [D loss: 1.001021] [G loss: 1.030179]\n",
      "123 [D loss: 1.001034] [G loss: 1.030856]\n",
      "124 [D loss: 1.000135] [G loss: 1.030659]\n",
      "125 [D loss: 1.000463] [G loss: 1.030510]\n",
      "126 [D loss: 1.002529] [G loss: 1.030026]\n",
      "127 [D loss: 0.999822] [G loss: 1.030140]\n",
      "128 [D loss: 1.001346] [G loss: 1.029843]\n",
      "129 [D loss: 0.999797] [G loss: 1.030914]\n",
      "130 [D loss: 1.000228] [G loss: 1.032078]\n",
      "131 [D loss: 1.000744] [G loss: 1.030587]\n",
      "132 [D loss: 1.001566] [G loss: 1.029601]\n",
      "133 [D loss: 1.001725] [G loss: 1.031027]\n",
      "134 [D loss: 1.002540] [G loss: 1.029883]\n",
      "135 [D loss: 1.000933] [G loss: 1.030389]\n",
      "136 [D loss: 0.999991] [G loss: 1.030311]\n",
      "137 [D loss: 1.001205] [G loss: 1.029739]\n",
      "138 [D loss: 1.000262] [G loss: 1.031032]\n",
      "139 [D loss: 0.999986] [G loss: 1.032005]\n",
      "140 [D loss: 1.001275] [G loss: 1.030393]\n",
      "141 [D loss: 0.999855] [G loss: 1.031430]\n",
      "142 [D loss: 1.000616] [G loss: 1.029787]\n",
      "143 [D loss: 1.000414] [G loss: 1.030446]\n",
      "144 [D loss: 1.000530] [G loss: 1.030798]\n",
      "145 [D loss: 1.000497] [G loss: 1.030236]\n",
      "146 [D loss: 1.000698] [G loss: 1.029690]\n",
      "147 [D loss: 1.001094] [G loss: 1.029966]\n",
      "148 [D loss: 1.001623] [G loss: 1.028561]\n",
      "149 [D loss: 1.001111] [G loss: 1.030780]\n",
      "150 [D loss: 1.001400] [G loss: 1.030887]\n",
      "151 [D loss: 1.000880] [G loss: 1.030732]\n",
      "152 [D loss: 1.001251] [G loss: 1.029583]\n",
      "153 [D loss: 1.002616] [G loss: 1.030144]\n",
      "154 [D loss: 1.000744] [G loss: 1.031033]\n",
      "155 [D loss: 1.001029] [G loss: 1.029504]\n",
      "156 [D loss: 1.001331] [G loss: 1.030946]\n",
      "157 [D loss: 1.000645] [G loss: 1.029969]\n",
      "158 [D loss: 1.001525] [G loss: 1.030092]\n",
      "159 [D loss: 1.001690] [G loss: 1.030365]\n",
      "160 [D loss: 1.001310] [G loss: 1.029539]\n",
      "161 [D loss: 1.000771] [G loss: 1.028810]\n",
      "162 [D loss: 1.000620] [G loss: 1.029447]\n",
      "163 [D loss: 1.002679] [G loss: 1.029676]\n",
      "164 [D loss: 0.999701] [G loss: 1.030506]\n",
      "165 [D loss: 1.000799] [G loss: 1.029114]\n",
      "166 [D loss: 1.001538] [G loss: 1.029773]\n",
      "167 [D loss: 1.001795] [G loss: 1.030521]\n",
      "168 [D loss: 1.002370] [G loss: 1.028899]\n",
      "169 [D loss: 1.002192] [G loss: 1.031616]\n",
      "170 [D loss: 1.002771] [G loss: 1.029716]\n",
      "171 [D loss: 1.003118] [G loss: 1.030087]\n",
      "172 [D loss: 1.003731] [G loss: 1.029943]\n",
      "173 [D loss: 0.999836] [G loss: 1.029365]\n",
      "174 [D loss: 0.999808] [G loss: 1.031374]\n",
      "175 [D loss: 1.000665] [G loss: 1.030772]\n",
      "176 [D loss: 1.000967] [G loss: 1.031115]\n",
      "177 [D loss: 1.002477] [G loss: 1.029210]\n",
      "178 [D loss: 1.001534] [G loss: 1.030026]\n",
      "179 [D loss: 1.000359] [G loss: 1.030325]\n",
      "180 [D loss: 1.000995] [G loss: 1.031581]\n",
      "181 [D loss: 1.000802] [G loss: 1.030363]\n",
      "182 [D loss: 1.001744] [G loss: 1.029790]\n",
      "183 [D loss: 1.000402] [G loss: 1.030878]\n",
      "184 [D loss: 1.000170] [G loss: 1.027302]\n",
      "185 [D loss: 1.001027] [G loss: 1.029809]\n",
      "186 [D loss: 1.000529] [G loss: 1.030629]\n",
      "187 [D loss: 1.001580] [G loss: 1.030438]\n",
      "188 [D loss: 1.001693] [G loss: 1.028245]\n",
      "189 [D loss: 1.001733] [G loss: 1.028501]\n",
      "190 [D loss: 1.001490] [G loss: 1.032266]\n",
      "191 [D loss: 1.001374] [G loss: 1.030240]\n",
      "192 [D loss: 1.000899] [G loss: 1.029714]\n",
      "193 [D loss: 1.002509] [G loss: 1.028390]\n",
      "194 [D loss: 1.001303] [G loss: 1.029549]\n",
      "195 [D loss: 1.001352] [G loss: 1.029055]\n",
      "196 [D loss: 1.000930] [G loss: 1.029717]\n",
      "197 [D loss: 1.001411] [G loss: 1.031201]\n",
      "198 [D loss: 1.000636] [G loss: 1.030177]\n",
      "199 [D loss: 1.000432] [G loss: 1.029788]\n",
      "200 [D loss: 1.000751] [G loss: 1.028233]\n",
      "201 [D loss: 1.001832] [G loss: 1.028935]\n",
      "202 [D loss: 1.002049] [G loss: 1.027760]\n",
      "203 [D loss: 1.000944] [G loss: 1.031144]\n",
      "204 [D loss: 1.003057] [G loss: 1.031285]\n",
      "205 [D loss: 1.001237] [G loss: 1.027874]\n",
      "206 [D loss: 1.000833] [G loss: 1.028427]\n",
      "207 [D loss: 1.001118] [G loss: 1.028177]\n",
      "208 [D loss: 1.000474] [G loss: 1.031089]\n",
      "209 [D loss: 1.002109] [G loss: 1.029239]\n",
      "210 [D loss: 1.001637] [G loss: 1.029652]\n",
      "211 [D loss: 1.002699] [G loss: 1.028688]\n",
      "212 [D loss: 1.000910] [G loss: 1.028945]\n",
      "213 [D loss: 0.999784] [G loss: 1.029598]\n",
      "214 [D loss: 1.002606] [G loss: 1.027906]\n",
      "215 [D loss: 1.002229] [G loss: 1.030191]\n",
      "216 [D loss: 1.002115] [G loss: 1.029499]\n",
      "217 [D loss: 1.002323] [G loss: 1.032475]\n",
      "218 [D loss: 1.000277] [G loss: 1.030413]\n",
      "219 [D loss: 1.002037] [G loss: 1.028578]\n",
      "220 [D loss: 1.000612] [G loss: 1.028572]\n",
      "221 [D loss: 1.002870] [G loss: 1.029605]\n",
      "222 [D loss: 1.001581] [G loss: 1.029627]\n",
      "223 [D loss: 1.000793] [G loss: 1.029370]\n",
      "224 [D loss: 1.001175] [G loss: 1.028011]\n",
      "225 [D loss: 1.000651] [G loss: 1.030092]\n",
      "226 [D loss: 1.002749] [G loss: 1.029429]\n",
      "227 [D loss: 1.001667] [G loss: 1.029178]\n",
      "228 [D loss: 1.004355] [G loss: 1.027379]\n",
      "229 [D loss: 1.000500] [G loss: 1.033442]\n",
      "230 [D loss: 1.002576] [G loss: 1.026736]\n",
      "231 [D loss: 1.001363] [G loss: 1.028945]\n",
      "232 [D loss: 1.001216] [G loss: 1.027361]\n",
      "233 [D loss: 1.000727] [G loss: 1.031383]\n",
      "234 [D loss: 1.000804] [G loss: 1.030064]\n",
      "235 [D loss: 1.000326] [G loss: 1.029515]\n",
      "236 [D loss: 1.003544] [G loss: 1.028995]\n",
      "237 [D loss: 1.000386] [G loss: 1.031417]\n",
      "238 [D loss: 1.000318] [G loss: 1.032126]\n",
      "239 [D loss: 0.999761] [G loss: 1.031511]\n",
      "240 [D loss: 1.001087] [G loss: 1.029472]\n",
      "241 [D loss: 1.001175] [G loss: 1.030476]\n",
      "242 [D loss: 0.997850] [G loss: 1.031894]\n",
      "243 [D loss: 1.001488] [G loss: 1.029741]\n",
      "244 [D loss: 1.000318] [G loss: 1.030247]\n",
      "245 [D loss: 1.000066] [G loss: 1.030135]\n",
      "246 [D loss: 1.001251] [G loss: 1.029297]\n",
      "247 [D loss: 1.001745] [G loss: 1.028321]\n",
      "248 [D loss: 1.000465] [G loss: 1.028398]\n",
      "249 [D loss: 0.999212] [G loss: 1.029677]\n",
      "250 [D loss: 0.999022] [G loss: 1.030357]\n",
      "251 [D loss: 1.000588] [G loss: 1.028571]\n",
      "252 [D loss: 1.001327] [G loss: 1.029576]\n",
      "253 [D loss: 0.999334] [G loss: 1.028486]\n",
      "254 [D loss: 1.000216] [G loss: 1.029977]\n",
      "255 [D loss: 0.999262] [G loss: 1.029534]\n",
      "256 [D loss: 1.000317] [G loss: 1.029959]\n",
      "257 [D loss: 0.999567] [G loss: 1.029893]\n",
      "258 [D loss: 1.002250] [G loss: 1.028150]\n",
      "259 [D loss: 0.998020] [G loss: 1.033554]\n",
      "260 [D loss: 1.000371] [G loss: 1.029611]\n",
      "261 [D loss: 1.001905] [G loss: 1.030917]\n",
      "262 [D loss: 1.000573] [G loss: 1.031007]\n",
      "263 [D loss: 1.000502] [G loss: 1.032447]\n",
      "264 [D loss: 1.000686] [G loss: 1.028454]\n",
      "265 [D loss: 1.000127] [G loss: 1.030872]\n",
      "266 [D loss: 0.999401] [G loss: 1.028470]\n",
      "267 [D loss: 0.999596] [G loss: 1.028016]\n",
      "268 [D loss: 1.000211] [G loss: 1.027299]\n",
      "269 [D loss: 0.997383] [G loss: 1.027235]\n",
      "270 [D loss: 1.000312] [G loss: 1.029243]\n",
      "271 [D loss: 0.998551] [G loss: 1.027135]\n",
      "272 [D loss: 1.000375] [G loss: 1.027386]\n",
      "273 [D loss: 0.999467] [G loss: 1.029407]\n",
      "274 [D loss: 1.001094] [G loss: 1.025665]\n",
      "275 [D loss: 1.000187] [G loss: 1.026529]\n",
      "276 [D loss: 0.999295] [G loss: 1.026685]\n",
      "277 [D loss: 0.999917] [G loss: 1.029341]\n",
      "278 [D loss: 1.001537] [G loss: 1.026565]\n",
      "279 [D loss: 1.000564] [G loss: 1.024316]\n",
      "280 [D loss: 0.999284] [G loss: 1.027658]\n",
      "281 [D loss: 1.001755] [G loss: 1.026268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 [D loss: 1.000594] [G loss: 1.025922]\n",
      "283 [D loss: 0.999835] [G loss: 1.027860]\n",
      "284 [D loss: 1.001866] [G loss: 1.027099]\n",
      "285 [D loss: 1.002433] [G loss: 1.027652]\n",
      "286 [D loss: 1.000759] [G loss: 1.026922]\n",
      "287 [D loss: 0.998733] [G loss: 1.025899]\n",
      "288 [D loss: 0.999994] [G loss: 1.026280]\n",
      "289 [D loss: 1.000837] [G loss: 1.023921]\n",
      "290 [D loss: 1.000880] [G loss: 1.024126]\n",
      "291 [D loss: 1.002564] [G loss: 1.023866]\n",
      "292 [D loss: 1.000437] [G loss: 1.022888]\n",
      "293 [D loss: 0.999341] [G loss: 1.022718]\n",
      "294 [D loss: 0.999982] [G loss: 1.022769]\n",
      "295 [D loss: 1.001189] [G loss: 1.022553]\n",
      "296 [D loss: 0.999835] [G loss: 1.025177]\n",
      "297 [D loss: 0.999134] [G loss: 1.024351]\n",
      "298 [D loss: 1.001104] [G loss: 1.021520]\n",
      "299 [D loss: 1.001325] [G loss: 1.022051]\n",
      "300 [D loss: 1.001589] [G loss: 1.021245]\n",
      "301 [D loss: 1.001154] [G loss: 1.021471]\n",
      "302 [D loss: 1.000452] [G loss: 1.024048]\n",
      "303 [D loss: 1.000979] [G loss: 1.022610]\n",
      "304 [D loss: 1.000582] [G loss: 1.021753]\n",
      "305 [D loss: 1.001735] [G loss: 1.021850]\n",
      "306 [D loss: 1.002455] [G loss: 1.020536]\n",
      "307 [D loss: 0.999630] [G loss: 1.022255]\n",
      "308 [D loss: 1.001323] [G loss: 1.020014]\n",
      "309 [D loss: 1.000855] [G loss: 1.020618]\n",
      "310 [D loss: 1.000936] [G loss: 1.021006]\n",
      "311 [D loss: 0.999045] [G loss: 1.022158]\n",
      "312 [D loss: 1.000733] [G loss: 1.019111]\n",
      "313 [D loss: 1.001195] [G loss: 1.019797]\n",
      "314 [D loss: 1.001342] [G loss: 1.018405]\n",
      "315 [D loss: 1.001597] [G loss: 1.018398]\n",
      "316 [D loss: 0.998837] [G loss: 1.018131]\n",
      "317 [D loss: 1.000863] [G loss: 1.020128]\n",
      "318 [D loss: 1.001687] [G loss: 1.018427]\n",
      "319 [D loss: 1.000674] [G loss: 1.019373]\n",
      "320 [D loss: 0.999581] [G loss: 1.018293]\n",
      "321 [D loss: 1.001347] [G loss: 1.017958]\n",
      "322 [D loss: 1.000211] [G loss: 1.018347]\n",
      "323 [D loss: 1.000074] [G loss: 1.019056]\n",
      "324 [D loss: 1.000717] [G loss: 1.019303]\n",
      "325 [D loss: 1.000010] [G loss: 1.017998]\n",
      "326 [D loss: 1.001180] [G loss: 1.018915]\n",
      "327 [D loss: 1.001345] [G loss: 1.017587]\n",
      "328 [D loss: 1.000510] [G loss: 1.015981]\n",
      "329 [D loss: 1.000442] [G loss: 1.018303]\n",
      "330 [D loss: 1.000286] [G loss: 1.017479]\n",
      "331 [D loss: 0.999056] [G loss: 1.017483]\n",
      "332 [D loss: 1.001241] [G loss: 1.014288]\n",
      "333 [D loss: 1.001006] [G loss: 1.017414]\n",
      "334 [D loss: 1.000717] [G loss: 1.017293]\n",
      "335 [D loss: 0.999869] [G loss: 1.014726]\n",
      "336 [D loss: 1.000904] [G loss: 1.015596]\n",
      "337 [D loss: 1.001078] [G loss: 1.018104]\n",
      "338 [D loss: 1.001059] [G loss: 1.016409]\n",
      "339 [D loss: 0.998927] [G loss: 1.016492]\n",
      "340 [D loss: 0.998935] [G loss: 1.016239]\n",
      "341 [D loss: 1.000220] [G loss: 1.015055]\n",
      "342 [D loss: 1.000088] [G loss: 1.015602]\n",
      "343 [D loss: 1.000311] [G loss: 1.014016]\n",
      "344 [D loss: 1.000904] [G loss: 1.016529]\n",
      "345 [D loss: 0.999987] [G loss: 1.015898]\n",
      "346 [D loss: 1.000405] [G loss: 1.015925]\n",
      "347 [D loss: 0.999719] [G loss: 1.015559]\n",
      "348 [D loss: 0.999662] [G loss: 1.013996]\n",
      "349 [D loss: 1.000208] [G loss: 1.014580]\n",
      "350 [D loss: 1.000736] [G loss: 1.013446]\n",
      "351 [D loss: 1.000429] [G loss: 1.015757]\n",
      "352 [D loss: 0.999977] [G loss: 1.015540]\n",
      "353 [D loss: 1.000007] [G loss: 1.012989]\n",
      "354 [D loss: 1.000098] [G loss: 1.014097]\n",
      "355 [D loss: 1.000460] [G loss: 1.015580]\n",
      "356 [D loss: 1.000480] [G loss: 1.013378]\n",
      "357 [D loss: 0.999678] [G loss: 1.015518]\n",
      "358 [D loss: 1.000583] [G loss: 1.012424]\n",
      "359 [D loss: 0.998942] [G loss: 1.014828]\n",
      "360 [D loss: 1.001389] [G loss: 1.013107]\n",
      "361 [D loss: 0.999759] [G loss: 1.013910]\n",
      "362 [D loss: 0.997352] [G loss: 1.012888]\n",
      "363 [D loss: 1.000687] [G loss: 1.012337]\n",
      "364 [D loss: 1.000867] [G loss: 1.013868]\n",
      "365 [D loss: 0.998936] [G loss: 1.013383]\n",
      "366 [D loss: 1.000806] [G loss: 1.010618]\n",
      "367 [D loss: 0.999263] [G loss: 1.013918]\n",
      "368 [D loss: 0.999358] [G loss: 1.011914]\n",
      "369 [D loss: 0.999240] [G loss: 1.014904]\n",
      "370 [D loss: 1.001254] [G loss: 1.014058]\n",
      "371 [D loss: 0.998999] [G loss: 1.013000]\n",
      "372 [D loss: 1.000404] [G loss: 1.009937]\n",
      "373 [D loss: 0.997727] [G loss: 1.013534]\n",
      "374 [D loss: 1.000681] [G loss: 1.011797]\n",
      "375 [D loss: 1.000130] [G loss: 1.013566]\n",
      "376 [D loss: 1.000599] [G loss: 1.011166]\n",
      "377 [D loss: 1.001157] [G loss: 1.011306]\n",
      "378 [D loss: 1.000878] [G loss: 1.013691]\n",
      "379 [D loss: 1.001100] [G loss: 1.014651]\n",
      "380 [D loss: 1.000364] [G loss: 1.011016]\n",
      "381 [D loss: 1.000955] [G loss: 1.012158]\n",
      "382 [D loss: 1.000094] [G loss: 1.013825]\n",
      "383 [D loss: 1.000136] [G loss: 1.010609]\n",
      "384 [D loss: 0.999529] [G loss: 1.013322]\n",
      "385 [D loss: 1.000004] [G loss: 1.014352]\n",
      "386 [D loss: 1.000312] [G loss: 1.013701]\n",
      "387 [D loss: 0.999691] [G loss: 1.013092]\n",
      "388 [D loss: 1.000394] [G loss: 1.011596]\n",
      "389 [D loss: 0.999896] [G loss: 1.012970]\n",
      "390 [D loss: 0.999103] [G loss: 1.011736]\n",
      "391 [D loss: 1.001016] [G loss: 1.014178]\n",
      "392 [D loss: 1.001065] [G loss: 1.013319]\n",
      "393 [D loss: 1.001347] [G loss: 1.013861]\n",
      "394 [D loss: 1.001529] [G loss: 1.014321]\n",
      "395 [D loss: 0.999557] [G loss: 1.013187]\n",
      "396 [D loss: 1.003185] [G loss: 1.011183]\n",
      "397 [D loss: 1.001458] [G loss: 1.012359]\n",
      "398 [D loss: 1.002859] [G loss: 1.011254]\n",
      "399 [D loss: 1.000282] [G loss: 1.013722]\n",
      "400 [D loss: 1.000648] [G loss: 1.010706]\n",
      "401 [D loss: 1.001914] [G loss: 1.012456]\n",
      "402 [D loss: 0.999801] [G loss: 1.012164]\n",
      "403 [D loss: 1.000639] [G loss: 1.012746]\n",
      "404 [D loss: 0.999856] [G loss: 1.011367]\n",
      "405 [D loss: 1.000912] [G loss: 1.010314]\n",
      "406 [D loss: 1.001025] [G loss: 1.011964]\n",
      "407 [D loss: 1.000311] [G loss: 1.010523]\n",
      "408 [D loss: 0.998599] [G loss: 1.011635]\n",
      "409 [D loss: 1.000837] [G loss: 1.013380]\n",
      "410 [D loss: 1.000578] [G loss: 1.014292]\n",
      "411 [D loss: 1.001596] [G loss: 1.011061]\n",
      "412 [D loss: 1.001701] [G loss: 1.011351]\n",
      "413 [D loss: 1.001776] [G loss: 1.011062]\n",
      "414 [D loss: 1.000425] [G loss: 1.011974]\n",
      "415 [D loss: 1.000944] [G loss: 1.007886]\n",
      "416 [D loss: 1.000222] [G loss: 1.011631]\n",
      "417 [D loss: 1.000579] [G loss: 1.010658]\n",
      "418 [D loss: 1.002383] [G loss: 1.010267]\n",
      "419 [D loss: 1.001104] [G loss: 1.007832]\n",
      "420 [D loss: 1.001782] [G loss: 1.010616]\n",
      "421 [D loss: 0.999542] [G loss: 1.011181]\n",
      "422 [D loss: 1.000278] [G loss: 1.009387]\n",
      "423 [D loss: 1.002043] [G loss: 1.008412]\n",
      "424 [D loss: 1.000567] [G loss: 1.009490]\n",
      "425 [D loss: 1.003071] [G loss: 1.010429]\n",
      "426 [D loss: 1.000296] [G loss: 1.011390]\n",
      "427 [D loss: 1.000713] [G loss: 1.011067]\n",
      "428 [D loss: 1.001508] [G loss: 1.009069]\n",
      "429 [D loss: 1.001428] [G loss: 1.010734]\n",
      "430 [D loss: 1.001920] [G loss: 1.009884]\n",
      "431 [D loss: 0.999350] [G loss: 1.010369]\n",
      "432 [D loss: 1.000058] [G loss: 1.010070]\n",
      "433 [D loss: 0.999993] [G loss: 1.009985]\n",
      "434 [D loss: 1.000978] [G loss: 1.009318]\n",
      "435 [D loss: 1.000896] [G loss: 1.008683]\n",
      "436 [D loss: 1.001471] [G loss: 1.007279]\n",
      "437 [D loss: 1.001721] [G loss: 1.008169]\n",
      "438 [D loss: 1.000728] [G loss: 1.010946]\n",
      "439 [D loss: 1.001473] [G loss: 1.008812]\n",
      "440 [D loss: 1.001307] [G loss: 1.011082]\n",
      "441 [D loss: 1.001373] [G loss: 1.011565]\n",
      "442 [D loss: 1.003503] [G loss: 1.008243]\n",
      "443 [D loss: 1.001085] [G loss: 1.009730]\n",
      "444 [D loss: 1.002083] [G loss: 1.009803]\n",
      "445 [D loss: 1.000856] [G loss: 1.009160]\n",
      "446 [D loss: 0.999885] [G loss: 1.009037]\n",
      "447 [D loss: 1.002545] [G loss: 1.007859]\n",
      "448 [D loss: 1.002962] [G loss: 1.007667]\n",
      "449 [D loss: 0.999680] [G loss: 1.010198]\n",
      "450 [D loss: 1.001787] [G loss: 1.009422]\n",
      "451 [D loss: 1.002528] [G loss: 1.005284]\n",
      "452 [D loss: 1.001327] [G loss: 1.007382]\n",
      "453 [D loss: 1.002501] [G loss: 1.005679]\n",
      "454 [D loss: 1.002540] [G loss: 1.008223]\n",
      "455 [D loss: 1.002941] [G loss: 1.005536]\n",
      "456 [D loss: 1.001253] [G loss: 1.003978]\n",
      "457 [D loss: 0.999574] [G loss: 1.007280]\n",
      "458 [D loss: 1.001790] [G loss: 1.006977]\n",
      "459 [D loss: 1.001337] [G loss: 1.006429]\n",
      "460 [D loss: 1.000075] [G loss: 1.008348]\n",
      "461 [D loss: 1.003135] [G loss: 1.005796]\n",
      "462 [D loss: 1.000907] [G loss: 1.008643]\n",
      "463 [D loss: 1.002572] [G loss: 1.006997]\n",
      "464 [D loss: 1.001106] [G loss: 1.010108]\n",
      "465 [D loss: 1.001203] [G loss: 1.007042]\n",
      "466 [D loss: 1.001687] [G loss: 1.007251]\n",
      "467 [D loss: 1.002000] [G loss: 1.008571]\n",
      "468 [D loss: 1.001030] [G loss: 1.005512]\n",
      "469 [D loss: 1.002034] [G loss: 1.010784]\n",
      "470 [D loss: 1.000692] [G loss: 1.007786]\n",
      "471 [D loss: 0.998657] [G loss: 1.009065]\n",
      "472 [D loss: 1.000610] [G loss: 1.009862]\n",
      "473 [D loss: 1.001829] [G loss: 1.010104]\n",
      "474 [D loss: 1.002389] [G loss: 1.006191]\n",
      "475 [D loss: 1.001598] [G loss: 1.008895]\n",
      "476 [D loss: 1.002977] [G loss: 1.007175]\n",
      "477 [D loss: 1.000990] [G loss: 1.009925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 [D loss: 1.000571] [G loss: 1.006506]\n",
      "479 [D loss: 1.001629] [G loss: 1.005088]\n",
      "480 [D loss: 1.000479] [G loss: 1.003350]\n",
      "481 [D loss: 1.004001] [G loss: 1.004987]\n",
      "482 [D loss: 1.001817] [G loss: 1.004623]\n",
      "483 [D loss: 1.001884] [G loss: 1.005954]\n",
      "484 [D loss: 1.001599] [G loss: 1.005977]\n",
      "485 [D loss: 1.001178] [G loss: 1.007533]\n",
      "486 [D loss: 1.002003] [G loss: 1.005505]\n",
      "487 [D loss: 1.002686] [G loss: 1.007459]\n",
      "488 [D loss: 1.000727] [G loss: 1.007466]\n",
      "489 [D loss: 1.000467] [G loss: 1.004441]\n",
      "490 [D loss: 1.002463] [G loss: 1.008149]\n",
      "491 [D loss: 1.000099] [G loss: 1.008744]\n",
      "492 [D loss: 1.001268] [G loss: 1.008366]\n",
      "493 [D loss: 0.999519] [G loss: 1.004643]\n",
      "494 [D loss: 1.001881] [G loss: 1.006498]\n",
      "495 [D loss: 1.001592] [G loss: 1.006939]\n",
      "496 [D loss: 1.000909] [G loss: 1.004155]\n",
      "497 [D loss: 1.001074] [G loss: 1.004356]\n",
      "498 [D loss: 1.002036] [G loss: 1.003369]\n",
      "499 [D loss: 1.001461] [G loss: 1.005293]\n",
      "500 [D loss: 1.002020] [G loss: 1.005387]\n",
      "501 [D loss: 1.002238] [G loss: 1.004357]\n",
      "502 [D loss: 1.003137] [G loss: 1.003256]\n",
      "503 [D loss: 1.004384] [G loss: 1.002191]\n",
      "504 [D loss: 1.000970] [G loss: 1.001794]\n",
      "505 [D loss: 1.001799] [G loss: 1.005148]\n",
      "506 [D loss: 1.003235] [G loss: 1.004207]\n",
      "507 [D loss: 1.000833] [G loss: 1.007105]\n",
      "508 [D loss: 1.002206] [G loss: 1.002468]\n",
      "509 [D loss: 1.001413] [G loss: 0.999855]\n",
      "510 [D loss: 1.002592] [G loss: 1.002780]\n",
      "511 [D loss: 1.001581] [G loss: 1.002927]\n",
      "512 [D loss: 1.003696] [G loss: 1.001432]\n",
      "513 [D loss: 1.002220] [G loss: 0.999794]\n",
      "514 [D loss: 0.999562] [G loss: 1.004086]\n",
      "515 [D loss: 0.999948] [G loss: 1.003645]\n",
      "516 [D loss: 1.000641] [G loss: 1.003589]\n",
      "517 [D loss: 1.000936] [G loss: 1.001779]\n",
      "518 [D loss: 0.999043] [G loss: 1.001018]\n",
      "519 [D loss: 1.002342] [G loss: 0.998894]\n",
      "520 [D loss: 1.001612] [G loss: 0.998615]\n",
      "521 [D loss: 1.003063] [G loss: 0.996218]\n",
      "522 [D loss: 1.003714] [G loss: 1.001710]\n",
      "523 [D loss: 1.002574] [G loss: 1.002552]\n",
      "524 [D loss: 1.002731] [G loss: 0.999482]\n",
      "525 [D loss: 1.001667] [G loss: 1.000311]\n",
      "526 [D loss: 1.001624] [G loss: 1.000173]\n",
      "527 [D loss: 1.002041] [G loss: 1.000301]\n",
      "528 [D loss: 1.002994] [G loss: 0.999720]\n",
      "529 [D loss: 1.001324] [G loss: 0.999386]\n",
      "530 [D loss: 1.002622] [G loss: 0.999664]\n",
      "531 [D loss: 1.001501] [G loss: 1.001654]\n",
      "532 [D loss: 1.002539] [G loss: 1.001193]\n",
      "533 [D loss: 1.001497] [G loss: 0.998802]\n",
      "534 [D loss: 1.003254] [G loss: 0.999881]\n",
      "535 [D loss: 1.002881] [G loss: 0.999876]\n",
      "536 [D loss: 1.002794] [G loss: 1.000805]\n",
      "537 [D loss: 1.001843] [G loss: 1.000060]\n",
      "538 [D loss: 1.003817] [G loss: 0.998536]\n",
      "539 [D loss: 1.001604] [G loss: 0.999223]\n",
      "540 [D loss: 1.000269] [G loss: 1.000802]\n",
      "541 [D loss: 1.004275] [G loss: 0.999120]\n",
      "542 [D loss: 1.002270] [G loss: 0.999589]\n",
      "543 [D loss: 1.000809] [G loss: 1.000495]\n",
      "544 [D loss: 1.002907] [G loss: 0.997865]\n",
      "545 [D loss: 1.002049] [G loss: 0.997414]\n",
      "546 [D loss: 1.004588] [G loss: 1.000691]\n",
      "547 [D loss: 1.005202] [G loss: 0.998822]\n",
      "548 [D loss: 1.001563] [G loss: 0.997371]\n",
      "549 [D loss: 1.003898] [G loss: 0.999809]\n",
      "550 [D loss: 1.000694] [G loss: 0.999301]\n",
      "551 [D loss: 1.002929] [G loss: 1.000522]\n",
      "552 [D loss: 1.002319] [G loss: 1.000931]\n",
      "553 [D loss: 1.003584] [G loss: 1.000293]\n",
      "554 [D loss: 1.003175] [G loss: 0.997993]\n",
      "555 [D loss: 1.002967] [G loss: 1.003020]\n",
      "556 [D loss: 1.004669] [G loss: 1.000591]\n",
      "557 [D loss: 1.002914] [G loss: 1.001077]\n",
      "558 [D loss: 1.003901] [G loss: 0.998539]\n",
      "559 [D loss: 1.003235] [G loss: 1.000820]\n",
      "560 [D loss: 1.005274] [G loss: 0.996404]\n",
      "561 [D loss: 1.003295] [G loss: 0.997746]\n",
      "562 [D loss: 1.004411] [G loss: 0.998124]\n",
      "563 [D loss: 1.004023] [G loss: 0.994952]\n",
      "564 [D loss: 1.002906] [G loss: 0.994628]\n",
      "565 [D loss: 1.003549] [G loss: 0.996840]\n",
      "566 [D loss: 1.003093] [G loss: 0.999360]\n",
      "567 [D loss: 1.002804] [G loss: 0.995788]\n",
      "568 [D loss: 1.002219] [G loss: 0.995599]\n",
      "569 [D loss: 1.004040] [G loss: 0.994721]\n",
      "570 [D loss: 1.001149] [G loss: 1.005310]\n",
      "571 [D loss: 1.002252] [G loss: 0.999276]\n",
      "572 [D loss: 1.005054] [G loss: 0.998096]\n",
      "573 [D loss: 1.004223] [G loss: 1.001258]\n",
      "574 [D loss: 1.002341] [G loss: 1.000841]\n",
      "575 [D loss: 1.000702] [G loss: 1.000197]\n",
      "576 [D loss: 1.003510] [G loss: 0.997052]\n",
      "577 [D loss: 1.001806] [G loss: 1.003762]\n",
      "578 [D loss: 1.002937] [G loss: 0.997206]\n",
      "579 [D loss: 1.004722] [G loss: 0.996361]\n",
      "580 [D loss: 1.002948] [G loss: 0.998675]\n",
      "581 [D loss: 1.002063] [G loss: 0.994858]\n",
      "582 [D loss: 1.002267] [G loss: 1.001893]\n",
      "583 [D loss: 1.004364] [G loss: 0.997592]\n",
      "584 [D loss: 1.003715] [G loss: 0.996402]\n",
      "585 [D loss: 1.005723] [G loss: 0.994017]\n",
      "586 [D loss: 1.003042] [G loss: 0.995940]\n",
      "587 [D loss: 1.003690] [G loss: 0.995743]\n",
      "588 [D loss: 1.001400] [G loss: 0.998563]\n",
      "589 [D loss: 1.003808] [G loss: 1.000027]\n",
      "590 [D loss: 1.004925] [G loss: 0.998899]\n",
      "591 [D loss: 1.004670] [G loss: 0.997095]\n",
      "592 [D loss: 1.005130] [G loss: 0.996350]\n",
      "593 [D loss: 1.002763] [G loss: 0.994669]\n",
      "594 [D loss: 1.001710] [G loss: 0.999421]\n",
      "595 [D loss: 1.001576] [G loss: 0.998558]\n",
      "596 [D loss: 1.006825] [G loss: 0.992301]\n",
      "597 [D loss: 1.003831] [G loss: 0.995545]\n",
      "598 [D loss: 1.002978] [G loss: 0.991847]\n",
      "599 [D loss: 1.003204] [G loss: 0.995867]\n",
      "600 [D loss: 1.004996] [G loss: 0.987918]\n",
      "601 [D loss: 1.005128] [G loss: 0.989432]\n",
      "602 [D loss: 1.005816] [G loss: 0.992842]\n",
      "603 [D loss: 1.002271] [G loss: 0.998070]\n",
      "604 [D loss: 1.003546] [G loss: 0.994597]\n",
      "605 [D loss: 1.001932] [G loss: 0.992269]\n",
      "606 [D loss: 1.004397] [G loss: 0.992489]\n",
      "607 [D loss: 1.003801] [G loss: 0.990191]\n",
      "608 [D loss: 1.004655] [G loss: 0.993940]\n",
      "609 [D loss: 1.003824] [G loss: 0.991655]\n",
      "610 [D loss: 1.003745] [G loss: 0.997536]\n",
      "611 [D loss: 1.005273] [G loss: 0.995400]\n",
      "612 [D loss: 1.002159] [G loss: 0.994752]\n",
      "613 [D loss: 1.003063] [G loss: 0.997803]\n",
      "614 [D loss: 1.006452] [G loss: 0.992241]\n",
      "615 [D loss: 1.005295] [G loss: 0.993396]\n",
      "616 [D loss: 1.005921] [G loss: 0.993381]\n",
      "617 [D loss: 1.005006] [G loss: 0.995876]\n",
      "618 [D loss: 1.003044] [G loss: 0.993588]\n",
      "619 [D loss: 1.002976] [G loss: 0.995070]\n",
      "620 [D loss: 1.005504] [G loss: 0.994561]\n",
      "621 [D loss: 1.004644] [G loss: 0.992333]\n",
      "622 [D loss: 1.005495] [G loss: 0.991852]\n",
      "623 [D loss: 1.004212] [G loss: 0.990580]\n",
      "624 [D loss: 1.001432] [G loss: 0.995771]\n",
      "625 [D loss: 1.003089] [G loss: 0.995281]\n",
      "626 [D loss: 1.005591] [G loss: 0.991461]\n",
      "627 [D loss: 1.000647] [G loss: 0.992841]\n",
      "628 [D loss: 1.001817] [G loss: 0.992647]\n",
      "629 [D loss: 1.002911] [G loss: 0.992888]\n",
      "630 [D loss: 1.003551] [G loss: 0.990372]\n",
      "631 [D loss: 1.006166] [G loss: 0.992712]\n",
      "632 [D loss: 1.006430] [G loss: 0.989491]\n",
      "633 [D loss: 1.004546] [G loss: 0.995645]\n",
      "634 [D loss: 1.002882] [G loss: 0.997394]\n",
      "635 [D loss: 1.001738] [G loss: 0.993660]\n",
      "636 [D loss: 1.004990] [G loss: 0.991892]\n",
      "637 [D loss: 1.005971] [G loss: 0.995638]\n",
      "638 [D loss: 1.001430] [G loss: 0.993279]\n",
      "639 [D loss: 1.006707] [G loss: 0.988493]\n",
      "640 [D loss: 1.005119] [G loss: 0.992934]\n",
      "641 [D loss: 1.005291] [G loss: 0.994392]\n",
      "642 [D loss: 1.002638] [G loss: 0.990476]\n",
      "643 [D loss: 1.006227] [G loss: 0.992340]\n",
      "644 [D loss: 1.007944] [G loss: 0.985320]\n",
      "645 [D loss: 1.005564] [G loss: 0.990418]\n",
      "646 [D loss: 1.005734] [G loss: 0.990598]\n",
      "647 [D loss: 1.004593] [G loss: 0.990963]\n",
      "648 [D loss: 1.009544] [G loss: 0.988302]\n",
      "649 [D loss: 1.005867] [G loss: 0.998313]\n",
      "650 [D loss: 1.000481] [G loss: 0.998401]\n",
      "651 [D loss: 1.003732] [G loss: 0.994608]\n",
      "652 [D loss: 1.004172] [G loss: 0.993605]\n",
      "653 [D loss: 1.001586] [G loss: 0.992405]\n",
      "654 [D loss: 1.005718] [G loss: 0.993876]\n",
      "655 [D loss: 1.007026] [G loss: 0.992766]\n",
      "656 [D loss: 1.002758] [G loss: 0.992985]\n",
      "657 [D loss: 1.005864] [G loss: 0.993029]\n",
      "658 [D loss: 1.004719] [G loss: 0.994515]\n",
      "659 [D loss: 1.010627] [G loss: 0.987779]\n",
      "660 [D loss: 1.004100] [G loss: 0.991815]\n",
      "661 [D loss: 1.004004] [G loss: 0.989197]\n",
      "662 [D loss: 1.009506] [G loss: 0.986711]\n",
      "663 [D loss: 1.002952] [G loss: 0.984503]\n",
      "664 [D loss: 1.003403] [G loss: 0.990637]\n",
      "665 [D loss: 1.002667] [G loss: 0.988983]\n",
      "666 [D loss: 1.005458] [G loss: 0.987762]\n",
      "667 [D loss: 1.008392] [G loss: 0.985999]\n",
      "668 [D loss: 1.005995] [G loss: 0.992924]\n",
      "669 [D loss: 1.005100] [G loss: 0.987413]\n",
      "670 [D loss: 1.006872] [G loss: 0.991410]\n",
      "671 [D loss: 1.008442] [G loss: 0.989024]\n",
      "672 [D loss: 1.004171] [G loss: 0.987356]\n",
      "673 [D loss: 1.003273] [G loss: 0.992763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 [D loss: 1.009466] [G loss: 0.990894]\n",
      "675 [D loss: 1.006267] [G loss: 0.988162]\n",
      "676 [D loss: 1.007480] [G loss: 0.989681]\n",
      "677 [D loss: 1.002610] [G loss: 0.997033]\n",
      "678 [D loss: 1.000701] [G loss: 0.992255]\n",
      "679 [D loss: 1.003106] [G loss: 0.985880]\n",
      "680 [D loss: 1.011298] [G loss: 0.987304]\n",
      "681 [D loss: 1.004126] [G loss: 0.989335]\n",
      "682 [D loss: 1.009130] [G loss: 0.988190]\n",
      "683 [D loss: 1.006961] [G loss: 0.989270]\n",
      "684 [D loss: 1.005904] [G loss: 0.995703]\n",
      "685 [D loss: 1.008139] [G loss: 0.989569]\n",
      "686 [D loss: 1.003975] [G loss: 0.988540]\n",
      "687 [D loss: 1.001222] [G loss: 0.990393]\n",
      "688 [D loss: 1.010084] [G loss: 0.987182]\n",
      "689 [D loss: 1.005469] [G loss: 0.991716]\n",
      "690 [D loss: 1.003927] [G loss: 0.985965]\n",
      "691 [D loss: 1.007889] [G loss: 0.985619]\n",
      "692 [D loss: 1.003516] [G loss: 0.989829]\n",
      "693 [D loss: 1.002467] [G loss: 0.987327]\n",
      "694 [D loss: 1.003952] [G loss: 0.984183]\n",
      "695 [D loss: 1.002780] [G loss: 0.989137]\n",
      "696 [D loss: 1.005050] [G loss: 0.987955]\n",
      "697 [D loss: 1.006513] [G loss: 0.993046]\n",
      "698 [D loss: 1.006476] [G loss: 0.990594]\n",
      "699 [D loss: 1.002355] [G loss: 0.989208]\n",
      "700 [D loss: 1.007834] [G loss: 0.986872]\n",
      "701 [D loss: 1.005171] [G loss: 0.991733]\n",
      "702 [D loss: 1.002158] [G loss: 0.984764]\n",
      "703 [D loss: 1.007534] [G loss: 0.991678]\n",
      "704 [D loss: 1.007621] [G loss: 0.988657]\n",
      "705 [D loss: 1.006929] [G loss: 0.988626]\n",
      "706 [D loss: 1.006471] [G loss: 0.989300]\n",
      "707 [D loss: 1.004128] [G loss: 0.985883]\n",
      "708 [D loss: 1.007879] [G loss: 0.983330]\n",
      "709 [D loss: 1.008687] [G loss: 0.979447]\n",
      "710 [D loss: 1.007634] [G loss: 0.988525]\n",
      "711 [D loss: 1.004042] [G loss: 0.990382]\n",
      "712 [D loss: 1.006901] [G loss: 0.986491]\n",
      "713 [D loss: 1.003149] [G loss: 0.995267]\n",
      "714 [D loss: 1.004248] [G loss: 0.988222]\n",
      "715 [D loss: 1.004435] [G loss: 0.987634]\n",
      "716 [D loss: 1.006950] [G loss: 0.992181]\n",
      "717 [D loss: 1.005972] [G loss: 0.981570]\n",
      "718 [D loss: 1.007243] [G loss: 0.981205]\n",
      "719 [D loss: 1.004491] [G loss: 0.982595]\n",
      "720 [D loss: 1.004167] [G loss: 0.987874]\n",
      "721 [D loss: 1.009602] [G loss: 0.987195]\n",
      "722 [D loss: 1.009527] [G loss: 0.983487]\n",
      "723 [D loss: 1.004297] [G loss: 0.991500]\n",
      "724 [D loss: 1.000294] [G loss: 0.995783]\n",
      "725 [D loss: 1.008565] [G loss: 0.985397]\n",
      "726 [D loss: 1.004515] [G loss: 0.995898]\n",
      "727 [D loss: 1.006717] [G loss: 0.984847]\n",
      "728 [D loss: 1.011052] [G loss: 0.985046]\n",
      "729 [D loss: 1.007206] [G loss: 0.990449]\n",
      "730 [D loss: 1.007463] [G loss: 0.989297]\n",
      "731 [D loss: 1.005649] [G loss: 0.984231]\n",
      "732 [D loss: 1.006670] [G loss: 0.987029]\n",
      "733 [D loss: 1.006101] [G loss: 0.985484]\n",
      "734 [D loss: 1.001153] [G loss: 0.989060]\n",
      "735 [D loss: 1.003009] [G loss: 0.988600]\n",
      "736 [D loss: 1.003678] [G loss: 0.984703]\n",
      "737 [D loss: 1.004684] [G loss: 0.990021]\n",
      "738 [D loss: 1.006605] [G loss: 0.985510]\n",
      "739 [D loss: 1.007489] [G loss: 0.988489]\n",
      "740 [D loss: 1.006595] [G loss: 0.975609]\n",
      "741 [D loss: 1.007791] [G loss: 0.981257]\n",
      "742 [D loss: 1.011068] [G loss: 0.980903]\n",
      "743 [D loss: 1.006944] [G loss: 0.987168]\n",
      "744 [D loss: 1.010129] [G loss: 0.976373]\n",
      "745 [D loss: 1.005790] [G loss: 0.981245]\n",
      "746 [D loss: 1.007781] [G loss: 0.988409]\n",
      "747 [D loss: 1.011187] [G loss: 0.982393]\n",
      "748 [D loss: 1.006404] [G loss: 0.985359]\n",
      "749 [D loss: 1.007619] [G loss: 0.979325]\n",
      "750 [D loss: 1.006246] [G loss: 0.978558]\n",
      "751 [D loss: 1.004717] [G loss: 0.982782]\n",
      "752 [D loss: 1.011218] [G loss: 0.979443]\n",
      "753 [D loss: 1.008181] [G loss: 0.982072]\n",
      "754 [D loss: 1.001010] [G loss: 0.983656]\n",
      "755 [D loss: 1.009836] [G loss: 0.976654]\n",
      "756 [D loss: 1.003412] [G loss: 0.978311]\n",
      "757 [D loss: 1.008981] [G loss: 0.979755]\n",
      "758 [D loss: 1.006533] [G loss: 0.974438]\n",
      "759 [D loss: 1.007355] [G loss: 0.984815]\n",
      "760 [D loss: 1.007355] [G loss: 0.980494]\n",
      "761 [D loss: 1.004677] [G loss: 0.980609]\n",
      "762 [D loss: 1.008454] [G loss: 0.989147]\n",
      "763 [D loss: 1.005294] [G loss: 0.986284]\n",
      "764 [D loss: 1.010953] [G loss: 0.984136]\n",
      "765 [D loss: 1.009910] [G loss: 0.980665]\n",
      "766 [D loss: 1.004520] [G loss: 0.979111]\n",
      "767 [D loss: 1.006324] [G loss: 0.984320]\n",
      "768 [D loss: 1.005964] [G loss: 0.987283]\n",
      "769 [D loss: 1.006610] [G loss: 0.979199]\n",
      "770 [D loss: 1.010563] [G loss: 0.979445]\n",
      "771 [D loss: 1.007581] [G loss: 0.983024]\n",
      "772 [D loss: 1.008673] [G loss: 0.986116]\n",
      "773 [D loss: 1.003702] [G loss: 0.985860]\n",
      "774 [D loss: 1.008063] [G loss: 0.979337]\n",
      "775 [D loss: 1.004820] [G loss: 0.978772]\n",
      "776 [D loss: 1.012117] [G loss: 0.976750]\n",
      "777 [D loss: 1.007228] [G loss: 0.973845]\n",
      "778 [D loss: 1.005038] [G loss: 0.972357]\n",
      "779 [D loss: 1.005548] [G loss: 0.984946]\n",
      "780 [D loss: 1.015026] [G loss: 0.981299]\n",
      "781 [D loss: 1.006084] [G loss: 0.988132]\n",
      "782 [D loss: 1.006958] [G loss: 0.981748]\n",
      "783 [D loss: 1.012824] [G loss: 0.974860]\n",
      "784 [D loss: 1.013365] [G loss: 0.974951]\n",
      "785 [D loss: 1.014486] [G loss: 0.984917]\n",
      "786 [D loss: 1.009698] [G loss: 0.980347]\n",
      "787 [D loss: 1.005369] [G loss: 0.976286]\n",
      "788 [D loss: 1.013269] [G loss: 0.981238]\n",
      "789 [D loss: 1.004546] [G loss: 0.985989]\n",
      "790 [D loss: 1.010590] [G loss: 0.983703]\n",
      "791 [D loss: 1.006140] [G loss: 0.985987]\n",
      "792 [D loss: 1.004279] [G loss: 0.977626]\n",
      "793 [D loss: 1.004744] [G loss: 0.980462]\n",
      "794 [D loss: 1.009541] [G loss: 0.970292]\n",
      "795 [D loss: 1.007122] [G loss: 0.979317]\n",
      "796 [D loss: 1.004240] [G loss: 0.976901]\n",
      "797 [D loss: 1.008927] [G loss: 0.969212]\n",
      "798 [D loss: 1.005066] [G loss: 0.981284]\n",
      "799 [D loss: 1.010042] [G loss: 0.977849]\n",
      "800 [D loss: 1.010956] [G loss: 0.980407]\n",
      "801 [D loss: 1.005507] [G loss: 0.980814]\n",
      "802 [D loss: 1.010471] [G loss: 0.980548]\n",
      "803 [D loss: 1.010449] [G loss: 0.981595]\n",
      "804 [D loss: 1.008905] [G loss: 0.982877]\n",
      "805 [D loss: 1.011860] [G loss: 0.973909]\n",
      "806 [D loss: 1.011957] [G loss: 0.976591]\n",
      "807 [D loss: 1.006509] [G loss: 0.976484]\n",
      "808 [D loss: 1.005883] [G loss: 0.976556]\n",
      "809 [D loss: 1.007033] [G loss: 0.976948]\n",
      "810 [D loss: 1.010387] [G loss: 0.970297]\n",
      "811 [D loss: 1.007180] [G loss: 0.974232]\n",
      "812 [D loss: 1.003628] [G loss: 0.977000]\n",
      "813 [D loss: 1.009017] [G loss: 0.973767]\n",
      "814 [D loss: 1.010342] [G loss: 0.977453]\n",
      "815 [D loss: 1.011452] [G loss: 0.980865]\n",
      "816 [D loss: 1.008436] [G loss: 0.977953]\n",
      "817 [D loss: 1.010418] [G loss: 0.980793]\n",
      "818 [D loss: 1.005841] [G loss: 0.976486]\n",
      "819 [D loss: 1.012549] [G loss: 0.978562]\n",
      "820 [D loss: 1.012696] [G loss: 0.977513]\n",
      "821 [D loss: 1.016364] [G loss: 0.967928]\n",
      "822 [D loss: 1.008790] [G loss: 0.970054]\n",
      "823 [D loss: 1.009482] [G loss: 0.972976]\n",
      "824 [D loss: 1.006516] [G loss: 0.983659]\n",
      "825 [D loss: 1.009108] [G loss: 0.971560]\n",
      "826 [D loss: 0.999443] [G loss: 0.976237]\n",
      "827 [D loss: 1.012689] [G loss: 0.974647]\n",
      "828 [D loss: 1.005482] [G loss: 0.977159]\n",
      "829 [D loss: 1.004327] [G loss: 0.983273]\n",
      "830 [D loss: 1.014206] [G loss: 0.967592]\n",
      "831 [D loss: 1.003900] [G loss: 0.991511]\n",
      "832 [D loss: 1.012794] [G loss: 0.982296]\n",
      "833 [D loss: 1.008314] [G loss: 0.976632]\n",
      "834 [D loss: 1.008829] [G loss: 0.969086]\n",
      "835 [D loss: 1.012584] [G loss: 0.980092]\n",
      "836 [D loss: 1.005120] [G loss: 0.976465]\n",
      "837 [D loss: 1.007020] [G loss: 0.977727]\n",
      "838 [D loss: 1.013243] [G loss: 0.984718]\n",
      "839 [D loss: 1.013711] [G loss: 0.982546]\n",
      "840 [D loss: 1.011276] [G loss: 0.981561]\n",
      "841 [D loss: 1.011169] [G loss: 0.977965]\n",
      "842 [D loss: 1.005989] [G loss: 0.973390]\n",
      "843 [D loss: 1.007569] [G loss: 0.983818]\n",
      "844 [D loss: 1.007575] [G loss: 0.979095]\n",
      "845 [D loss: 1.009147] [G loss: 0.977049]\n",
      "846 [D loss: 1.009454] [G loss: 0.980067]\n",
      "847 [D loss: 1.003814] [G loss: 0.976108]\n",
      "848 [D loss: 1.008574] [G loss: 0.982319]\n",
      "849 [D loss: 1.009312] [G loss: 0.981426]\n",
      "850 [D loss: 1.010001] [G loss: 0.986662]\n",
      "851 [D loss: 1.005881] [G loss: 0.977281]\n",
      "852 [D loss: 1.004286] [G loss: 0.987714]\n",
      "853 [D loss: 1.014120] [G loss: 0.972839]\n",
      "854 [D loss: 1.010626] [G loss: 0.982115]\n",
      "855 [D loss: 1.012861] [G loss: 0.981417]\n",
      "856 [D loss: 1.007812] [G loss: 0.978418]\n",
      "857 [D loss: 1.009472] [G loss: 0.984862]\n",
      "858 [D loss: 1.005643] [G loss: 0.985476]\n",
      "859 [D loss: 1.004004] [G loss: 0.987294]\n",
      "860 [D loss: 1.004619] [G loss: 0.980135]\n",
      "861 [D loss: 1.011624] [G loss: 0.991126]\n",
      "862 [D loss: 1.012127] [G loss: 0.975398]\n",
      "863 [D loss: 1.008120] [G loss: 0.986609]\n",
      "864 [D loss: 1.008819] [G loss: 0.987967]\n",
      "865 [D loss: 1.009450] [G loss: 0.983579]\n",
      "866 [D loss: 1.011144] [G loss: 0.976836]\n",
      "867 [D loss: 1.016454] [G loss: 0.986801]\n",
      "868 [D loss: 1.009241] [G loss: 0.982016]\n",
      "869 [D loss: 1.006451] [G loss: 0.983242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870 [D loss: 1.010071] [G loss: 0.977045]\n",
      "871 [D loss: 1.008571] [G loss: 0.978243]\n",
      "872 [D loss: 1.015716] [G loss: 0.973587]\n",
      "873 [D loss: 1.016684] [G loss: 0.976855]\n",
      "874 [D loss: 1.009356] [G loss: 0.975907]\n",
      "875 [D loss: 1.010727] [G loss: 0.978275]\n",
      "876 [D loss: 1.007239] [G loss: 0.979752]\n",
      "877 [D loss: 1.007144] [G loss: 0.979936]\n",
      "878 [D loss: 1.006617] [G loss: 0.986285]\n",
      "879 [D loss: 1.010445] [G loss: 0.981002]\n",
      "880 [D loss: 1.020910] [G loss: 0.982442]\n",
      "881 [D loss: 1.010514] [G loss: 0.979414]\n",
      "882 [D loss: 1.010881] [G loss: 0.990891]\n",
      "883 [D loss: 1.015400] [G loss: 0.987453]\n",
      "884 [D loss: 1.009616] [G loss: 0.978367]\n",
      "885 [D loss: 1.014885] [G loss: 0.976075]\n",
      "886 [D loss: 1.009486] [G loss: 0.985968]\n",
      "887 [D loss: 1.010291] [G loss: 0.988157]\n",
      "888 [D loss: 1.012482] [G loss: 0.980137]\n",
      "889 [D loss: 1.011338] [G loss: 0.974929]\n",
      "890 [D loss: 1.016036] [G loss: 0.978462]\n",
      "891 [D loss: 1.005380] [G loss: 0.982933]\n",
      "892 [D loss: 1.005466] [G loss: 0.986259]\n",
      "893 [D loss: 1.004735] [G loss: 0.986025]\n",
      "894 [D loss: 1.000187] [G loss: 0.980469]\n",
      "895 [D loss: 1.013498] [G loss: 0.981324]\n",
      "896 [D loss: 1.013313] [G loss: 0.981217]\n",
      "897 [D loss: 1.016205] [G loss: 0.977561]\n",
      "898 [D loss: 1.015631] [G loss: 0.975469]\n",
      "899 [D loss: 1.010132] [G loss: 0.989713]\n",
      "900 [D loss: 1.010269] [G loss: 0.982112]\n",
      "901 [D loss: 1.011460] [G loss: 0.975886]\n",
      "902 [D loss: 1.020192] [G loss: 0.985586]\n",
      "903 [D loss: 1.010181] [G loss: 0.995395]\n",
      "904 [D loss: 1.011186] [G loss: 0.979626]\n",
      "905 [D loss: 1.009390] [G loss: 0.981605]\n",
      "906 [D loss: 1.009929] [G loss: 0.990010]\n",
      "907 [D loss: 1.014798] [G loss: 0.982265]\n",
      "908 [D loss: 1.007023] [G loss: 0.994374]\n",
      "909 [D loss: 1.014653] [G loss: 0.988786]\n",
      "910 [D loss: 1.013212] [G loss: 0.980833]\n",
      "911 [D loss: 1.003822] [G loss: 0.977121]\n",
      "912 [D loss: 1.008679] [G loss: 0.988985]\n",
      "913 [D loss: 1.011427] [G loss: 0.977999]\n",
      "914 [D loss: 1.004215] [G loss: 0.991423]\n",
      "915 [D loss: 1.007680] [G loss: 0.991494]\n",
      "916 [D loss: 1.008183] [G loss: 0.977167]\n",
      "917 [D loss: 1.010419] [G loss: 0.990521]\n",
      "918 [D loss: 1.015259] [G loss: 0.984431]\n",
      "919 [D loss: 1.006601] [G loss: 0.990380]\n",
      "920 [D loss: 1.009752] [G loss: 0.991519]\n",
      "921 [D loss: 1.012633] [G loss: 0.990179]\n",
      "922 [D loss: 1.011237] [G loss: 0.984114]\n",
      "923 [D loss: 1.007679] [G loss: 0.982499]\n",
      "924 [D loss: 1.015227] [G loss: 0.985987]\n",
      "925 [D loss: 1.013488] [G loss: 0.990493]\n",
      "926 [D loss: 1.008772] [G loss: 0.999803]\n",
      "927 [D loss: 1.014816] [G loss: 0.990714]\n",
      "928 [D loss: 1.014774] [G loss: 0.992626]\n",
      "929 [D loss: 1.007246] [G loss: 0.988634]\n",
      "930 [D loss: 1.016456] [G loss: 0.982295]\n",
      "931 [D loss: 1.011516] [G loss: 0.986346]\n",
      "932 [D loss: 1.013326] [G loss: 0.979916]\n",
      "933 [D loss: 1.012840] [G loss: 0.973609]\n",
      "934 [D loss: 1.017188] [G loss: 0.983150]\n",
      "935 [D loss: 1.010501] [G loss: 0.981078]\n",
      "936 [D loss: 1.016101] [G loss: 0.985788]\n",
      "937 [D loss: 1.009488] [G loss: 0.988078]\n",
      "938 [D loss: 1.010183] [G loss: 0.985036]\n",
      "939 [D loss: 1.014272] [G loss: 0.982709]\n",
      "940 [D loss: 1.010836] [G loss: 0.990083]\n",
      "941 [D loss: 1.004897] [G loss: 0.982391]\n",
      "942 [D loss: 1.003973] [G loss: 0.990261]\n",
      "943 [D loss: 1.009888] [G loss: 0.999191]\n",
      "944 [D loss: 1.014387] [G loss: 0.982919]\n",
      "945 [D loss: 0.997830] [G loss: 0.992693]\n",
      "946 [D loss: 1.007639] [G loss: 0.981865]\n",
      "947 [D loss: 1.011184] [G loss: 0.970191]\n",
      "948 [D loss: 1.003292] [G loss: 0.986757]\n",
      "949 [D loss: 1.007960] [G loss: 0.983589]\n",
      "950 [D loss: 1.015982] [G loss: 0.983501]\n",
      "951 [D loss: 1.002820] [G loss: 0.975644]\n",
      "952 [D loss: 1.006683] [G loss: 0.967572]\n",
      "953 [D loss: 1.010216] [G loss: 0.979272]\n",
      "954 [D loss: 1.014013] [G loss: 0.986782]\n",
      "955 [D loss: 1.007807] [G loss: 0.986308]\n",
      "956 [D loss: 1.009784] [G loss: 0.984594]\n",
      "957 [D loss: 1.014697] [G loss: 0.984009]\n",
      "958 [D loss: 1.010097] [G loss: 0.983078]\n",
      "959 [D loss: 1.010715] [G loss: 0.982628]\n",
      "960 [D loss: 1.018508] [G loss: 0.981576]\n",
      "961 [D loss: 1.013589] [G loss: 0.984591]\n",
      "962 [D loss: 1.013665] [G loss: 0.974563]\n",
      "963 [D loss: 1.012993] [G loss: 0.980737]\n",
      "964 [D loss: 1.012710] [G loss: 0.990108]\n",
      "965 [D loss: 1.009309] [G loss: 0.982411]\n",
      "966 [D loss: 1.013941] [G loss: 0.981773]\n",
      "967 [D loss: 1.006095] [G loss: 0.990520]\n",
      "968 [D loss: 1.012418] [G loss: 0.981705]\n",
      "969 [D loss: 1.005642] [G loss: 0.984538]\n",
      "970 [D loss: 1.021038] [G loss: 0.974884]\n",
      "971 [D loss: 1.010584] [G loss: 0.983785]\n",
      "972 [D loss: 1.006554] [G loss: 0.986709]\n",
      "973 [D loss: 1.021781] [G loss: 0.974134]\n",
      "974 [D loss: 1.014227] [G loss: 0.983463]\n",
      "975 [D loss: 1.009408] [G loss: 0.988385]\n",
      "976 [D loss: 1.009228] [G loss: 1.007264]\n",
      "977 [D loss: 1.023460] [G loss: 0.982755]\n",
      "978 [D loss: 1.009204] [G loss: 0.994112]\n",
      "979 [D loss: 1.010462] [G loss: 1.006235]\n",
      "980 [D loss: 1.013485] [G loss: 1.001600]\n",
      "981 [D loss: 1.007076] [G loss: 1.005284]\n",
      "982 [D loss: 1.008933] [G loss: 0.993316]\n",
      "983 [D loss: 1.003229] [G loss: 1.001206]\n",
      "984 [D loss: 1.011237] [G loss: 0.980422]\n",
      "985 [D loss: 1.015889] [G loss: 0.980778]\n",
      "986 [D loss: 1.009297] [G loss: 0.985570]\n",
      "987 [D loss: 1.015826] [G loss: 0.983473]\n",
      "988 [D loss: 1.015533] [G loss: 0.974415]\n",
      "989 [D loss: 1.008363] [G loss: 0.990601]\n",
      "990 [D loss: 1.007254] [G loss: 0.981877]\n",
      "991 [D loss: 1.009266] [G loss: 0.983090]\n",
      "992 [D loss: 1.004883] [G loss: 0.995360]\n",
      "993 [D loss: 1.010001] [G loss: 0.988217]\n",
      "994 [D loss: 1.011615] [G loss: 0.983441]\n",
      "995 [D loss: 1.020085] [G loss: 0.986995]\n",
      "996 [D loss: 1.020670] [G loss: 0.984352]\n",
      "997 [D loss: 1.021294] [G loss: 0.988560]\n",
      "998 [D loss: 1.007284] [G loss: 0.991649]\n",
      "999 [D loss: 1.009492] [G loss: 1.002334]\n",
      "1000 [D loss: 1.014296] [G loss: 0.987667]\n",
      "1001 [D loss: 1.009102] [G loss: 0.990617]\n",
      "1002 [D loss: 1.023236] [G loss: 0.974585]\n",
      "1003 [D loss: 1.008615] [G loss: 0.994318]\n",
      "1004 [D loss: 1.018822] [G loss: 0.986790]\n",
      "1005 [D loss: 1.019182] [G loss: 0.987966]\n",
      "1006 [D loss: 1.017782] [G loss: 0.999048]\n",
      "1007 [D loss: 1.010874] [G loss: 1.008698]\n",
      "1008 [D loss: 1.003552] [G loss: 1.000566]\n",
      "1009 [D loss: 1.007426] [G loss: 0.991824]\n",
      "1010 [D loss: 1.007743] [G loss: 0.989716]\n",
      "1011 [D loss: 1.015918] [G loss: 0.993645]\n",
      "1012 [D loss: 1.009961] [G loss: 0.991053]\n",
      "1013 [D loss: 1.009652] [G loss: 0.993407]\n",
      "1014 [D loss: 1.003249] [G loss: 0.997899]\n",
      "1015 [D loss: 1.009053] [G loss: 0.989947]\n",
      "1016 [D loss: 1.009986] [G loss: 1.002002]\n",
      "1017 [D loss: 1.010703] [G loss: 0.997351]\n",
      "1018 [D loss: 1.018403] [G loss: 0.994366]\n",
      "1019 [D loss: 1.011743] [G loss: 0.999019]\n",
      "1020 [D loss: 1.010256] [G loss: 1.010040]\n",
      "1021 [D loss: 1.014117] [G loss: 1.007409]\n",
      "1022 [D loss: 1.003842] [G loss: 1.000967]\n",
      "1023 [D loss: 1.002744] [G loss: 1.000550]\n",
      "1024 [D loss: 1.015681] [G loss: 0.990971]\n",
      "1025 [D loss: 1.018755] [G loss: 0.998426]\n",
      "1026 [D loss: 1.016831] [G loss: 1.004821]\n",
      "1027 [D loss: 1.014024] [G loss: 0.994107]\n",
      "1028 [D loss: 1.013684] [G loss: 0.994890]\n",
      "1029 [D loss: 1.010488] [G loss: 1.001681]\n",
      "1030 [D loss: 1.006702] [G loss: 1.009151]\n",
      "1031 [D loss: 1.009698] [G loss: 0.992673]\n",
      "1032 [D loss: 1.018886] [G loss: 0.992766]\n",
      "1033 [D loss: 1.004203] [G loss: 0.999756]\n",
      "1034 [D loss: 1.002054] [G loss: 0.993364]\n",
      "1035 [D loss: 1.019732] [G loss: 0.992516]\n",
      "1036 [D loss: 1.019506] [G loss: 0.988488]\n",
      "1037 [D loss: 1.012362] [G loss: 0.992091]\n",
      "1038 [D loss: 1.008808] [G loss: 0.991513]\n",
      "1039 [D loss: 1.001292] [G loss: 0.991965]\n",
      "1040 [D loss: 1.010761] [G loss: 1.003594]\n",
      "1041 [D loss: 1.017378] [G loss: 1.008035]\n",
      "1042 [D loss: 1.012087] [G loss: 0.994868]\n",
      "1043 [D loss: 1.011350] [G loss: 0.994508]\n",
      "1044 [D loss: 1.010942] [G loss: 0.987584]\n",
      "1045 [D loss: 1.015476] [G loss: 0.996562]\n",
      "1046 [D loss: 1.013060] [G loss: 0.996026]\n",
      "1047 [D loss: 1.016146] [G loss: 0.996101]\n",
      "1048 [D loss: 0.999761] [G loss: 1.009263]\n",
      "1049 [D loss: 1.020725] [G loss: 0.987578]\n",
      "1050 [D loss: 1.010896] [G loss: 0.993609]\n",
      "1051 [D loss: 1.020239] [G loss: 0.987309]\n",
      "1052 [D loss: 1.014072] [G loss: 0.996958]\n",
      "1053 [D loss: 1.014911] [G loss: 0.993518]\n",
      "1054 [D loss: 1.013165] [G loss: 0.996494]\n",
      "1055 [D loss: 1.018062] [G loss: 1.008626]\n",
      "1056 [D loss: 1.020645] [G loss: 0.989926]\n",
      "1057 [D loss: 1.006299] [G loss: 1.003225]\n",
      "1058 [D loss: 1.019770] [G loss: 1.002418]\n",
      "1059 [D loss: 1.012174] [G loss: 1.005416]\n",
      "1060 [D loss: 1.020239] [G loss: 0.999394]\n",
      "1061 [D loss: 1.008187] [G loss: 0.994021]\n",
      "1062 [D loss: 1.018506] [G loss: 0.992971]\n",
      "1063 [D loss: 1.018261] [G loss: 1.007967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064 [D loss: 1.011272] [G loss: 0.986487]\n",
      "1065 [D loss: 1.017305] [G loss: 1.004329]\n",
      "1066 [D loss: 1.010191] [G loss: 1.001807]\n",
      "1067 [D loss: 1.016947] [G loss: 1.008288]\n",
      "1068 [D loss: 1.012334] [G loss: 1.004067]\n",
      "1069 [D loss: 1.013595] [G loss: 0.992897]\n",
      "1070 [D loss: 1.021843] [G loss: 1.001795]\n",
      "1071 [D loss: 1.019501] [G loss: 0.990585]\n",
      "1072 [D loss: 1.011628] [G loss: 0.979267]\n",
      "1073 [D loss: 1.004740] [G loss: 0.997744]\n",
      "1074 [D loss: 1.015629] [G loss: 0.991213]\n",
      "1075 [D loss: 1.005883] [G loss: 1.010083]\n",
      "1076 [D loss: 1.013125] [G loss: 0.992895]\n",
      "1077 [D loss: 1.013941] [G loss: 0.996578]\n",
      "1078 [D loss: 1.012832] [G loss: 0.992929]\n",
      "1079 [D loss: 1.018790] [G loss: 1.010427]\n",
      "1080 [D loss: 1.015718] [G loss: 1.001848]\n",
      "1081 [D loss: 1.015078] [G loss: 0.995055]\n",
      "1082 [D loss: 1.023644] [G loss: 0.994182]\n",
      "1083 [D loss: 1.013752] [G loss: 0.992269]\n",
      "1084 [D loss: 1.024198] [G loss: 1.000706]\n",
      "1085 [D loss: 1.012310] [G loss: 1.012309]\n",
      "1086 [D loss: 1.014065] [G loss: 1.004319]\n",
      "1087 [D loss: 1.021293] [G loss: 0.997459]\n",
      "1088 [D loss: 1.015091] [G loss: 0.994616]\n",
      "1089 [D loss: 1.017420] [G loss: 0.999681]\n",
      "1090 [D loss: 1.011606] [G loss: 1.008294]\n",
      "1091 [D loss: 1.015772] [G loss: 0.986741]\n",
      "1092 [D loss: 1.018375] [G loss: 0.990557]\n",
      "1093 [D loss: 1.016293] [G loss: 0.992103]\n",
      "1094 [D loss: 1.011570] [G loss: 0.987749]\n",
      "1095 [D loss: 1.022345] [G loss: 0.988642]\n",
      "1096 [D loss: 1.009205] [G loss: 0.996027]\n",
      "1097 [D loss: 1.014662] [G loss: 1.002712]\n",
      "1098 [D loss: 1.023091] [G loss: 1.000349]\n",
      "1099 [D loss: 1.016726] [G loss: 0.996139]\n",
      "1100 [D loss: 1.015345] [G loss: 0.994996]\n",
      "1101 [D loss: 1.015887] [G loss: 0.988832]\n",
      "1102 [D loss: 1.007975] [G loss: 0.994697]\n",
      "1103 [D loss: 1.002738] [G loss: 0.994040]\n",
      "1104 [D loss: 1.011741] [G loss: 0.990965]\n",
      "1105 [D loss: 1.004142] [G loss: 1.001222]\n",
      "1106 [D loss: 1.013067] [G loss: 0.993831]\n",
      "1107 [D loss: 1.013526] [G loss: 0.986897]\n",
      "1108 [D loss: 1.021492] [G loss: 0.994462]\n",
      "1109 [D loss: 1.018515] [G loss: 0.995122]\n",
      "1110 [D loss: 1.018885] [G loss: 0.983981]\n",
      "1111 [D loss: 1.009412] [G loss: 0.997693]\n",
      "1112 [D loss: 1.015514] [G loss: 1.007614]\n",
      "1113 [D loss: 1.020335] [G loss: 0.988664]\n",
      "1114 [D loss: 1.020129] [G loss: 0.982378]\n",
      "1115 [D loss: 1.013094] [G loss: 0.996415]\n",
      "1116 [D loss: 1.017254] [G loss: 1.002300]\n",
      "1117 [D loss: 1.014947] [G loss: 0.995548]\n",
      "1118 [D loss: 1.013302] [G loss: 1.005314]\n",
      "1119 [D loss: 1.011514] [G loss: 0.989586]\n",
      "1120 [D loss: 1.017452] [G loss: 1.002101]\n",
      "1121 [D loss: 1.015133] [G loss: 0.979577]\n",
      "1122 [D loss: 1.015966] [G loss: 1.002460]\n",
      "1123 [D loss: 1.014753] [G loss: 1.001697]\n",
      "1124 [D loss: 1.007148] [G loss: 0.995542]\n",
      "1125 [D loss: 1.016732] [G loss: 0.994374]\n",
      "1126 [D loss: 1.017703] [G loss: 0.982754]\n",
      "1127 [D loss: 1.026671] [G loss: 1.000517]\n",
      "1128 [D loss: 1.016190] [G loss: 0.996335]\n",
      "1129 [D loss: 1.016510] [G loss: 0.992034]\n",
      "1130 [D loss: 1.004126] [G loss: 1.018456]\n",
      "1131 [D loss: 1.010524] [G loss: 0.997692]\n",
      "1132 [D loss: 1.014906] [G loss: 1.004608]\n",
      "1133 [D loss: 1.013300] [G loss: 0.991134]\n",
      "1134 [D loss: 1.015726] [G loss: 0.998690]\n",
      "1135 [D loss: 1.022379] [G loss: 1.004000]\n",
      "1136 [D loss: 1.023998] [G loss: 0.980237]\n",
      "1137 [D loss: 1.016632] [G loss: 0.991941]\n",
      "1138 [D loss: 1.013558] [G loss: 0.990585]\n",
      "1139 [D loss: 1.007082] [G loss: 1.000869]\n",
      "1140 [D loss: 1.007302] [G loss: 0.994255]\n",
      "1141 [D loss: 1.024999] [G loss: 0.988050]\n",
      "1142 [D loss: 1.011400] [G loss: 1.002624]\n",
      "1143 [D loss: 1.011978] [G loss: 1.000998]\n",
      "1144 [D loss: 1.020810] [G loss: 0.989460]\n",
      "1145 [D loss: 1.019014] [G loss: 1.002753]\n",
      "1146 [D loss: 1.012139] [G loss: 0.993158]\n",
      "1147 [D loss: 1.011485] [G loss: 0.990175]\n",
      "1148 [D loss: 1.018424] [G loss: 0.992924]\n",
      "1149 [D loss: 1.015754] [G loss: 0.995556]\n",
      "1150 [D loss: 1.006444] [G loss: 0.999868]\n",
      "1151 [D loss: 1.018930] [G loss: 0.989818]\n",
      "1152 [D loss: 1.012582] [G loss: 0.996458]\n",
      "1153 [D loss: 1.005448] [G loss: 1.008041]\n",
      "1154 [D loss: 1.014150] [G loss: 0.988131]\n",
      "1155 [D loss: 1.018335] [G loss: 0.975448]\n",
      "1156 [D loss: 1.007790] [G loss: 0.979681]\n",
      "1157 [D loss: 1.020175] [G loss: 0.982531]\n",
      "1158 [D loss: 1.014763] [G loss: 0.989008]\n",
      "1159 [D loss: 1.005864] [G loss: 1.001716]\n",
      "1160 [D loss: 0.999579] [G loss: 0.985552]\n",
      "1161 [D loss: 1.019259] [G loss: 0.997420]\n",
      "1162 [D loss: 1.013465] [G loss: 0.990844]\n",
      "1163 [D loss: 1.016804] [G loss: 0.986262]\n",
      "1164 [D loss: 1.006105] [G loss: 0.990892]\n",
      "1165 [D loss: 1.025505] [G loss: 0.977858]\n",
      "1166 [D loss: 1.020176] [G loss: 1.004048]\n",
      "1167 [D loss: 1.014280] [G loss: 0.997321]\n",
      "1168 [D loss: 1.019742] [G loss: 0.983784]\n",
      "1169 [D loss: 1.015568] [G loss: 1.000464]\n",
      "1170 [D loss: 1.015296] [G loss: 0.988065]\n",
      "1171 [D loss: 1.014233] [G loss: 0.999060]\n",
      "1172 [D loss: 1.012760] [G loss: 0.984068]\n",
      "1173 [D loss: 1.013972] [G loss: 1.002260]\n",
      "1174 [D loss: 1.008808] [G loss: 0.998646]\n",
      "1175 [D loss: 1.013165] [G loss: 1.005042]\n",
      "1176 [D loss: 1.013834] [G loss: 0.999649]\n",
      "1177 [D loss: 1.017114] [G loss: 0.996805]\n",
      "1178 [D loss: 1.007735] [G loss: 0.993579]\n",
      "1179 [D loss: 1.021514] [G loss: 0.999528]\n",
      "1180 [D loss: 1.006556] [G loss: 0.981888]\n",
      "1181 [D loss: 1.024636] [G loss: 0.998220]\n",
      "1182 [D loss: 1.026394] [G loss: 0.987597]\n",
      "1183 [D loss: 1.020013] [G loss: 1.008458]\n",
      "1184 [D loss: 1.000929] [G loss: 1.011824]\n",
      "1185 [D loss: 1.006417] [G loss: 0.991727]\n",
      "1186 [D loss: 1.012334] [G loss: 0.976687]\n",
      "1187 [D loss: 1.012977] [G loss: 0.999651]\n",
      "1188 [D loss: 1.020657] [G loss: 0.989413]\n",
      "1189 [D loss: 1.006229] [G loss: 1.002220]\n",
      "1190 [D loss: 1.010135] [G loss: 1.004778]\n",
      "1191 [D loss: 1.013216] [G loss: 1.004636]\n",
      "1192 [D loss: 1.017559] [G loss: 0.991761]\n",
      "1193 [D loss: 1.016959] [G loss: 1.011757]\n",
      "1194 [D loss: 1.015238] [G loss: 0.994598]\n",
      "1195 [D loss: 1.002424] [G loss: 1.012513]\n",
      "1196 [D loss: 1.019213] [G loss: 1.006248]\n",
      "1197 [D loss: 1.023376] [G loss: 0.992918]\n",
      "1198 [D loss: 1.019420] [G loss: 1.020015]\n",
      "1199 [D loss: 1.016809] [G loss: 0.985540]\n",
      "1200 [D loss: 1.014118] [G loss: 0.995369]\n",
      "1201 [D loss: 1.011651] [G loss: 1.021294]\n",
      "1202 [D loss: 1.017425] [G loss: 0.990538]\n",
      "1203 [D loss: 1.009185] [G loss: 0.997105]\n",
      "1204 [D loss: 1.004962] [G loss: 0.997822]\n",
      "1205 [D loss: 1.016018] [G loss: 1.003056]\n",
      "1206 [D loss: 1.009123] [G loss: 1.010592]\n",
      "1207 [D loss: 1.021117] [G loss: 1.007138]\n",
      "1208 [D loss: 1.019372] [G loss: 0.998549]\n",
      "1209 [D loss: 1.009338] [G loss: 0.995013]\n",
      "1210 [D loss: 1.020911] [G loss: 0.987844]\n",
      "1211 [D loss: 1.021678] [G loss: 0.988512]\n",
      "1212 [D loss: 1.015831] [G loss: 0.986790]\n",
      "1213 [D loss: 1.011040] [G loss: 0.997344]\n",
      "1214 [D loss: 1.026206] [G loss: 1.000818]\n",
      "1215 [D loss: 1.013197] [G loss: 1.012697]\n",
      "1216 [D loss: 1.024165] [G loss: 1.003136]\n",
      "1217 [D loss: 1.015064] [G loss: 1.012259]\n",
      "1218 [D loss: 1.007092] [G loss: 1.006651]\n",
      "1219 [D loss: 1.010384] [G loss: 0.996144]\n",
      "1220 [D loss: 1.017356] [G loss: 0.998046]\n",
      "1221 [D loss: 1.002170] [G loss: 1.002051]\n",
      "1222 [D loss: 1.013714] [G loss: 1.015717]\n",
      "1223 [D loss: 1.003748] [G loss: 0.994821]\n",
      "1224 [D loss: 1.018645] [G loss: 1.008574]\n",
      "1225 [D loss: 1.019108] [G loss: 1.011608]\n",
      "1226 [D loss: 1.019998] [G loss: 1.000548]\n",
      "1227 [D loss: 1.007938] [G loss: 1.008504]\n",
      "1228 [D loss: 1.001608] [G loss: 1.004079]\n",
      "1229 [D loss: 1.014822] [G loss: 1.012104]\n",
      "1230 [D loss: 1.004528] [G loss: 1.004569]\n",
      "1231 [D loss: 1.007275] [G loss: 1.018046]\n",
      "1232 [D loss: 1.006241] [G loss: 0.999278]\n",
      "1233 [D loss: 1.013632] [G loss: 0.987621]\n",
      "1234 [D loss: 1.017447] [G loss: 0.992091]\n",
      "1235 [D loss: 1.017962] [G loss: 1.014999]\n",
      "1236 [D loss: 1.015077] [G loss: 0.989775]\n",
      "1237 [D loss: 1.017234] [G loss: 0.995920]\n",
      "1238 [D loss: 1.012584] [G loss: 1.010144]\n",
      "1239 [D loss: 1.006782] [G loss: 1.010729]\n",
      "1240 [D loss: 1.003035] [G loss: 1.009263]\n",
      "1241 [D loss: 1.017783] [G loss: 1.010179]\n",
      "1242 [D loss: 1.018128] [G loss: 1.005457]\n",
      "1243 [D loss: 1.010353] [G loss: 1.014819]\n",
      "1244 [D loss: 1.024572] [G loss: 0.988017]\n",
      "1245 [D loss: 1.003862] [G loss: 0.991677]\n",
      "1246 [D loss: 1.010658] [G loss: 1.001974]\n",
      "1247 [D loss: 1.026662] [G loss: 1.010764]\n",
      "1248 [D loss: 1.024937] [G loss: 0.990730]\n",
      "1249 [D loss: 1.019072] [G loss: 0.985110]\n",
      "1250 [D loss: 1.024760] [G loss: 1.000461]\n",
      "1251 [D loss: 1.010356] [G loss: 1.014524]\n",
      "1252 [D loss: 1.017033] [G loss: 0.991634]\n",
      "1253 [D loss: 1.015621] [G loss: 1.006744]\n",
      "1254 [D loss: 1.015223] [G loss: 1.008135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1255 [D loss: 1.006156] [G loss: 1.010725]\n",
      "1256 [D loss: 1.020022] [G loss: 1.004619]\n",
      "1257 [D loss: 1.016348] [G loss: 0.995888]\n",
      "1258 [D loss: 1.015146] [G loss: 1.008420]\n",
      "1259 [D loss: 1.018057] [G loss: 0.985688]\n",
      "1260 [D loss: 1.025251] [G loss: 1.008626]\n",
      "1261 [D loss: 1.015707] [G loss: 1.016578]\n",
      "1262 [D loss: 1.007812] [G loss: 1.011925]\n",
      "1263 [D loss: 1.017537] [G loss: 0.997790]\n",
      "1264 [D loss: 1.019086] [G loss: 1.014918]\n",
      "1265 [D loss: 1.022133] [G loss: 1.000865]\n",
      "1266 [D loss: 1.008695] [G loss: 1.014760]\n",
      "1267 [D loss: 1.011186] [G loss: 1.009730]\n",
      "1268 [D loss: 1.002561] [G loss: 1.010180]\n",
      "1269 [D loss: 1.015430] [G loss: 1.005038]\n",
      "1270 [D loss: 1.013114] [G loss: 0.994699]\n",
      "1271 [D loss: 1.003675] [G loss: 1.003754]\n",
      "1272 [D loss: 1.015882] [G loss: 0.982071]\n",
      "1273 [D loss: 1.015430] [G loss: 0.999387]\n",
      "1274 [D loss: 1.009569] [G loss: 0.983525]\n",
      "1275 [D loss: 1.015339] [G loss: 0.977415]\n",
      "1276 [D loss: 1.007430] [G loss: 0.999362]\n",
      "1277 [D loss: 1.021776] [G loss: 0.999610]\n",
      "1278 [D loss: 1.019263] [G loss: 0.987844]\n",
      "1279 [D loss: 1.017598] [G loss: 0.984984]\n",
      "1280 [D loss: 1.011029] [G loss: 0.980267]\n",
      "1281 [D loss: 1.016899] [G loss: 0.989941]\n",
      "1282 [D loss: 1.018587] [G loss: 0.981942]\n",
      "1283 [D loss: 1.014157] [G loss: 1.002356]\n",
      "1284 [D loss: 0.997543] [G loss: 1.002582]\n",
      "1285 [D loss: 1.007514] [G loss: 1.001633]\n",
      "1286 [D loss: 1.001776] [G loss: 1.010513]\n",
      "1287 [D loss: 1.003016] [G loss: 0.999059]\n",
      "1288 [D loss: 1.006237] [G loss: 0.997415]\n",
      "1289 [D loss: 1.006039] [G loss: 1.002944]\n",
      "1290 [D loss: 1.001719] [G loss: 0.994001]\n",
      "1291 [D loss: 1.008501] [G loss: 0.988148]\n",
      "1292 [D loss: 1.016351] [G loss: 0.984519]\n",
      "1293 [D loss: 1.008327] [G loss: 0.986000]\n",
      "1294 [D loss: 0.999316] [G loss: 0.996209]\n",
      "1295 [D loss: 1.013805] [G loss: 0.986599]\n",
      "1296 [D loss: 1.004267] [G loss: 0.984828]\n",
      "1297 [D loss: 1.021070] [G loss: 0.987545]\n",
      "1298 [D loss: 1.019714] [G loss: 0.994214]\n",
      "1299 [D loss: 1.009918] [G loss: 0.994196]\n",
      "1300 [D loss: 1.020516] [G loss: 0.984082]\n",
      "1301 [D loss: 1.012464] [G loss: 1.000107]\n",
      "1302 [D loss: 1.016449] [G loss: 0.982146]\n",
      "1303 [D loss: 1.017022] [G loss: 1.003913]\n",
      "1304 [D loss: 1.005607] [G loss: 0.993368]\n",
      "1305 [D loss: 1.020853] [G loss: 1.009273]\n",
      "1306 [D loss: 1.014131] [G loss: 0.992319]\n",
      "1307 [D loss: 1.008956] [G loss: 0.989428]\n",
      "1308 [D loss: 1.012678] [G loss: 0.988886]\n",
      "1309 [D loss: 1.021909] [G loss: 1.010856]\n",
      "1310 [D loss: 1.002261] [G loss: 1.004479]\n",
      "1311 [D loss: 1.001998] [G loss: 1.004515]\n",
      "1312 [D loss: 1.022629] [G loss: 1.005261]\n",
      "1313 [D loss: 1.016307] [G loss: 0.968020]\n",
      "1314 [D loss: 1.003697] [G loss: 0.984747]\n",
      "1315 [D loss: 1.015919] [G loss: 0.998002]\n",
      "1316 [D loss: 1.007367] [G loss: 1.015202]\n",
      "1317 [D loss: 1.017961] [G loss: 1.000613]\n",
      "1318 [D loss: 1.015919] [G loss: 0.985816]\n",
      "1319 [D loss: 1.011025] [G loss: 0.995014]\n",
      "1320 [D loss: 1.010945] [G loss: 1.003759]\n",
      "1321 [D loss: 1.012306] [G loss: 0.987372]\n",
      "1322 [D loss: 1.017163] [G loss: 0.995888]\n",
      "1323 [D loss: 1.016461] [G loss: 1.000566]\n",
      "1324 [D loss: 1.014639] [G loss: 0.986090]\n",
      "1325 [D loss: 1.008610] [G loss: 0.991213]\n",
      "1326 [D loss: 1.018936] [G loss: 1.000965]\n",
      "1327 [D loss: 1.010025] [G loss: 0.989141]\n",
      "1328 [D loss: 1.019139] [G loss: 1.000109]\n",
      "1329 [D loss: 1.006732] [G loss: 1.019257]\n",
      "1330 [D loss: 1.001425] [G loss: 1.008854]\n",
      "1331 [D loss: 1.010708] [G loss: 1.004021]\n",
      "1332 [D loss: 1.009118] [G loss: 1.002931]\n",
      "1333 [D loss: 1.017352] [G loss: 1.006969]\n",
      "1334 [D loss: 1.015514] [G loss: 1.018561]\n",
      "1335 [D loss: 1.008578] [G loss: 1.017672]\n",
      "1336 [D loss: 1.017565] [G loss: 0.989267]\n",
      "1337 [D loss: 1.008934] [G loss: 0.982805]\n",
      "1338 [D loss: 1.017521] [G loss: 1.002946]\n",
      "1339 [D loss: 1.018653] [G loss: 0.991273]\n",
      "1340 [D loss: 1.015176] [G loss: 0.999208]\n",
      "1341 [D loss: 1.012849] [G loss: 0.989397]\n",
      "1342 [D loss: 1.015139] [G loss: 0.994356]\n",
      "1343 [D loss: 1.002252] [G loss: 1.005047]\n",
      "1344 [D loss: 0.998272] [G loss: 0.994202]\n",
      "1345 [D loss: 1.007503] [G loss: 1.005803]\n",
      "1346 [D loss: 1.018119] [G loss: 0.998208]\n",
      "1347 [D loss: 1.000846] [G loss: 1.003062]\n",
      "1348 [D loss: 1.016295] [G loss: 1.008720]\n",
      "1349 [D loss: 1.011911] [G loss: 1.006739]\n",
      "1350 [D loss: 1.024526] [G loss: 1.011849]\n",
      "1351 [D loss: 1.015174] [G loss: 1.000310]\n",
      "1352 [D loss: 1.012520] [G loss: 1.004618]\n",
      "1353 [D loss: 1.006312] [G loss: 0.998419]\n",
      "1354 [D loss: 1.018115] [G loss: 0.998626]\n",
      "1355 [D loss: 1.012415] [G loss: 0.996277]\n",
      "1356 [D loss: 1.001606] [G loss: 1.004046]\n",
      "1357 [D loss: 1.016397] [G loss: 0.998128]\n",
      "1358 [D loss: 1.007815] [G loss: 1.001123]\n",
      "1359 [D loss: 1.011970] [G loss: 1.002301]\n",
      "1360 [D loss: 1.012004] [G loss: 0.993035]\n",
      "1361 [D loss: 1.012445] [G loss: 0.989050]\n",
      "1362 [D loss: 1.017618] [G loss: 0.984367]\n",
      "1363 [D loss: 1.016240] [G loss: 0.991875]\n",
      "1364 [D loss: 1.005268] [G loss: 1.010976]\n",
      "1365 [D loss: 1.023441] [G loss: 0.978518]\n",
      "1366 [D loss: 1.016775] [G loss: 0.992981]\n",
      "1367 [D loss: 1.017185] [G loss: 0.981679]\n",
      "1368 [D loss: 1.016252] [G loss: 0.994348]\n",
      "1369 [D loss: 1.007884] [G loss: 0.983449]\n",
      "1370 [D loss: 1.023029] [G loss: 0.995370]\n",
      "1371 [D loss: 1.007764] [G loss: 0.988569]\n",
      "1372 [D loss: 1.019907] [G loss: 0.993395]\n",
      "1373 [D loss: 1.011409] [G loss: 0.988594]\n",
      "1374 [D loss: 1.015443] [G loss: 0.994527]\n",
      "1375 [D loss: 1.009068] [G loss: 0.970653]\n",
      "1376 [D loss: 1.016772] [G loss: 1.002586]\n",
      "1377 [D loss: 1.014127] [G loss: 0.989156]\n",
      "1378 [D loss: 1.006309] [G loss: 1.005473]\n",
      "1379 [D loss: 1.014661] [G loss: 0.997330]\n",
      "1380 [D loss: 1.008328] [G loss: 0.972244]\n",
      "1381 [D loss: 1.023643] [G loss: 0.996032]\n",
      "1382 [D loss: 1.005766] [G loss: 0.983450]\n",
      "1383 [D loss: 1.009499] [G loss: 0.997654]\n",
      "1384 [D loss: 1.007796] [G loss: 1.004567]\n",
      "1385 [D loss: 1.015895] [G loss: 0.979938]\n",
      "1386 [D loss: 1.012735] [G loss: 0.988232]\n",
      "1387 [D loss: 1.023731] [G loss: 0.995174]\n",
      "1388 [D loss: 1.015495] [G loss: 0.991110]\n",
      "1389 [D loss: 1.012461] [G loss: 0.994344]\n",
      "1390 [D loss: 1.010823] [G loss: 0.973198]\n",
      "1391 [D loss: 1.019173] [G loss: 0.989390]\n",
      "1392 [D loss: 1.019330] [G loss: 0.990388]\n",
      "1393 [D loss: 1.006753] [G loss: 0.985497]\n",
      "1394 [D loss: 1.004627] [G loss: 0.978579]\n",
      "1395 [D loss: 1.021010] [G loss: 0.986352]\n",
      "1396 [D loss: 1.012193] [G loss: 0.988772]\n",
      "1397 [D loss: 1.016778] [G loss: 0.987211]\n",
      "1398 [D loss: 1.018477] [G loss: 0.981184]\n",
      "1399 [D loss: 1.018362] [G loss: 1.005862]\n",
      "1400 [D loss: 1.021643] [G loss: 0.978775]\n",
      "1401 [D loss: 1.010526] [G loss: 0.983539]\n",
      "1402 [D loss: 1.022456] [G loss: 0.998184]\n",
      "1403 [D loss: 0.993430] [G loss: 0.989072]\n",
      "1404 [D loss: 1.016331] [G loss: 0.993396]\n",
      "1405 [D loss: 1.026582] [G loss: 0.982563]\n",
      "1406 [D loss: 1.006962] [G loss: 0.990050]\n",
      "1407 [D loss: 1.020674] [G loss: 0.987947]\n",
      "1408 [D loss: 1.015335] [G loss: 0.986951]\n",
      "1409 [D loss: 1.005354] [G loss: 0.994550]\n",
      "1410 [D loss: 1.017312] [G loss: 0.986242]\n",
      "1411 [D loss: 1.015438] [G loss: 0.993825]\n",
      "1412 [D loss: 1.000732] [G loss: 0.992860]\n",
      "1413 [D loss: 1.020192] [G loss: 0.979205]\n",
      "1414 [D loss: 1.018884] [G loss: 0.989873]\n",
      "1415 [D loss: 1.008368] [G loss: 0.997043]\n",
      "1416 [D loss: 1.025679] [G loss: 0.996429]\n",
      "1417 [D loss: 1.023022] [G loss: 0.983897]\n",
      "1418 [D loss: 1.007900] [G loss: 0.978143]\n",
      "1419 [D loss: 1.011015] [G loss: 0.998758]\n",
      "1420 [D loss: 1.012913] [G loss: 1.011163]\n",
      "1421 [D loss: 1.014469] [G loss: 0.972491]\n",
      "1422 [D loss: 1.012155] [G loss: 0.995756]\n",
      "1423 [D loss: 1.023279] [G loss: 1.005647]\n",
      "1424 [D loss: 1.008088] [G loss: 0.991628]\n",
      "1425 [D loss: 1.007995] [G loss: 1.001426]\n",
      "1426 [D loss: 1.014693] [G loss: 0.983873]\n",
      "1427 [D loss: 1.025437] [G loss: 0.977317]\n",
      "1428 [D loss: 1.008791] [G loss: 0.986763]\n",
      "1429 [D loss: 1.005322] [G loss: 0.989150]\n",
      "1430 [D loss: 1.006184] [G loss: 0.997287]\n",
      "1431 [D loss: 1.017611] [G loss: 1.004125]\n",
      "1432 [D loss: 1.013996] [G loss: 0.983552]\n",
      "1433 [D loss: 1.002776] [G loss: 0.994655]\n",
      "1434 [D loss: 1.002413] [G loss: 0.986016]\n",
      "1435 [D loss: 1.008404] [G loss: 0.996285]\n",
      "1436 [D loss: 1.009738] [G loss: 0.993702]\n",
      "1437 [D loss: 1.017029] [G loss: 0.983636]\n",
      "1438 [D loss: 1.015577] [G loss: 0.985812]\n",
      "1439 [D loss: 1.008674] [G loss: 0.984474]\n",
      "1440 [D loss: 1.011786] [G loss: 0.990012]\n",
      "1441 [D loss: 1.009020] [G loss: 0.990590]\n",
      "1442 [D loss: 1.009030] [G loss: 0.974084]\n",
      "1443 [D loss: 1.002423] [G loss: 0.988253]\n",
      "1444 [D loss: 1.014950] [G loss: 0.984794]\n",
      "1445 [D loss: 1.006651] [G loss: 0.988301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446 [D loss: 1.014478] [G loss: 0.989008]\n",
      "1447 [D loss: 1.023889] [G loss: 0.974808]\n",
      "1448 [D loss: 1.018829] [G loss: 0.971930]\n",
      "1449 [D loss: 1.004374] [G loss: 0.977351]\n",
      "1450 [D loss: 1.014323] [G loss: 0.978639]\n",
      "1451 [D loss: 1.018435] [G loss: 0.983619]\n",
      "1452 [D loss: 1.011603] [G loss: 0.993430]\n",
      "1453 [D loss: 1.013653] [G loss: 0.978537]\n",
      "1454 [D loss: 1.013319] [G loss: 0.979525]\n",
      "1455 [D loss: 1.007062] [G loss: 0.977267]\n",
      "1456 [D loss: 1.011881] [G loss: 0.974764]\n",
      "1457 [D loss: 0.996398] [G loss: 0.973595]\n",
      "1458 [D loss: 1.018608] [G loss: 0.967876]\n",
      "1459 [D loss: 1.012686] [G loss: 0.995625]\n",
      "1460 [D loss: 1.016682] [G loss: 0.977312]\n",
      "1461 [D loss: 1.021435] [G loss: 0.990819]\n",
      "1462 [D loss: 1.005119] [G loss: 0.986688]\n",
      "1463 [D loss: 1.016493] [G loss: 0.960907]\n",
      "1464 [D loss: 1.021015] [G loss: 0.981643]\n",
      "1465 [D loss: 1.010704] [G loss: 0.980883]\n",
      "1466 [D loss: 1.000535] [G loss: 0.987674]\n",
      "1467 [D loss: 1.014761] [G loss: 0.996646]\n",
      "1468 [D loss: 1.004449] [G loss: 1.001202]\n",
      "1469 [D loss: 1.014363] [G loss: 0.989255]\n",
      "1470 [D loss: 1.014867] [G loss: 0.994111]\n",
      "1471 [D loss: 1.014038] [G loss: 0.985711]\n",
      "1472 [D loss: 1.014889] [G loss: 0.972905]\n",
      "1473 [D loss: 1.009393] [G loss: 0.977688]\n",
      "1474 [D loss: 1.012011] [G loss: 0.991417]\n",
      "1475 [D loss: 1.007090] [G loss: 0.997861]\n",
      "1476 [D loss: 1.012803] [G loss: 0.966678]\n",
      "1477 [D loss: 1.008621] [G loss: 0.982114]\n",
      "1478 [D loss: 1.008189] [G loss: 1.004508]\n",
      "1479 [D loss: 1.002604] [G loss: 1.000493]\n",
      "1480 [D loss: 1.018304] [G loss: 0.984707]\n",
      "1481 [D loss: 1.006719] [G loss: 0.993750]\n",
      "1482 [D loss: 1.017195] [G loss: 1.009955]\n",
      "1483 [D loss: 1.020412] [G loss: 0.983104]\n",
      "1484 [D loss: 1.020013] [G loss: 0.985901]\n",
      "1485 [D loss: 1.011212] [G loss: 1.006548]\n",
      "1486 [D loss: 1.021690] [G loss: 0.978909]\n",
      "1487 [D loss: 0.988387] [G loss: 0.983376]\n",
      "1488 [D loss: 1.008549] [G loss: 0.989683]\n",
      "1489 [D loss: 1.011122] [G loss: 1.000998]\n",
      "1490 [D loss: 1.004335] [G loss: 0.995871]\n",
      "1491 [D loss: 1.011668] [G loss: 0.993621]\n",
      "1492 [D loss: 1.019257] [G loss: 0.963971]\n",
      "1493 [D loss: 1.003058] [G loss: 0.984731]\n",
      "1494 [D loss: 1.012938] [G loss: 0.978290]\n",
      "1495 [D loss: 1.004178] [G loss: 0.996790]\n",
      "1496 [D loss: 1.010511] [G loss: 0.993198]\n",
      "1497 [D loss: 0.997374] [G loss: 0.993061]\n",
      "1498 [D loss: 1.001953] [G loss: 0.992307]\n",
      "1499 [D loss: 1.016207] [G loss: 1.001167]\n",
      "1500 [D loss: 1.009069] [G loss: 0.985216]\n",
      "1501 [D loss: 1.025649] [G loss: 0.986096]\n",
      "1502 [D loss: 1.013485] [G loss: 0.983756]\n",
      "1503 [D loss: 1.020942] [G loss: 0.991719]\n",
      "1504 [D loss: 1.003592] [G loss: 0.992470]\n",
      "1505 [D loss: 1.012479] [G loss: 0.992522]\n",
      "1506 [D loss: 1.000629] [G loss: 0.978065]\n",
      "1507 [D loss: 0.994386] [G loss: 0.987498]\n",
      "1508 [D loss: 1.003577] [G loss: 0.975473]\n",
      "1509 [D loss: 1.017100] [G loss: 0.990446]\n",
      "1510 [D loss: 1.002745] [G loss: 0.976283]\n",
      "1511 [D loss: 1.011023] [G loss: 0.969416]\n",
      "1512 [D loss: 1.015458] [G loss: 0.986829]\n",
      "1513 [D loss: 1.019447] [G loss: 0.983355]\n",
      "1514 [D loss: 0.999745] [G loss: 0.994692]\n",
      "1515 [D loss: 0.998024] [G loss: 1.005917]\n",
      "1516 [D loss: 1.002632] [G loss: 0.994111]\n",
      "1517 [D loss: 1.005322] [G loss: 0.986972]\n",
      "1518 [D loss: 1.003629] [G loss: 0.983333]\n",
      "1519 [D loss: 1.019157] [G loss: 0.988705]\n",
      "1520 [D loss: 1.011740] [G loss: 0.982196]\n",
      "1521 [D loss: 1.006761] [G loss: 0.985333]\n",
      "1522 [D loss: 0.993882] [G loss: 0.973761]\n",
      "1523 [D loss: 1.015783] [G loss: 0.995845]\n",
      "1524 [D loss: 1.002627] [G loss: 0.976657]\n",
      "1525 [D loss: 0.996932] [G loss: 0.980991]\n",
      "1526 [D loss: 0.999338] [G loss: 0.993480]\n",
      "1527 [D loss: 1.010500] [G loss: 0.972538]\n",
      "1528 [D loss: 1.017062] [G loss: 1.000883]\n",
      "1529 [D loss: 1.017319] [G loss: 0.992483]\n",
      "1530 [D loss: 1.004635] [G loss: 0.973428]\n",
      "1531 [D loss: 1.001355] [G loss: 0.997785]\n",
      "1532 [D loss: 1.007669] [G loss: 0.995128]\n",
      "1533 [D loss: 0.991752] [G loss: 0.991182]\n",
      "1534 [D loss: 1.001698] [G loss: 0.985890]\n",
      "1535 [D loss: 1.013240] [G loss: 0.972978]\n",
      "1536 [D loss: 1.005872] [G loss: 0.987142]\n",
      "1537 [D loss: 1.010598] [G loss: 0.977983]\n",
      "1538 [D loss: 1.001218] [G loss: 0.969579]\n",
      "1539 [D loss: 1.007937] [G loss: 0.980606]\n",
      "1540 [D loss: 1.023391] [G loss: 0.988728]\n",
      "1541 [D loss: 1.007011] [G loss: 0.981392]\n",
      "1542 [D loss: 1.014308] [G loss: 0.979152]\n",
      "1543 [D loss: 1.008977] [G loss: 0.976460]\n",
      "1544 [D loss: 1.021748] [G loss: 0.961798]\n",
      "1545 [D loss: 1.010460] [G loss: 0.983241]\n",
      "1546 [D loss: 1.012031] [G loss: 0.980172]\n",
      "1547 [D loss: 1.016930] [G loss: 0.981899]\n",
      "1548 [D loss: 1.009726] [G loss: 0.979454]\n",
      "1549 [D loss: 1.000595] [G loss: 0.973199]\n",
      "1550 [D loss: 1.020151] [G loss: 0.978740]\n",
      "1551 [D loss: 1.022569] [G loss: 0.977302]\n",
      "1552 [D loss: 1.011210] [G loss: 1.000905]\n",
      "1553 [D loss: 1.010255] [G loss: 0.996595]\n",
      "1554 [D loss: 1.021269] [G loss: 0.976360]\n",
      "1555 [D loss: 1.008652] [G loss: 0.977635]\n",
      "1556 [D loss: 1.024196] [G loss: 0.973852]\n",
      "1557 [D loss: 1.020130] [G loss: 0.980315]\n",
      "1558 [D loss: 1.014285] [G loss: 0.976445]\n",
      "1559 [D loss: 1.004168] [G loss: 0.994186]\n",
      "1560 [D loss: 1.007463] [G loss: 0.991146]\n",
      "1561 [D loss: 1.012699] [G loss: 0.981803]\n",
      "1562 [D loss: 1.021078] [G loss: 0.968437]\n",
      "1563 [D loss: 1.005282] [G loss: 0.981896]\n",
      "1564 [D loss: 1.007434] [G loss: 0.982933]\n",
      "1565 [D loss: 1.003674] [G loss: 0.999960]\n",
      "1566 [D loss: 0.997983] [G loss: 0.976952]\n",
      "1567 [D loss: 1.009934] [G loss: 0.971097]\n",
      "1568 [D loss: 1.015758] [G loss: 0.963875]\n",
      "1569 [D loss: 1.005993] [G loss: 0.977870]\n",
      "1570 [D loss: 1.006388] [G loss: 0.987811]\n",
      "1571 [D loss: 1.015357] [G loss: 0.990106]\n",
      "1572 [D loss: 0.987960] [G loss: 0.999398]\n",
      "1573 [D loss: 1.014529] [G loss: 0.970892]\n",
      "1574 [D loss: 1.013247] [G loss: 0.966616]\n",
      "1575 [D loss: 1.006892] [G loss: 0.981069]\n",
      "1576 [D loss: 1.019003] [G loss: 0.991233]\n",
      "1577 [D loss: 0.999336] [G loss: 0.993367]\n",
      "1578 [D loss: 1.004035] [G loss: 1.001292]\n",
      "1579 [D loss: 1.007441] [G loss: 1.000040]\n",
      "1580 [D loss: 1.012042] [G loss: 0.979905]\n",
      "1581 [D loss: 1.010634] [G loss: 0.978203]\n",
      "1582 [D loss: 1.004086] [G loss: 0.991967]\n",
      "1583 [D loss: 1.012347] [G loss: 0.981618]\n",
      "1584 [D loss: 1.012125] [G loss: 0.998232]\n",
      "1585 [D loss: 1.003928] [G loss: 0.979716]\n",
      "1586 [D loss: 1.012096] [G loss: 0.984681]\n",
      "1587 [D loss: 1.010419] [G loss: 0.993868]\n",
      "1588 [D loss: 1.008521] [G loss: 0.992337]\n",
      "1589 [D loss: 1.013661] [G loss: 0.998419]\n",
      "1590 [D loss: 1.018952] [G loss: 0.987251]\n",
      "1591 [D loss: 1.002931] [G loss: 0.988263]\n",
      "1592 [D loss: 1.011512] [G loss: 0.975677]\n",
      "1593 [D loss: 1.016447] [G loss: 0.979202]\n",
      "1594 [D loss: 0.998830] [G loss: 0.978777]\n",
      "1595 [D loss: 1.007652] [G loss: 0.993939]\n",
      "1596 [D loss: 1.008302] [G loss: 0.994047]\n",
      "1597 [D loss: 1.001124] [G loss: 0.999897]\n",
      "1598 [D loss: 1.016150] [G loss: 0.994226]\n",
      "1599 [D loss: 1.011284] [G loss: 0.984567]\n",
      "1600 [D loss: 1.020437] [G loss: 0.984901]\n",
      "1601 [D loss: 1.015012] [G loss: 0.979286]\n",
      "1602 [D loss: 1.011969] [G loss: 0.978930]\n",
      "1603 [D loss: 0.999108] [G loss: 0.993565]\n",
      "1604 [D loss: 1.007869] [G loss: 0.979295]\n",
      "1605 [D loss: 1.018804] [G loss: 0.981808]\n",
      "1606 [D loss: 1.015190] [G loss: 0.988546]\n",
      "1607 [D loss: 1.010289] [G loss: 0.980476]\n",
      "1608 [D loss: 1.003899] [G loss: 0.971205]\n",
      "1609 [D loss: 1.010666] [G loss: 0.998665]\n",
      "1610 [D loss: 1.009528] [G loss: 0.998135]\n",
      "1611 [D loss: 1.008689] [G loss: 0.977526]\n",
      "1612 [D loss: 1.001702] [G loss: 0.984799]\n",
      "1613 [D loss: 0.997571] [G loss: 0.990820]\n",
      "1614 [D loss: 1.014294] [G loss: 0.989784]\n",
      "1615 [D loss: 1.017988] [G loss: 1.002940]\n",
      "1616 [D loss: 1.011722] [G loss: 0.985325]\n",
      "1617 [D loss: 1.007564] [G loss: 0.968070]\n",
      "1618 [D loss: 1.000479] [G loss: 0.987703]\n",
      "1619 [D loss: 1.007726] [G loss: 0.977259]\n",
      "1620 [D loss: 1.019433] [G loss: 0.973764]\n",
      "1621 [D loss: 1.004141] [G loss: 0.987499]\n",
      "1622 [D loss: 1.010106] [G loss: 0.981544]\n",
      "1623 [D loss: 1.000372] [G loss: 0.999120]\n",
      "1624 [D loss: 1.010416] [G loss: 0.985397]\n",
      "1625 [D loss: 1.011612] [G loss: 0.985037]\n",
      "1626 [D loss: 1.016345] [G loss: 0.974775]\n",
      "1627 [D loss: 1.015809] [G loss: 0.982691]\n",
      "1628 [D loss: 1.015212] [G loss: 0.987941]\n",
      "1629 [D loss: 0.997789] [G loss: 0.989822]\n",
      "1630 [D loss: 1.014727] [G loss: 0.974760]\n",
      "1631 [D loss: 1.012112] [G loss: 0.982373]\n",
      "1632 [D loss: 1.007639] [G loss: 0.997526]\n",
      "1633 [D loss: 1.004839] [G loss: 0.977773]\n",
      "1634 [D loss: 0.997103] [G loss: 0.990872]\n",
      "1635 [D loss: 1.001855] [G loss: 0.996147]\n",
      "1636 [D loss: 1.018224] [G loss: 0.975817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637 [D loss: 1.018774] [G loss: 0.961782]\n",
      "1638 [D loss: 1.010751] [G loss: 0.982237]\n",
      "1639 [D loss: 1.006647] [G loss: 0.987628]\n",
      "1640 [D loss: 1.004694] [G loss: 0.987321]\n",
      "1641 [D loss: 1.009863] [G loss: 0.986757]\n",
      "1642 [D loss: 1.009964] [G loss: 0.994316]\n",
      "1643 [D loss: 1.003614] [G loss: 0.985189]\n",
      "1644 [D loss: 1.013869] [G loss: 0.984572]\n",
      "1645 [D loss: 1.012861] [G loss: 0.979906]\n",
      "1646 [D loss: 1.011923] [G loss: 0.981727]\n",
      "1647 [D loss: 1.005527] [G loss: 0.973396]\n",
      "1648 [D loss: 1.024231] [G loss: 0.992889]\n",
      "1649 [D loss: 1.007341] [G loss: 0.983879]\n",
      "1650 [D loss: 1.004528] [G loss: 0.987827]\n",
      "1651 [D loss: 0.996019] [G loss: 0.969158]\n",
      "1652 [D loss: 1.010054] [G loss: 0.985284]\n",
      "1653 [D loss: 1.008302] [G loss: 0.989299]\n",
      "1654 [D loss: 1.008083] [G loss: 0.984286]\n",
      "1655 [D loss: 1.016889] [G loss: 0.975785]\n",
      "1656 [D loss: 1.004540] [G loss: 0.976121]\n",
      "1657 [D loss: 1.006749] [G loss: 0.979200]\n",
      "1658 [D loss: 1.013229] [G loss: 0.980772]\n",
      "1659 [D loss: 1.007526] [G loss: 0.975460]\n",
      "1660 [D loss: 1.011423] [G loss: 0.985748]\n",
      "1661 [D loss: 1.014948] [G loss: 0.984655]\n",
      "1662 [D loss: 1.004463] [G loss: 0.990048]\n",
      "1663 [D loss: 1.022004] [G loss: 0.971591]\n",
      "1664 [D loss: 1.009797] [G loss: 0.976332]\n",
      "1665 [D loss: 1.012100] [G loss: 0.966631]\n",
      "1666 [D loss: 1.006055] [G loss: 0.980280]\n",
      "1667 [D loss: 1.003665] [G loss: 0.970892]\n",
      "1668 [D loss: 1.027479] [G loss: 0.968515]\n",
      "1669 [D loss: 1.014642] [G loss: 0.965496]\n",
      "1670 [D loss: 1.013811] [G loss: 0.965475]\n",
      "1671 [D loss: 1.007515] [G loss: 0.999908]\n",
      "1672 [D loss: 1.011823] [G loss: 0.959858]\n",
      "1673 [D loss: 1.010346] [G loss: 0.966607]\n",
      "1674 [D loss: 1.012744] [G loss: 0.961067]\n",
      "1675 [D loss: 1.025069] [G loss: 0.978607]\n",
      "1676 [D loss: 0.998022] [G loss: 0.991252]\n",
      "1677 [D loss: 1.004360] [G loss: 0.980587]\n",
      "1678 [D loss: 1.009455] [G loss: 0.989220]\n",
      "1679 [D loss: 1.014655] [G loss: 0.986318]\n",
      "1680 [D loss: 0.993067] [G loss: 0.987345]\n",
      "1681 [D loss: 1.024578] [G loss: 0.966223]\n",
      "1682 [D loss: 1.002372] [G loss: 0.979470]\n",
      "1683 [D loss: 1.014869] [G loss: 0.963037]\n",
      "1684 [D loss: 1.009699] [G loss: 0.978398]\n",
      "1685 [D loss: 1.005590] [G loss: 0.987901]\n",
      "1686 [D loss: 1.002477] [G loss: 0.986507]\n",
      "1687 [D loss: 1.005062] [G loss: 0.981012]\n",
      "1688 [D loss: 1.003735] [G loss: 0.978394]\n",
      "1689 [D loss: 1.001451] [G loss: 0.981142]\n",
      "1690 [D loss: 1.018721] [G loss: 0.971085]\n",
      "1691 [D loss: 1.022664] [G loss: 0.984403]\n",
      "1692 [D loss: 1.010155] [G loss: 0.976519]\n",
      "1693 [D loss: 0.996275] [G loss: 0.973990]\n",
      "1694 [D loss: 1.014713] [G loss: 0.971833]\n",
      "1695 [D loss: 1.009698] [G loss: 0.987946]\n",
      "1696 [D loss: 1.019210] [G loss: 0.975517]\n",
      "1697 [D loss: 1.010960] [G loss: 0.972466]\n",
      "1698 [D loss: 0.997698] [G loss: 0.959281]\n",
      "1699 [D loss: 0.997733] [G loss: 0.973343]\n",
      "1700 [D loss: 1.020799] [G loss: 0.972369]\n",
      "1701 [D loss: 1.005476] [G loss: 0.977435]\n",
      "1702 [D loss: 1.017849] [G loss: 0.971018]\n",
      "1703 [D loss: 1.011021] [G loss: 0.980616]\n",
      "1704 [D loss: 1.010250] [G loss: 0.963788]\n",
      "1705 [D loss: 1.019547] [G loss: 0.960789]\n",
      "1706 [D loss: 1.013947] [G loss: 0.973600]\n",
      "1707 [D loss: 1.015763] [G loss: 0.987542]\n",
      "1708 [D loss: 1.018527] [G loss: 0.968981]\n",
      "1709 [D loss: 1.010313] [G loss: 0.966710]\n",
      "1710 [D loss: 1.017937] [G loss: 0.988761]\n",
      "1711 [D loss: 1.003019] [G loss: 1.006559]\n",
      "1712 [D loss: 1.013997] [G loss: 0.957589]\n",
      "1713 [D loss: 1.016599] [G loss: 0.978948]\n",
      "1714 [D loss: 1.007676] [G loss: 0.979801]\n",
      "1715 [D loss: 1.007589] [G loss: 0.974749]\n",
      "1716 [D loss: 1.010404] [G loss: 0.970914]\n",
      "1717 [D loss: 1.009560] [G loss: 0.972095]\n",
      "1718 [D loss: 1.009676] [G loss: 0.968878]\n",
      "1719 [D loss: 1.011593] [G loss: 0.973941]\n",
      "1720 [D loss: 1.028980] [G loss: 0.967384]\n",
      "1721 [D loss: 1.022532] [G loss: 0.980713]\n",
      "1722 [D loss: 1.015721] [G loss: 0.981892]\n",
      "1723 [D loss: 1.016844] [G loss: 0.981204]\n",
      "1724 [D loss: 1.015124] [G loss: 0.969745]\n",
      "1725 [D loss: 1.015526] [G loss: 0.965890]\n",
      "1726 [D loss: 1.017330] [G loss: 0.971075]\n",
      "1727 [D loss: 1.010376] [G loss: 0.968627]\n",
      "1728 [D loss: 1.011319] [G loss: 0.978485]\n",
      "1729 [D loss: 1.002761] [G loss: 0.994083]\n",
      "1730 [D loss: 1.007269] [G loss: 0.997165]\n",
      "1731 [D loss: 0.997267] [G loss: 0.994080]\n",
      "1732 [D loss: 1.019728] [G loss: 0.980248]\n",
      "1733 [D loss: 1.006172] [G loss: 0.996714]\n",
      "1734 [D loss: 1.007847] [G loss: 0.991176]\n",
      "1735 [D loss: 1.018354] [G loss: 0.982686]\n",
      "1736 [D loss: 1.013591] [G loss: 0.992709]\n",
      "1737 [D loss: 1.007218] [G loss: 0.979564]\n",
      "1738 [D loss: 1.012188] [G loss: 0.975354]\n",
      "1739 [D loss: 1.006262] [G loss: 0.994976]\n",
      "1740 [D loss: 1.014043] [G loss: 0.965645]\n",
      "1741 [D loss: 1.005622] [G loss: 0.975739]\n",
      "1742 [D loss: 1.004525] [G loss: 0.975010]\n",
      "1743 [D loss: 1.015273] [G loss: 0.990965]\n",
      "1744 [D loss: 1.011232] [G loss: 0.991229]\n",
      "1745 [D loss: 1.002858] [G loss: 0.981772]\n",
      "1746 [D loss: 1.003601] [G loss: 0.973668]\n",
      "1747 [D loss: 1.011034] [G loss: 0.987281]\n",
      "1748 [D loss: 1.021678] [G loss: 0.977016]\n",
      "1749 [D loss: 1.004696] [G loss: 0.990340]\n",
      "1750 [D loss: 1.003676] [G loss: 0.975846]\n",
      "1751 [D loss: 1.003324] [G loss: 0.973075]\n",
      "1752 [D loss: 1.016227] [G loss: 0.992799]\n",
      "1753 [D loss: 0.994243] [G loss: 0.989343]\n",
      "1754 [D loss: 1.014552] [G loss: 0.975116]\n",
      "1755 [D loss: 1.022574] [G loss: 0.982453]\n",
      "1756 [D loss: 1.004102] [G loss: 0.982880]\n",
      "1757 [D loss: 1.011048] [G loss: 0.976782]\n",
      "1758 [D loss: 1.024778] [G loss: 0.980147]\n",
      "1759 [D loss: 1.012197] [G loss: 0.980061]\n",
      "1760 [D loss: 1.010314] [G loss: 0.994184]\n",
      "1761 [D loss: 0.997644] [G loss: 0.988953]\n",
      "1762 [D loss: 1.003695] [G loss: 0.977981]\n",
      "1763 [D loss: 1.013857] [G loss: 0.971633]\n",
      "1764 [D loss: 1.016769] [G loss: 0.968308]\n",
      "1765 [D loss: 1.009185] [G loss: 0.982667]\n",
      "1766 [D loss: 0.999759] [G loss: 0.988863]\n",
      "1767 [D loss: 1.006007] [G loss: 0.994048]\n",
      "1768 [D loss: 1.014460] [G loss: 0.984819]\n",
      "1769 [D loss: 1.003212] [G loss: 0.979794]\n",
      "1770 [D loss: 0.999721] [G loss: 1.003074]\n",
      "1771 [D loss: 1.018101] [G loss: 0.986595]\n",
      "1772 [D loss: 0.998498] [G loss: 0.991767]\n",
      "1773 [D loss: 0.998302] [G loss: 0.994659]\n",
      "1774 [D loss: 1.020380] [G loss: 0.991263]\n",
      "1775 [D loss: 1.010248] [G loss: 0.989789]\n",
      "1776 [D loss: 0.998167] [G loss: 0.992603]\n",
      "1777 [D loss: 1.005996] [G loss: 1.007611]\n",
      "1778 [D loss: 1.003679] [G loss: 0.981610]\n",
      "1779 [D loss: 1.003421] [G loss: 0.995590]\n",
      "1780 [D loss: 1.004758] [G loss: 0.989740]\n",
      "1781 [D loss: 1.012241] [G loss: 0.995028]\n",
      "1782 [D loss: 1.011201] [G loss: 0.984065]\n",
      "1783 [D loss: 1.008983] [G loss: 0.991842]\n",
      "1784 [D loss: 0.998590] [G loss: 1.003805]\n",
      "1785 [D loss: 0.999522] [G loss: 0.996985]\n",
      "1786 [D loss: 0.992330] [G loss: 0.997287]\n",
      "1787 [D loss: 1.009254] [G loss: 0.990996]\n",
      "1788 [D loss: 1.009260] [G loss: 1.005939]\n",
      "1789 [D loss: 1.008267] [G loss: 0.990047]\n",
      "1790 [D loss: 1.006576] [G loss: 0.995935]\n",
      "1791 [D loss: 1.009793] [G loss: 0.998652]\n",
      "1792 [D loss: 1.015711] [G loss: 0.985811]\n",
      "1793 [D loss: 1.010374] [G loss: 0.999188]\n",
      "1794 [D loss: 1.001250] [G loss: 0.996028]\n",
      "1795 [D loss: 1.003278] [G loss: 1.000098]\n",
      "1796 [D loss: 1.000265] [G loss: 1.008393]\n",
      "1797 [D loss: 1.010470] [G loss: 0.987932]\n",
      "1798 [D loss: 0.994465] [G loss: 1.006853]\n",
      "1799 [D loss: 1.007844] [G loss: 0.980849]\n",
      "1800 [D loss: 1.006969] [G loss: 0.986185]\n",
      "1801 [D loss: 1.008909] [G loss: 0.984086]\n",
      "1802 [D loss: 1.002148] [G loss: 0.997411]\n",
      "1803 [D loss: 1.010758] [G loss: 0.985367]\n",
      "1804 [D loss: 1.008263] [G loss: 0.977480]\n",
      "1805 [D loss: 1.019449] [G loss: 0.996685]\n",
      "1806 [D loss: 1.005319] [G loss: 0.995490]\n",
      "1807 [D loss: 1.001720] [G loss: 0.987109]\n",
      "1808 [D loss: 1.015716] [G loss: 0.980105]\n",
      "1809 [D loss: 1.004399] [G loss: 1.001090]\n",
      "1810 [D loss: 1.012295] [G loss: 1.008124]\n",
      "1811 [D loss: 1.006028] [G loss: 1.014917]\n",
      "1812 [D loss: 0.984820] [G loss: 0.996670]\n",
      "1813 [D loss: 1.009125] [G loss: 0.983462]\n",
      "1814 [D loss: 1.018853] [G loss: 0.990387]\n",
      "1815 [D loss: 1.007240] [G loss: 0.987310]\n",
      "1816 [D loss: 1.017639] [G loss: 0.984037]\n",
      "1817 [D loss: 1.017303] [G loss: 0.986298]\n",
      "1818 [D loss: 1.006710] [G loss: 0.996237]\n",
      "1819 [D loss: 1.018890] [G loss: 0.974696]\n",
      "1820 [D loss: 1.004360] [G loss: 0.981883]\n",
      "1821 [D loss: 1.000371] [G loss: 0.976310]\n",
      "1822 [D loss: 1.003875] [G loss: 0.997622]\n",
      "1823 [D loss: 0.995517] [G loss: 1.000594]\n",
      "1824 [D loss: 1.013749] [G loss: 0.993035]\n",
      "1825 [D loss: 1.017830] [G loss: 0.973597]\n",
      "1826 [D loss: 1.018036] [G loss: 0.995844]\n",
      "1827 [D loss: 0.997749] [G loss: 0.997782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828 [D loss: 1.019822] [G loss: 0.988199]\n",
      "1829 [D loss: 1.015265] [G loss: 0.988743]\n",
      "1830 [D loss: 1.011144] [G loss: 1.007845]\n",
      "1831 [D loss: 0.996494] [G loss: 0.990312]\n",
      "1832 [D loss: 1.005510] [G loss: 0.996173]\n",
      "1833 [D loss: 1.001169] [G loss: 0.997509]\n",
      "1834 [D loss: 1.013929] [G loss: 0.994080]\n",
      "1835 [D loss: 1.020045] [G loss: 0.994882]\n",
      "1836 [D loss: 0.999441] [G loss: 0.994835]\n",
      "1837 [D loss: 1.017731] [G loss: 0.996594]\n",
      "1838 [D loss: 0.995537] [G loss: 0.976619]\n",
      "1839 [D loss: 1.000673] [G loss: 0.982708]\n",
      "1840 [D loss: 1.008498] [G loss: 0.980105]\n",
      "1841 [D loss: 0.996956] [G loss: 0.986305]\n",
      "1842 [D loss: 1.006046] [G loss: 0.966152]\n",
      "1843 [D loss: 1.016149] [G loss: 0.974272]\n",
      "1844 [D loss: 1.013199] [G loss: 0.990968]\n",
      "1845 [D loss: 1.004283] [G loss: 0.973452]\n",
      "1846 [D loss: 1.003473] [G loss: 0.979472]\n",
      "1847 [D loss: 1.016291] [G loss: 0.970315]\n",
      "1848 [D loss: 1.006341] [G loss: 0.964899]\n",
      "1849 [D loss: 1.002572] [G loss: 0.973751]\n",
      "1850 [D loss: 1.001422] [G loss: 0.984795]\n",
      "1851 [D loss: 1.007140] [G loss: 0.987192]\n",
      "1852 [D loss: 1.014621] [G loss: 0.973521]\n",
      "1853 [D loss: 1.009890] [G loss: 0.992528]\n",
      "1854 [D loss: 1.006047] [G loss: 1.009473]\n",
      "1855 [D loss: 1.001545] [G loss: 0.999740]\n",
      "1856 [D loss: 1.008911] [G loss: 0.989805]\n",
      "1857 [D loss: 1.005842] [G loss: 0.983787]\n",
      "1858 [D loss: 0.998873] [G loss: 0.971010]\n",
      "1859 [D loss: 0.995738] [G loss: 0.992520]\n",
      "1860 [D loss: 1.013759] [G loss: 0.987461]\n",
      "1861 [D loss: 1.005683] [G loss: 0.973862]\n",
      "1862 [D loss: 1.012442] [G loss: 0.981723]\n",
      "1863 [D loss: 0.996296] [G loss: 0.988098]\n",
      "1864 [D loss: 0.992713] [G loss: 0.975831]\n",
      "1865 [D loss: 1.011098] [G loss: 0.970832]\n",
      "1866 [D loss: 1.018376] [G loss: 0.986913]\n",
      "1867 [D loss: 1.012314] [G loss: 0.970832]\n",
      "1868 [D loss: 1.021221] [G loss: 0.976645]\n",
      "1869 [D loss: 1.016928] [G loss: 0.970727]\n",
      "1870 [D loss: 1.013345] [G loss: 0.971187]\n",
      "1871 [D loss: 1.003391] [G loss: 0.971437]\n",
      "1872 [D loss: 0.995784] [G loss: 0.987012]\n",
      "1873 [D loss: 1.001870] [G loss: 0.977525]\n",
      "1874 [D loss: 1.008365] [G loss: 0.983764]\n",
      "1875 [D loss: 1.015762] [G loss: 0.986080]\n",
      "1876 [D loss: 1.008899] [G loss: 0.995373]\n",
      "1877 [D loss: 1.005400] [G loss: 0.992717]\n",
      "1878 [D loss: 0.999643] [G loss: 0.980813]\n",
      "1879 [D loss: 1.023160] [G loss: 0.972378]\n",
      "1880 [D loss: 1.009630] [G loss: 1.007097]\n",
      "1881 [D loss: 0.998797] [G loss: 0.999350]\n",
      "1882 [D loss: 1.005582] [G loss: 0.989242]\n",
      "1883 [D loss: 1.010027] [G loss: 0.994931]\n",
      "1884 [D loss: 0.995761] [G loss: 0.992918]\n",
      "1885 [D loss: 1.007061] [G loss: 0.992726]\n",
      "1886 [D loss: 1.005036] [G loss: 0.976222]\n",
      "1887 [D loss: 1.018563] [G loss: 1.008627]\n",
      "1888 [D loss: 1.016396] [G loss: 0.988046]\n",
      "1889 [D loss: 1.005277] [G loss: 0.987767]\n",
      "1890 [D loss: 1.006409] [G loss: 0.983655]\n",
      "1891 [D loss: 1.004064] [G loss: 1.001268]\n",
      "1892 [D loss: 1.010597] [G loss: 0.999289]\n",
      "1893 [D loss: 0.996603] [G loss: 0.991772]\n",
      "1894 [D loss: 1.002513] [G loss: 0.974387]\n",
      "1895 [D loss: 0.999710] [G loss: 0.995109]\n",
      "1896 [D loss: 1.001173] [G loss: 0.981422]\n",
      "1897 [D loss: 1.015857] [G loss: 0.984402]\n",
      "1898 [D loss: 1.013964] [G loss: 0.986059]\n",
      "1899 [D loss: 1.004144] [G loss: 0.994390]\n",
      "1900 [D loss: 1.010397] [G loss: 0.973351]\n",
      "1901 [D loss: 1.000870] [G loss: 1.004505]\n",
      "1902 [D loss: 1.007170] [G loss: 1.003933]\n",
      "1903 [D loss: 1.016392] [G loss: 0.999480]\n",
      "1904 [D loss: 0.999710] [G loss: 0.999511]\n",
      "1905 [D loss: 1.003364] [G loss: 0.985288]\n",
      "1906 [D loss: 0.995937] [G loss: 0.984106]\n",
      "1907 [D loss: 1.012402] [G loss: 0.995725]\n",
      "1908 [D loss: 1.009112] [G loss: 0.978939]\n",
      "1909 [D loss: 0.999441] [G loss: 0.985646]\n",
      "1910 [D loss: 1.004372] [G loss: 0.990507]\n",
      "1911 [D loss: 1.006706] [G loss: 0.985968]\n",
      "1912 [D loss: 1.012933] [G loss: 0.981110]\n",
      "1913 [D loss: 1.012000] [G loss: 0.989527]\n",
      "1914 [D loss: 0.999314] [G loss: 0.982100]\n",
      "1915 [D loss: 0.999262] [G loss: 1.003317]\n",
      "1916 [D loss: 1.017436] [G loss: 0.968681]\n",
      "1917 [D loss: 1.000547] [G loss: 0.969882]\n",
      "1918 [D loss: 0.999708] [G loss: 0.985068]\n",
      "1919 [D loss: 1.011355] [G loss: 0.985157]\n",
      "1920 [D loss: 1.005861] [G loss: 0.983305]\n",
      "1921 [D loss: 1.005837] [G loss: 0.991813]\n",
      "1922 [D loss: 1.005396] [G loss: 1.003311]\n",
      "1923 [D loss: 1.004391] [G loss: 0.995349]\n",
      "1924 [D loss: 1.005534] [G loss: 0.982891]\n",
      "1925 [D loss: 1.010952] [G loss: 0.990790]\n",
      "1926 [D loss: 1.007467] [G loss: 0.976799]\n",
      "1927 [D loss: 1.006905] [G loss: 0.981137]\n",
      "1928 [D loss: 1.008268] [G loss: 0.985900]\n",
      "1929 [D loss: 1.001375] [G loss: 0.997321]\n",
      "1930 [D loss: 1.003485] [G loss: 0.994302]\n",
      "1931 [D loss: 1.000697] [G loss: 0.992567]\n",
      "1932 [D loss: 1.010153] [G loss: 0.995792]\n",
      "1933 [D loss: 1.003962] [G loss: 0.991645]\n",
      "1934 [D loss: 1.004767] [G loss: 0.976871]\n",
      "1935 [D loss: 1.020608] [G loss: 0.986266]\n",
      "1936 [D loss: 1.008730] [G loss: 0.997323]\n",
      "1937 [D loss: 1.007674] [G loss: 1.000083]\n",
      "1938 [D loss: 0.999556] [G loss: 0.990535]\n",
      "1939 [D loss: 0.994446] [G loss: 0.979698]\n",
      "1940 [D loss: 1.015611] [G loss: 0.978650]\n",
      "1941 [D loss: 1.004605] [G loss: 1.000147]\n",
      "1942 [D loss: 1.001012] [G loss: 0.992632]\n",
      "1943 [D loss: 1.004741] [G loss: 0.983701]\n",
      "1944 [D loss: 1.019047] [G loss: 0.984527]\n",
      "1945 [D loss: 1.012050] [G loss: 1.007673]\n",
      "1946 [D loss: 0.996392] [G loss: 1.002294]\n",
      "1947 [D loss: 1.018386] [G loss: 0.965624]\n",
      "1948 [D loss: 1.018714] [G loss: 0.998010]\n",
      "1949 [D loss: 1.018544] [G loss: 0.977594]\n",
      "1950 [D loss: 1.002597] [G loss: 0.979893]\n",
      "1951 [D loss: 1.017139] [G loss: 0.977502]\n",
      "1952 [D loss: 1.014770] [G loss: 0.993574]\n",
      "1953 [D loss: 1.012393] [G loss: 0.978526]\n",
      "1954 [D loss: 1.012714] [G loss: 0.985470]\n",
      "1955 [D loss: 0.997984] [G loss: 0.972525]\n",
      "1956 [D loss: 1.031116] [G loss: 0.984487]\n",
      "1957 [D loss: 1.002970] [G loss: 0.996739]\n",
      "1958 [D loss: 1.010876] [G loss: 0.999237]\n",
      "1959 [D loss: 1.009067] [G loss: 0.988510]\n",
      "1960 [D loss: 1.011900] [G loss: 1.003835]\n",
      "1961 [D loss: 1.011857] [G loss: 0.991647]\n",
      "1962 [D loss: 1.000763] [G loss: 0.989730]\n",
      "1963 [D loss: 1.012539] [G loss: 0.994005]\n",
      "1964 [D loss: 1.000590] [G loss: 0.991302]\n",
      "1965 [D loss: 0.999587] [G loss: 1.004770]\n",
      "1966 [D loss: 1.009336] [G loss: 0.985709]\n",
      "1967 [D loss: 1.006174] [G loss: 0.989622]\n",
      "1968 [D loss: 1.014141] [G loss: 0.980895]\n",
      "1969 [D loss: 0.996766] [G loss: 0.988579]\n",
      "1970 [D loss: 1.006302] [G loss: 0.996801]\n",
      "1971 [D loss: 0.997878] [G loss: 0.970378]\n",
      "1972 [D loss: 1.008698] [G loss: 0.971491]\n",
      "1973 [D loss: 1.008667] [G loss: 0.980984]\n",
      "1974 [D loss: 1.011062] [G loss: 0.983021]\n",
      "1975 [D loss: 1.000055] [G loss: 1.003034]\n",
      "1976 [D loss: 1.005900] [G loss: 1.001999]\n",
      "1977 [D loss: 1.007258] [G loss: 0.993898]\n",
      "1978 [D loss: 1.002807] [G loss: 0.996227]\n",
      "1979 [D loss: 1.010091] [G loss: 0.981916]\n",
      "1980 [D loss: 0.999148] [G loss: 0.998979]\n",
      "1981 [D loss: 1.005500] [G loss: 0.993662]\n",
      "1982 [D loss: 1.013825] [G loss: 0.987344]\n",
      "1983 [D loss: 1.007145] [G loss: 0.961194]\n",
      "1984 [D loss: 1.001494] [G loss: 1.006993]\n",
      "1985 [D loss: 1.013595] [G loss: 0.984415]\n",
      "1986 [D loss: 1.012854] [G loss: 0.975291]\n",
      "1987 [D loss: 1.021055] [G loss: 0.989040]\n",
      "1988 [D loss: 1.005190] [G loss: 0.969557]\n",
      "1989 [D loss: 0.997278] [G loss: 0.994714]\n",
      "1990 [D loss: 1.000282] [G loss: 0.985783]\n",
      "1991 [D loss: 1.010459] [G loss: 0.986339]\n",
      "1992 [D loss: 1.011404] [G loss: 0.981637]\n",
      "1993 [D loss: 0.997482] [G loss: 0.973111]\n",
      "1994 [D loss: 1.012870] [G loss: 0.977498]\n",
      "1995 [D loss: 1.007586] [G loss: 0.987432]\n",
      "1996 [D loss: 1.001622] [G loss: 0.984049]\n",
      "1997 [D loss: 1.003507] [G loss: 0.990873]\n",
      "1998 [D loss: 1.011554] [G loss: 0.982998]\n",
      "1999 [D loss: 1.020061] [G loss: 0.976613]\n",
      "2000 [D loss: 1.006200] [G loss: 0.988370]\n",
      "2001 [D loss: 1.012405] [G loss: 1.000256]\n",
      "2002 [D loss: 1.004472] [G loss: 0.988469]\n",
      "2003 [D loss: 1.008631] [G loss: 0.993142]\n",
      "2004 [D loss: 0.994160] [G loss: 0.976091]\n",
      "2005 [D loss: 1.018853] [G loss: 0.991248]\n",
      "2006 [D loss: 1.026936] [G loss: 0.975160]\n",
      "2007 [D loss: 1.004217] [G loss: 0.986894]\n",
      "2008 [D loss: 1.012381] [G loss: 0.980456]\n",
      "2009 [D loss: 1.007755] [G loss: 0.988870]\n",
      "2010 [D loss: 1.021214] [G loss: 0.989206]\n",
      "2011 [D loss: 0.995576] [G loss: 0.988763]\n",
      "2012 [D loss: 1.014406] [G loss: 0.978570]\n",
      "2013 [D loss: 1.002865] [G loss: 0.971031]\n",
      "2014 [D loss: 0.998904] [G loss: 0.987494]\n",
      "2015 [D loss: 1.010251] [G loss: 0.984934]\n",
      "2016 [D loss: 1.006352] [G loss: 0.983649]\n",
      "2017 [D loss: 1.005134] [G loss: 0.990158]\n",
      "2018 [D loss: 1.010417] [G loss: 0.967410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 [D loss: 1.006327] [G loss: 0.995968]\n",
      "2020 [D loss: 1.006373] [G loss: 0.975857]\n",
      "2021 [D loss: 1.009403] [G loss: 0.994991]\n",
      "2022 [D loss: 1.013763] [G loss: 0.979741]\n",
      "2023 [D loss: 1.011624] [G loss: 0.979489]\n",
      "2024 [D loss: 1.012403] [G loss: 0.968042]\n",
      "2025 [D loss: 1.006725] [G loss: 0.998435]\n",
      "2026 [D loss: 0.985299] [G loss: 0.997627]\n",
      "2027 [D loss: 1.002401] [G loss: 0.986883]\n",
      "2028 [D loss: 0.995494] [G loss: 1.001538]\n",
      "2029 [D loss: 1.006305] [G loss: 0.984148]\n",
      "2030 [D loss: 1.012825] [G loss: 0.991140]\n",
      "2031 [D loss: 1.011531] [G loss: 0.978449]\n",
      "2032 [D loss: 0.994020] [G loss: 0.992919]\n",
      "2033 [D loss: 1.000473] [G loss: 0.987515]\n",
      "2034 [D loss: 1.019115] [G loss: 0.981846]\n",
      "2035 [D loss: 1.009349] [G loss: 0.992137]\n",
      "2036 [D loss: 1.012928] [G loss: 0.994492]\n",
      "2037 [D loss: 0.996067] [G loss: 0.977380]\n",
      "2038 [D loss: 0.998850] [G loss: 0.981176]\n",
      "2039 [D loss: 1.012323] [G loss: 0.993195]\n",
      "2040 [D loss: 1.007200] [G loss: 0.987913]\n",
      "2041 [D loss: 1.008110] [G loss: 1.000441]\n",
      "2042 [D loss: 0.989897] [G loss: 0.984407]\n",
      "2043 [D loss: 1.001883] [G loss: 0.988446]\n",
      "2044 [D loss: 1.002745] [G loss: 0.969714]\n",
      "2045 [D loss: 1.005077] [G loss: 0.990792]\n",
      "2046 [D loss: 0.996639] [G loss: 0.992123]\n",
      "2047 [D loss: 1.004306] [G loss: 0.995850]\n",
      "2048 [D loss: 1.015758] [G loss: 0.986644]\n",
      "2049 [D loss: 0.998176] [G loss: 0.993016]\n",
      "2050 [D loss: 1.000464] [G loss: 0.997838]\n",
      "2051 [D loss: 1.010823] [G loss: 0.992004]\n",
      "2052 [D loss: 1.006722] [G loss: 1.009393]\n",
      "2053 [D loss: 1.013537] [G loss: 0.983279]\n",
      "2054 [D loss: 1.001863] [G loss: 1.001395]\n",
      "2055 [D loss: 1.012550] [G loss: 0.990300]\n",
      "2056 [D loss: 0.997126] [G loss: 1.004951]\n",
      "2057 [D loss: 1.015850] [G loss: 0.976421]\n",
      "2058 [D loss: 0.993789] [G loss: 0.992303]\n",
      "2059 [D loss: 1.030356] [G loss: 0.990912]\n",
      "2060 [D loss: 1.004521] [G loss: 0.991396]\n",
      "2061 [D loss: 1.021080] [G loss: 1.000729]\n",
      "2062 [D loss: 1.010142] [G loss: 0.993365]\n",
      "2063 [D loss: 1.009186] [G loss: 0.976972]\n",
      "2064 [D loss: 0.996251] [G loss: 0.989966]\n",
      "2065 [D loss: 0.999131] [G loss: 1.010765]\n",
      "2066 [D loss: 1.003247] [G loss: 0.986641]\n",
      "2067 [D loss: 1.003749] [G loss: 0.994534]\n",
      "2068 [D loss: 1.003227] [G loss: 0.995784]\n",
      "2069 [D loss: 1.009866] [G loss: 0.991390]\n",
      "2070 [D loss: 0.999878] [G loss: 0.991197]\n",
      "2071 [D loss: 1.005893] [G loss: 0.999585]\n",
      "2072 [D loss: 1.011612] [G loss: 0.979442]\n",
      "2073 [D loss: 1.016071] [G loss: 0.993780]\n",
      "2074 [D loss: 1.006245] [G loss: 0.993557]\n",
      "2075 [D loss: 1.010171] [G loss: 0.981800]\n",
      "2076 [D loss: 1.006388] [G loss: 0.987141]\n",
      "2077 [D loss: 1.001684] [G loss: 1.005114]\n",
      "2078 [D loss: 1.001977] [G loss: 0.993341]\n",
      "2079 [D loss: 1.010797] [G loss: 0.991023]\n",
      "2080 [D loss: 1.012627] [G loss: 0.985216]\n",
      "2081 [D loss: 1.004607] [G loss: 0.986600]\n",
      "2082 [D loss: 1.002383] [G loss: 0.987272]\n",
      "2083 [D loss: 1.011065] [G loss: 1.007507]\n",
      "2084 [D loss: 1.005029] [G loss: 0.990931]\n",
      "2085 [D loss: 1.006102] [G loss: 1.002467]\n",
      "2086 [D loss: 1.002682] [G loss: 0.996148]\n",
      "2087 [D loss: 1.013259] [G loss: 0.998846]\n",
      "2088 [D loss: 0.985191] [G loss: 0.987842]\n",
      "2089 [D loss: 1.004767] [G loss: 1.006224]\n",
      "2090 [D loss: 1.000257] [G loss: 0.992297]\n",
      "2091 [D loss: 1.015406] [G loss: 0.980873]\n",
      "2092 [D loss: 1.002935] [G loss: 0.997358]\n",
      "2093 [D loss: 0.999398] [G loss: 0.979764]\n",
      "2094 [D loss: 0.991926] [G loss: 0.987880]\n",
      "2095 [D loss: 1.012671] [G loss: 0.982857]\n",
      "2096 [D loss: 1.007128] [G loss: 0.985687]\n",
      "2097 [D loss: 1.011839] [G loss: 0.991800]\n",
      "2098 [D loss: 1.017210] [G loss: 0.981306]\n",
      "2099 [D loss: 1.021535] [G loss: 0.991379]\n",
      "2100 [D loss: 1.002164] [G loss: 0.998877]\n",
      "2101 [D loss: 1.000241] [G loss: 0.998984]\n",
      "2102 [D loss: 1.007724] [G loss: 0.968183]\n",
      "2103 [D loss: 1.011077] [G loss: 0.980689]\n",
      "2104 [D loss: 1.011987] [G loss: 0.973910]\n",
      "2105 [D loss: 1.004664] [G loss: 0.971418]\n",
      "2106 [D loss: 1.018063] [G loss: 1.000560]\n",
      "2107 [D loss: 1.005052] [G loss: 0.986169]\n",
      "2108 [D loss: 1.007557] [G loss: 0.980612]\n",
      "2109 [D loss: 1.008283] [G loss: 0.994560]\n",
      "2110 [D loss: 1.007877] [G loss: 0.978361]\n",
      "2111 [D loss: 0.989783] [G loss: 0.999356]\n",
      "2112 [D loss: 0.987472] [G loss: 0.975493]\n",
      "2113 [D loss: 1.009783] [G loss: 0.982809]\n",
      "2114 [D loss: 1.002847] [G loss: 1.000973]\n",
      "2115 [D loss: 1.012721] [G loss: 0.968058]\n",
      "2116 [D loss: 1.005040] [G loss: 0.999745]\n",
      "2117 [D loss: 1.011749] [G loss: 0.956603]\n",
      "2118 [D loss: 1.002095] [G loss: 0.975909]\n",
      "2119 [D loss: 1.012461] [G loss: 0.973248]\n",
      "2120 [D loss: 1.000516] [G loss: 0.989503]\n",
      "2121 [D loss: 1.012717] [G loss: 0.992452]\n",
      "2122 [D loss: 0.995585] [G loss: 0.979493]\n",
      "2123 [D loss: 0.997355] [G loss: 0.990917]\n",
      "2124 [D loss: 1.005307] [G loss: 0.982412]\n",
      "2125 [D loss: 1.006536] [G loss: 0.987326]\n",
      "2126 [D loss: 1.001037] [G loss: 0.966194]\n",
      "2127 [D loss: 1.006421] [G loss: 0.987462]\n",
      "2128 [D loss: 0.995113] [G loss: 0.964599]\n",
      "2129 [D loss: 1.009628] [G loss: 0.978513]\n",
      "2130 [D loss: 0.999858] [G loss: 0.968821]\n",
      "2131 [D loss: 0.999012] [G loss: 0.975616]\n",
      "2132 [D loss: 1.014183] [G loss: 0.976478]\n",
      "2133 [D loss: 1.005864] [G loss: 0.979870]\n",
      "2134 [D loss: 1.011019] [G loss: 0.972730]\n",
      "2135 [D loss: 1.000258] [G loss: 0.992692]\n",
      "2136 [D loss: 1.002205] [G loss: 0.984167]\n",
      "2137 [D loss: 1.008207] [G loss: 0.989378]\n",
      "2138 [D loss: 1.013011] [G loss: 0.983647]\n",
      "2139 [D loss: 1.006758] [G loss: 0.977831]\n",
      "2140 [D loss: 1.005638] [G loss: 0.957583]\n",
      "2141 [D loss: 1.005677] [G loss: 0.991549]\n",
      "2142 [D loss: 0.996189] [G loss: 0.994737]\n",
      "2143 [D loss: 0.992357] [G loss: 0.995341]\n",
      "2144 [D loss: 1.008255] [G loss: 0.975495]\n",
      "2145 [D loss: 1.006245] [G loss: 0.975316]\n",
      "2146 [D loss: 1.009973] [G loss: 0.981266]\n",
      "2147 [D loss: 0.992477] [G loss: 0.999281]\n",
      "2148 [D loss: 1.016876] [G loss: 0.980900]\n",
      "2149 [D loss: 0.999818] [G loss: 0.971872]\n",
      "2150 [D loss: 0.998709] [G loss: 0.993985]\n",
      "2151 [D loss: 1.006836] [G loss: 0.987559]\n",
      "2152 [D loss: 1.005291] [G loss: 0.991191]\n",
      "2153 [D loss: 1.002162] [G loss: 0.978434]\n",
      "2154 [D loss: 1.007815] [G loss: 0.975056]\n",
      "2155 [D loss: 1.002502] [G loss: 0.974947]\n",
      "2156 [D loss: 1.007007] [G loss: 0.984900]\n",
      "2157 [D loss: 1.014795] [G loss: 0.974061]\n",
      "2158 [D loss: 1.000683] [G loss: 0.967951]\n",
      "2159 [D loss: 1.014970] [G loss: 0.979020]\n",
      "2160 [D loss: 1.004608] [G loss: 0.978506]\n",
      "2161 [D loss: 1.002564] [G loss: 0.991434]\n",
      "2162 [D loss: 1.017592] [G loss: 0.987630]\n",
      "2163 [D loss: 1.014649] [G loss: 0.966217]\n",
      "2164 [D loss: 1.010776] [G loss: 0.991421]\n",
      "2165 [D loss: 1.009378] [G loss: 0.991389]\n",
      "2166 [D loss: 1.005747] [G loss: 0.981620]\n",
      "2167 [D loss: 1.001048] [G loss: 0.990872]\n",
      "2168 [D loss: 1.012189] [G loss: 1.000314]\n",
      "2169 [D loss: 1.007059] [G loss: 0.978201]\n",
      "2170 [D loss: 0.996444] [G loss: 1.005286]\n",
      "2171 [D loss: 0.998301] [G loss: 0.994994]\n",
      "2172 [D loss: 1.003455] [G loss: 1.000859]\n",
      "2173 [D loss: 1.002641] [G loss: 0.987788]\n",
      "2174 [D loss: 1.004200] [G loss: 0.997913]\n",
      "2175 [D loss: 1.006440] [G loss: 0.981098]\n",
      "2176 [D loss: 1.005149] [G loss: 0.992475]\n",
      "2177 [D loss: 0.999174] [G loss: 0.991975]\n",
      "2178 [D loss: 1.008143] [G loss: 0.992423]\n",
      "2179 [D loss: 1.001590] [G loss: 0.978459]\n",
      "2180 [D loss: 0.998850] [G loss: 0.971241]\n",
      "2181 [D loss: 1.011812] [G loss: 0.983578]\n",
      "2182 [D loss: 0.996873] [G loss: 0.998332]\n",
      "2183 [D loss: 1.012068] [G loss: 0.990114]\n",
      "2184 [D loss: 1.013803] [G loss: 0.973592]\n",
      "2185 [D loss: 1.008267] [G loss: 1.004519]\n",
      "2186 [D loss: 1.004783] [G loss: 0.983442]\n",
      "2187 [D loss: 0.994201] [G loss: 1.006745]\n",
      "2188 [D loss: 1.020232] [G loss: 0.991308]\n",
      "2189 [D loss: 1.000982] [G loss: 0.992918]\n",
      "2190 [D loss: 0.996908] [G loss: 1.002086]\n",
      "2191 [D loss: 1.015873] [G loss: 0.985878]\n",
      "2192 [D loss: 1.018043] [G loss: 0.990732]\n",
      "2193 [D loss: 1.016737] [G loss: 0.997015]\n",
      "2194 [D loss: 1.001981] [G loss: 0.998875]\n",
      "2195 [D loss: 1.005864] [G loss: 0.984118]\n",
      "2196 [D loss: 0.997774] [G loss: 0.988855]\n",
      "2197 [D loss: 1.001741] [G loss: 1.011004]\n",
      "2198 [D loss: 1.004509] [G loss: 0.988094]\n",
      "2199 [D loss: 1.004593] [G loss: 1.006102]\n",
      "2200 [D loss: 1.001591] [G loss: 1.008225]\n",
      "2201 [D loss: 1.003043] [G loss: 1.003770]\n",
      "2202 [D loss: 1.002607] [G loss: 0.990824]\n",
      "2203 [D loss: 0.994936] [G loss: 1.017786]\n",
      "2204 [D loss: 0.999779] [G loss: 1.000633]\n",
      "2205 [D loss: 1.001750] [G loss: 0.995543]\n",
      "2206 [D loss: 1.005712] [G loss: 0.992785]\n",
      "2207 [D loss: 1.012026] [G loss: 0.991516]\n",
      "2208 [D loss: 1.003377] [G loss: 0.987450]\n",
      "2209 [D loss: 1.007965] [G loss: 1.001454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210 [D loss: 1.009275] [G loss: 0.993544]\n",
      "2211 [D loss: 1.000414] [G loss: 0.996322]\n",
      "2212 [D loss: 1.007162] [G loss: 0.991092]\n",
      "2213 [D loss: 1.018084] [G loss: 0.981508]\n",
      "2214 [D loss: 0.998022] [G loss: 0.998375]\n",
      "2215 [D loss: 1.015295] [G loss: 1.000708]\n",
      "2216 [D loss: 0.997511] [G loss: 0.981539]\n",
      "2217 [D loss: 1.006610] [G loss: 1.008760]\n",
      "2218 [D loss: 0.998633] [G loss: 0.977291]\n",
      "2219 [D loss: 1.009754] [G loss: 0.989254]\n",
      "2220 [D loss: 1.002869] [G loss: 0.986280]\n",
      "2221 [D loss: 1.010675] [G loss: 0.984920]\n",
      "2222 [D loss: 1.007072] [G loss: 1.001160]\n",
      "2223 [D loss: 1.016963] [G loss: 0.978554]\n",
      "2224 [D loss: 1.002832] [G loss: 0.978817]\n",
      "2225 [D loss: 1.017533] [G loss: 0.991398]\n",
      "2226 [D loss: 1.013774] [G loss: 0.996954]\n",
      "2227 [D loss: 0.998158] [G loss: 0.981264]\n",
      "2228 [D loss: 1.011904] [G loss: 0.983738]\n",
      "2229 [D loss: 0.998540] [G loss: 0.983292]\n",
      "2230 [D loss: 1.005550] [G loss: 0.992366]\n",
      "2231 [D loss: 1.004094] [G loss: 0.965838]\n",
      "2232 [D loss: 0.998820] [G loss: 0.985099]\n",
      "2233 [D loss: 1.010584] [G loss: 0.976297]\n",
      "2234 [D loss: 1.000387] [G loss: 0.969275]\n",
      "2235 [D loss: 1.010563] [G loss: 0.986033]\n",
      "2236 [D loss: 1.015864] [G loss: 0.985149]\n",
      "2237 [D loss: 1.008383] [G loss: 0.993393]\n",
      "2238 [D loss: 1.014644] [G loss: 0.962631]\n",
      "2239 [D loss: 1.005523] [G loss: 0.987995]\n",
      "2240 [D loss: 1.001896] [G loss: 1.000246]\n",
      "2241 [D loss: 1.014620] [G loss: 0.985412]\n",
      "2242 [D loss: 1.003803] [G loss: 0.972908]\n",
      "2243 [D loss: 0.996826] [G loss: 0.966590]\n",
      "2244 [D loss: 0.999829] [G loss: 0.988828]\n",
      "2245 [D loss: 0.998813] [G loss: 0.996822]\n",
      "2246 [D loss: 0.994070] [G loss: 0.976321]\n",
      "2247 [D loss: 1.008925] [G loss: 0.989570]\n",
      "2248 [D loss: 1.004629] [G loss: 0.979014]\n",
      "2249 [D loss: 1.014377] [G loss: 0.966816]\n",
      "2250 [D loss: 1.018813] [G loss: 0.967828]\n",
      "2251 [D loss: 0.999584] [G loss: 0.987881]\n",
      "2252 [D loss: 1.008432] [G loss: 0.975844]\n",
      "2253 [D loss: 1.016840] [G loss: 0.964383]\n",
      "2254 [D loss: 0.995555] [G loss: 0.978052]\n",
      "2255 [D loss: 1.003346] [G loss: 0.993165]\n",
      "2256 [D loss: 1.015835] [G loss: 0.975611]\n",
      "2257 [D loss: 1.002120] [G loss: 0.967363]\n",
      "2258 [D loss: 1.009208] [G loss: 0.979426]\n",
      "2259 [D loss: 0.996361] [G loss: 0.991062]\n",
      "2260 [D loss: 1.004639] [G loss: 0.982462]\n",
      "2261 [D loss: 1.003904] [G loss: 0.972279]\n",
      "2262 [D loss: 1.011796] [G loss: 0.988761]\n",
      "2263 [D loss: 1.012599] [G loss: 0.968262]\n",
      "2264 [D loss: 1.015811] [G loss: 0.963246]\n",
      "2265 [D loss: 1.009208] [G loss: 0.971555]\n",
      "2266 [D loss: 0.990521] [G loss: 0.982298]\n",
      "2267 [D loss: 0.994256] [G loss: 0.982472]\n",
      "2268 [D loss: 1.009863] [G loss: 0.974689]\n",
      "2269 [D loss: 1.007398] [G loss: 0.974582]\n",
      "2270 [D loss: 1.009958] [G loss: 0.969739]\n",
      "2271 [D loss: 1.003330] [G loss: 0.981540]\n",
      "2272 [D loss: 1.001810] [G loss: 0.981680]\n",
      "2273 [D loss: 1.004453] [G loss: 0.977060]\n",
      "2274 [D loss: 1.000969] [G loss: 0.987388]\n",
      "2275 [D loss: 1.005673] [G loss: 0.967472]\n",
      "2276 [D loss: 1.017713] [G loss: 0.970219]\n",
      "2277 [D loss: 1.004016] [G loss: 0.968561]\n",
      "2278 [D loss: 1.003643] [G loss: 0.997957]\n",
      "2279 [D loss: 1.004588] [G loss: 0.960943]\n",
      "2280 [D loss: 1.007473] [G loss: 0.970675]\n",
      "2281 [D loss: 0.999459] [G loss: 0.982009]\n",
      "2282 [D loss: 1.007298] [G loss: 0.983426]\n",
      "2283 [D loss: 1.005851] [G loss: 0.979488]\n",
      "2284 [D loss: 0.997144] [G loss: 0.972241]\n",
      "2285 [D loss: 1.005638] [G loss: 0.983738]\n",
      "2286 [D loss: 1.014231] [G loss: 0.976674]\n",
      "2287 [D loss: 1.008352] [G loss: 0.983712]\n",
      "2288 [D loss: 1.004247] [G loss: 0.981591]\n",
      "2289 [D loss: 1.003501] [G loss: 0.994613]\n",
      "2290 [D loss: 1.001423] [G loss: 0.976688]\n",
      "2291 [D loss: 1.015123] [G loss: 0.977144]\n",
      "2292 [D loss: 1.002585] [G loss: 0.971169]\n",
      "2293 [D loss: 0.997313] [G loss: 0.967176]\n",
      "2294 [D loss: 1.010604] [G loss: 0.995481]\n",
      "2295 [D loss: 1.019814] [G loss: 0.973752]\n",
      "2296 [D loss: 1.007160] [G loss: 0.985878]\n",
      "2297 [D loss: 1.001616] [G loss: 0.975948]\n",
      "2298 [D loss: 1.002253] [G loss: 0.983475]\n",
      "2299 [D loss: 1.007787] [G loss: 0.983540]\n",
      "2300 [D loss: 0.997402] [G loss: 0.997987]\n",
      "2301 [D loss: 1.004225] [G loss: 0.983792]\n",
      "2302 [D loss: 1.014325] [G loss: 0.993163]\n",
      "2303 [D loss: 1.000058] [G loss: 0.983491]\n",
      "2304 [D loss: 1.003651] [G loss: 0.988246]\n",
      "2305 [D loss: 1.002674] [G loss: 0.994702]\n",
      "2306 [D loss: 1.004991] [G loss: 0.992072]\n",
      "2307 [D loss: 0.991419] [G loss: 0.990274]\n",
      "2308 [D loss: 1.007422] [G loss: 0.990228]\n",
      "2309 [D loss: 0.993290] [G loss: 0.995044]\n",
      "2310 [D loss: 1.005488] [G loss: 1.004171]\n",
      "2311 [D loss: 1.012394] [G loss: 0.989573]\n",
      "2312 [D loss: 1.008409] [G loss: 0.981524]\n",
      "2313 [D loss: 1.004112] [G loss: 0.998348]\n",
      "2314 [D loss: 1.002064] [G loss: 0.998481]\n",
      "2315 [D loss: 0.999934] [G loss: 0.990154]\n",
      "2316 [D loss: 1.010166] [G loss: 1.006402]\n",
      "2317 [D loss: 0.996374] [G loss: 1.001765]\n",
      "2318 [D loss: 1.007754] [G loss: 0.998919]\n",
      "2319 [D loss: 1.002585] [G loss: 0.989056]\n",
      "2320 [D loss: 1.009655] [G loss: 0.994373]\n",
      "2321 [D loss: 1.000936] [G loss: 0.989926]\n",
      "2322 [D loss: 1.003269] [G loss: 1.004884]\n",
      "2323 [D loss: 1.001028] [G loss: 0.981361]\n",
      "2324 [D loss: 0.989306] [G loss: 0.998152]\n",
      "2325 [D loss: 1.005715] [G loss: 0.996643]\n",
      "2326 [D loss: 0.997172] [G loss: 0.990686]\n",
      "2327 [D loss: 1.001681] [G loss: 0.995193]\n",
      "2328 [D loss: 1.005138] [G loss: 0.971064]\n",
      "2329 [D loss: 1.000718] [G loss: 0.979375]\n",
      "2330 [D loss: 1.002932] [G loss: 0.980683]\n",
      "2331 [D loss: 1.009212] [G loss: 0.979297]\n",
      "2332 [D loss: 1.008914] [G loss: 0.997655]\n",
      "2333 [D loss: 1.021751] [G loss: 0.986095]\n",
      "2334 [D loss: 1.010718] [G loss: 0.977433]\n",
      "2335 [D loss: 1.004222] [G loss: 0.987032]\n",
      "2336 [D loss: 1.005867] [G loss: 0.996613]\n",
      "2337 [D loss: 0.992937] [G loss: 1.001868]\n",
      "2338 [D loss: 1.007952] [G loss: 0.979534]\n",
      "2339 [D loss: 1.001007] [G loss: 0.985668]\n",
      "2340 [D loss: 1.014453] [G loss: 0.980091]\n",
      "2341 [D loss: 1.004579] [G loss: 0.996509]\n",
      "2342 [D loss: 1.009413] [G loss: 1.002311]\n",
      "2343 [D loss: 1.007855] [G loss: 0.981739]\n",
      "2344 [D loss: 1.009101] [G loss: 0.995860]\n",
      "2345 [D loss: 0.997015] [G loss: 1.013574]\n",
      "2346 [D loss: 0.992511] [G loss: 1.006363]\n",
      "2347 [D loss: 0.990777] [G loss: 0.982329]\n",
      "2348 [D loss: 1.000424] [G loss: 0.995568]\n",
      "2349 [D loss: 1.015889] [G loss: 0.997488]\n",
      "2350 [D loss: 1.006208] [G loss: 0.999495]\n",
      "2351 [D loss: 1.011310] [G loss: 0.993100]\n",
      "2352 [D loss: 1.003773] [G loss: 0.976977]\n",
      "2353 [D loss: 1.002355] [G loss: 0.973025]\n",
      "2354 [D loss: 1.004373] [G loss: 1.000251]\n",
      "2355 [D loss: 1.006359] [G loss: 0.992039]\n",
      "2356 [D loss: 1.001519] [G loss: 0.986118]\n",
      "2357 [D loss: 1.008328] [G loss: 0.987473]\n",
      "2358 [D loss: 1.002720] [G loss: 0.967135]\n",
      "2359 [D loss: 0.998321] [G loss: 1.000857]\n",
      "2360 [D loss: 1.011720] [G loss: 0.982907]\n",
      "2361 [D loss: 1.011043] [G loss: 0.980638]\n",
      "2362 [D loss: 1.006037] [G loss: 0.991249]\n",
      "2363 [D loss: 1.010065] [G loss: 0.988641]\n",
      "2364 [D loss: 1.007610] [G loss: 0.986461]\n",
      "2365 [D loss: 1.009991] [G loss: 0.979643]\n",
      "2366 [D loss: 1.001690] [G loss: 0.984644]\n",
      "2367 [D loss: 1.003608] [G loss: 0.993440]\n",
      "2368 [D loss: 1.017236] [G loss: 0.983339]\n",
      "2369 [D loss: 0.999234] [G loss: 0.976575]\n",
      "2370 [D loss: 1.005042] [G loss: 0.998820]\n",
      "2371 [D loss: 0.996737] [G loss: 0.983525]\n",
      "2372 [D loss: 1.007889] [G loss: 0.979945]\n",
      "2373 [D loss: 1.002983] [G loss: 0.995641]\n",
      "2374 [D loss: 1.000510] [G loss: 0.998905]\n",
      "2375 [D loss: 0.997768] [G loss: 0.978024]\n",
      "2376 [D loss: 1.002037] [G loss: 0.994583]\n",
      "2377 [D loss: 1.003164] [G loss: 0.977142]\n",
      "2378 [D loss: 1.014514] [G loss: 0.985960]\n",
      "2379 [D loss: 0.994292] [G loss: 0.998061]\n",
      "2380 [D loss: 0.999076] [G loss: 0.997782]\n",
      "2381 [D loss: 1.010588] [G loss: 0.987424]\n",
      "2382 [D loss: 1.013344] [G loss: 0.965358]\n",
      "2383 [D loss: 0.995869] [G loss: 0.979245]\n",
      "2384 [D loss: 1.005159] [G loss: 0.983355]\n",
      "2385 [D loss: 1.009538] [G loss: 0.972587]\n",
      "2386 [D loss: 0.997941] [G loss: 0.991178]\n",
      "2387 [D loss: 1.000806] [G loss: 0.988234]\n",
      "2388 [D loss: 1.006422] [G loss: 0.981539]\n",
      "2389 [D loss: 1.015185] [G loss: 0.979942]\n",
      "2390 [D loss: 1.010880] [G loss: 0.992303]\n",
      "2391 [D loss: 1.012559] [G loss: 0.983727]\n",
      "2392 [D loss: 1.001099] [G loss: 0.979550]\n",
      "2393 [D loss: 1.002740] [G loss: 0.982985]\n",
      "2394 [D loss: 1.008627] [G loss: 0.984857]\n",
      "2395 [D loss: 1.002253] [G loss: 0.991652]\n",
      "2396 [D loss: 1.014287] [G loss: 0.991565]\n",
      "2397 [D loss: 1.015723] [G loss: 0.990990]\n",
      "2398 [D loss: 0.998035] [G loss: 0.983280]\n",
      "2399 [D loss: 1.006209] [G loss: 0.980969]\n",
      "2400 [D loss: 1.006313] [G loss: 0.985623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2401 [D loss: 1.004835] [G loss: 0.992945]\n",
      "2402 [D loss: 1.009538] [G loss: 0.978126]\n",
      "2403 [D loss: 0.993204] [G loss: 0.999441]\n",
      "2404 [D loss: 0.992890] [G loss: 1.003590]\n",
      "2405 [D loss: 1.014922] [G loss: 0.992062]\n",
      "2406 [D loss: 1.005760] [G loss: 0.982801]\n",
      "2407 [D loss: 1.013552] [G loss: 0.983326]\n",
      "2408 [D loss: 1.000125] [G loss: 0.993803]\n",
      "2409 [D loss: 1.006815] [G loss: 1.007870]\n",
      "2410 [D loss: 0.989744] [G loss: 0.982988]\n",
      "2411 [D loss: 1.005185] [G loss: 0.978124]\n",
      "2412 [D loss: 1.002624] [G loss: 1.010149]\n",
      "2413 [D loss: 1.004876] [G loss: 0.983046]\n",
      "2414 [D loss: 1.013084] [G loss: 0.982600]\n",
      "2415 [D loss: 0.996316] [G loss: 0.987199]\n",
      "2416 [D loss: 1.009923] [G loss: 0.987807]\n",
      "2417 [D loss: 1.005819] [G loss: 0.986901]\n",
      "2418 [D loss: 0.996489] [G loss: 0.987033]\n",
      "2419 [D loss: 1.003457] [G loss: 0.973757]\n",
      "2420 [D loss: 1.024656] [G loss: 0.994315]\n",
      "2421 [D loss: 1.004745] [G loss: 0.978548]\n",
      "2422 [D loss: 1.000911] [G loss: 0.983563]\n",
      "2423 [D loss: 1.018604] [G loss: 0.976191]\n",
      "2424 [D loss: 1.009558] [G loss: 0.978516]\n",
      "2425 [D loss: 1.014319] [G loss: 0.973732]\n",
      "2426 [D loss: 0.991571] [G loss: 0.996420]\n",
      "2427 [D loss: 1.012141] [G loss: 0.968327]\n",
      "2428 [D loss: 1.002441] [G loss: 0.974726]\n",
      "2429 [D loss: 1.000710] [G loss: 0.986699]\n",
      "2430 [D loss: 1.010540] [G loss: 0.975810]\n",
      "2431 [D loss: 1.022660] [G loss: 0.976038]\n",
      "2432 [D loss: 1.008587] [G loss: 0.983058]\n",
      "2433 [D loss: 1.027931] [G loss: 0.969408]\n",
      "2434 [D loss: 0.997757] [G loss: 0.972237]\n",
      "2435 [D loss: 1.002167] [G loss: 0.977650]\n",
      "2436 [D loss: 0.998831] [G loss: 0.975720]\n",
      "2437 [D loss: 0.997598] [G loss: 0.978025]\n",
      "2438 [D loss: 1.007804] [G loss: 0.969646]\n",
      "2439 [D loss: 1.005671] [G loss: 0.996615]\n",
      "2440 [D loss: 1.015275] [G loss: 0.990992]\n",
      "2441 [D loss: 1.007744] [G loss: 0.988616]\n",
      "2442 [D loss: 1.002604] [G loss: 0.973049]\n",
      "2443 [D loss: 1.008522] [G loss: 0.974333]\n",
      "2444 [D loss: 1.008384] [G loss: 0.973032]\n",
      "2445 [D loss: 1.005437] [G loss: 0.965561]\n",
      "2446 [D loss: 1.006848] [G loss: 0.992821]\n",
      "2447 [D loss: 0.996482] [G loss: 0.994243]\n",
      "2448 [D loss: 1.004917] [G loss: 0.983212]\n",
      "2449 [D loss: 1.011451] [G loss: 0.969026]\n",
      "2450 [D loss: 1.001114] [G loss: 0.988594]\n",
      "2451 [D loss: 0.999402] [G loss: 0.990032]\n",
      "2452 [D loss: 0.999371] [G loss: 0.963162]\n",
      "2453 [D loss: 1.007991] [G loss: 0.987330]\n",
      "2454 [D loss: 1.006466] [G loss: 0.989284]\n",
      "2455 [D loss: 1.014485] [G loss: 0.984004]\n",
      "2456 [D loss: 1.011961] [G loss: 0.980367]\n",
      "2457 [D loss: 1.005391] [G loss: 0.988743]\n",
      "2458 [D loss: 0.999656] [G loss: 0.976546]\n",
      "2459 [D loss: 0.999579] [G loss: 0.995416]\n",
      "2460 [D loss: 1.003157] [G loss: 0.991646]\n",
      "2461 [D loss: 0.998728] [G loss: 0.997372]\n",
      "2462 [D loss: 0.997386] [G loss: 1.004994]\n",
      "2463 [D loss: 1.008524] [G loss: 0.967695]\n",
      "2464 [D loss: 1.000041] [G loss: 0.980752]\n",
      "2465 [D loss: 1.015671] [G loss: 0.975197]\n",
      "2466 [D loss: 1.001258] [G loss: 0.986613]\n",
      "2467 [D loss: 1.016750] [G loss: 0.977084]\n",
      "2468 [D loss: 1.011979] [G loss: 0.981776]\n",
      "2469 [D loss: 1.016492] [G loss: 0.988443]\n",
      "2470 [D loss: 1.006183] [G loss: 0.978252]\n",
      "2471 [D loss: 1.014352] [G loss: 0.984841]\n",
      "2472 [D loss: 1.002502] [G loss: 0.992762]\n",
      "2473 [D loss: 0.994382] [G loss: 0.999736]\n",
      "2474 [D loss: 1.002111] [G loss: 0.997616]\n",
      "2475 [D loss: 1.003558] [G loss: 0.998877]\n",
      "2476 [D loss: 1.011170] [G loss: 0.978363]\n",
      "2477 [D loss: 0.999659] [G loss: 0.994975]\n",
      "2478 [D loss: 1.004216] [G loss: 0.992733]\n",
      "2479 [D loss: 0.998142] [G loss: 0.988513]\n",
      "2480 [D loss: 1.008379] [G loss: 0.990505]\n",
      "2481 [D loss: 1.003992] [G loss: 0.986365]\n",
      "2482 [D loss: 1.001486] [G loss: 0.985116]\n",
      "2483 [D loss: 0.999047] [G loss: 0.994577]\n",
      "2484 [D loss: 1.009413] [G loss: 0.981144]\n",
      "2485 [D loss: 1.013928] [G loss: 0.976768]\n",
      "2486 [D loss: 1.014465] [G loss: 0.995730]\n",
      "2487 [D loss: 1.000766] [G loss: 0.984401]\n",
      "2488 [D loss: 1.003195] [G loss: 0.972590]\n",
      "2489 [D loss: 1.003189] [G loss: 0.998207]\n",
      "2490 [D loss: 1.010741] [G loss: 0.983708]\n",
      "2491 [D loss: 1.000254] [G loss: 0.976918]\n",
      "2492 [D loss: 1.011021] [G loss: 0.968824]\n",
      "2493 [D loss: 1.012974] [G loss: 0.995257]\n",
      "2494 [D loss: 1.006242] [G loss: 0.988162]\n",
      "2495 [D loss: 1.001343] [G loss: 0.994144]\n",
      "2496 [D loss: 1.001067] [G loss: 0.984713]\n",
      "2497 [D loss: 1.002953] [G loss: 0.974677]\n",
      "2498 [D loss: 1.010733] [G loss: 0.981226]\n",
      "2499 [D loss: 0.999486] [G loss: 0.977483]\n",
      "2500 [D loss: 1.007339] [G loss: 0.982133]\n",
      "2501 [D loss: 1.007597] [G loss: 0.958808]\n",
      "2502 [D loss: 1.004859] [G loss: 0.991050]\n",
      "2503 [D loss: 1.010492] [G loss: 0.971998]\n",
      "2504 [D loss: 1.012324] [G loss: 0.985017]\n",
      "2505 [D loss: 1.001713] [G loss: 0.986147]\n",
      "2506 [D loss: 1.002282] [G loss: 0.987079]\n",
      "2507 [D loss: 1.001386] [G loss: 0.989751]\n",
      "2508 [D loss: 1.003357] [G loss: 0.981311]\n",
      "2509 [D loss: 1.003119] [G loss: 0.978249]\n",
      "2510 [D loss: 1.001397] [G loss: 0.980232]\n",
      "2511 [D loss: 1.004200] [G loss: 0.969948]\n",
      "2512 [D loss: 1.003525] [G loss: 0.981403]\n",
      "2513 [D loss: 1.006998] [G loss: 0.982249]\n",
      "2514 [D loss: 1.003516] [G loss: 0.985246]\n",
      "2515 [D loss: 1.008843] [G loss: 0.964801]\n",
      "2516 [D loss: 1.005950] [G loss: 0.968352]\n",
      "2517 [D loss: 1.001270] [G loss: 0.985690]\n",
      "2518 [D loss: 1.007288] [G loss: 0.995641]\n",
      "2519 [D loss: 1.003709] [G loss: 0.982411]\n",
      "2520 [D loss: 1.006910] [G loss: 0.977677]\n",
      "2521 [D loss: 1.009922] [G loss: 0.979176]\n",
      "2522 [D loss: 0.997171] [G loss: 0.999827]\n",
      "2523 [D loss: 1.007137] [G loss: 0.984797]\n",
      "2524 [D loss: 0.999232] [G loss: 0.991252]\n",
      "2525 [D loss: 0.993825] [G loss: 1.006568]\n",
      "2526 [D loss: 1.000458] [G loss: 0.987007]\n",
      "2527 [D loss: 1.006052] [G loss: 0.967111]\n",
      "2528 [D loss: 1.010841] [G loss: 0.983590]\n",
      "2529 [D loss: 0.999582] [G loss: 0.981265]\n",
      "2530 [D loss: 0.992895] [G loss: 0.999417]\n",
      "2531 [D loss: 1.013120] [G loss: 0.984205]\n",
      "2532 [D loss: 0.998840] [G loss: 1.001494]\n",
      "2533 [D loss: 1.002483] [G loss: 0.978665]\n",
      "2534 [D loss: 0.997259] [G loss: 0.984611]\n",
      "2535 [D loss: 1.007152] [G loss: 0.991230]\n",
      "2536 [D loss: 0.993910] [G loss: 0.973418]\n",
      "2537 [D loss: 1.008310] [G loss: 0.989173]\n",
      "2538 [D loss: 1.000978] [G loss: 0.989648]\n",
      "2539 [D loss: 1.011179] [G loss: 0.979624]\n",
      "2540 [D loss: 1.009163] [G loss: 0.987886]\n",
      "2541 [D loss: 1.006960] [G loss: 0.973514]\n",
      "2542 [D loss: 1.014507] [G loss: 0.966439]\n",
      "2543 [D loss: 0.995627] [G loss: 1.003321]\n",
      "2544 [D loss: 0.998444] [G loss: 0.975524]\n",
      "2545 [D loss: 1.014840] [G loss: 0.978025]\n",
      "2546 [D loss: 1.002474] [G loss: 0.985095]\n",
      "2547 [D loss: 1.018307] [G loss: 0.974932]\n",
      "2548 [D loss: 0.994065] [G loss: 0.983206]\n",
      "2549 [D loss: 1.009911] [G loss: 0.985455]\n",
      "2550 [D loss: 0.985607] [G loss: 0.993947]\n",
      "2551 [D loss: 1.000786] [G loss: 1.004219]\n",
      "2552 [D loss: 1.001508] [G loss: 0.985153]\n",
      "2553 [D loss: 0.995840] [G loss: 0.990067]\n",
      "2554 [D loss: 1.007224] [G loss: 0.986580]\n",
      "2555 [D loss: 1.011644] [G loss: 0.991415]\n",
      "2556 [D loss: 1.010747] [G loss: 0.975365]\n",
      "2557 [D loss: 1.008449] [G loss: 0.989474]\n",
      "2558 [D loss: 1.009327] [G loss: 0.984896]\n",
      "2559 [D loss: 0.996996] [G loss: 0.979914]\n",
      "2560 [D loss: 0.996408] [G loss: 0.992397]\n",
      "2561 [D loss: 1.011229] [G loss: 0.968909]\n",
      "2562 [D loss: 0.997564] [G loss: 0.970838]\n",
      "2563 [D loss: 0.999233] [G loss: 0.983062]\n",
      "2564 [D loss: 1.010430] [G loss: 0.983980]\n",
      "2565 [D loss: 1.013135] [G loss: 0.980731]\n",
      "2566 [D loss: 1.002085] [G loss: 0.978833]\n",
      "2567 [D loss: 0.993456] [G loss: 0.987985]\n",
      "2568 [D loss: 0.994127] [G loss: 0.976582]\n",
      "2569 [D loss: 1.009759] [G loss: 0.974252]\n",
      "2570 [D loss: 1.018798] [G loss: 0.986172]\n",
      "2571 [D loss: 0.994092] [G loss: 0.985650]\n",
      "2572 [D loss: 1.007083] [G loss: 0.968142]\n",
      "2573 [D loss: 0.997836] [G loss: 0.994044]\n",
      "2574 [D loss: 1.006092] [G loss: 0.995487]\n",
      "2575 [D loss: 1.018367] [G loss: 0.979256]\n",
      "2576 [D loss: 1.017705] [G loss: 0.989112]\n",
      "2577 [D loss: 1.000202] [G loss: 0.988258]\n",
      "2578 [D loss: 1.015924] [G loss: 1.001954]\n",
      "2579 [D loss: 1.005343] [G loss: 0.981572]\n",
      "2580 [D loss: 1.006252] [G loss: 0.979610]\n",
      "2581 [D loss: 1.002738] [G loss: 0.992194]\n",
      "2582 [D loss: 1.019205] [G loss: 0.990992]\n",
      "2583 [D loss: 1.016318] [G loss: 0.977015]\n",
      "2584 [D loss: 0.989534] [G loss: 1.006799]\n",
      "2585 [D loss: 1.008330] [G loss: 0.974256]\n",
      "2586 [D loss: 1.003941] [G loss: 0.977165]\n",
      "2587 [D loss: 0.988891] [G loss: 1.000728]\n",
      "2588 [D loss: 1.007025] [G loss: 0.984343]\n",
      "2589 [D loss: 1.006058] [G loss: 0.994855]\n",
      "2590 [D loss: 0.998220] [G loss: 0.993616]\n",
      "2591 [D loss: 1.016264] [G loss: 0.976600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592 [D loss: 1.007873] [G loss: 0.984987]\n",
      "2593 [D loss: 1.003538] [G loss: 0.983079]\n",
      "2594 [D loss: 0.997748] [G loss: 0.994743]\n",
      "2595 [D loss: 1.008618] [G loss: 0.991765]\n",
      "2596 [D loss: 0.998623] [G loss: 0.973211]\n",
      "2597 [D loss: 1.013369] [G loss: 0.979921]\n",
      "2598 [D loss: 1.006315] [G loss: 0.995377]\n",
      "2599 [D loss: 0.995790] [G loss: 0.989863]\n",
      "2600 [D loss: 1.005646] [G loss: 0.983839]\n",
      "2601 [D loss: 1.014284] [G loss: 0.988507]\n",
      "2602 [D loss: 1.011738] [G loss: 0.976401]\n",
      "2603 [D loss: 1.002015] [G loss: 1.005361]\n",
      "2604 [D loss: 1.011306] [G loss: 0.988705]\n",
      "2605 [D loss: 1.005863] [G loss: 1.003719]\n",
      "2606 [D loss: 1.002767] [G loss: 0.995875]\n",
      "2607 [D loss: 1.020211] [G loss: 0.977648]\n",
      "2608 [D loss: 0.996370] [G loss: 0.984325]\n",
      "2609 [D loss: 1.009574] [G loss: 0.989914]\n",
      "2610 [D loss: 0.998969] [G loss: 0.993258]\n",
      "2611 [D loss: 0.996308] [G loss: 0.991851]\n",
      "2612 [D loss: 1.003422] [G loss: 0.986529]\n",
      "2613 [D loss: 1.003369] [G loss: 0.977937]\n",
      "2614 [D loss: 1.007813] [G loss: 0.990381]\n",
      "2615 [D loss: 0.998332] [G loss: 0.991535]\n",
      "2616 [D loss: 1.004505] [G loss: 1.002357]\n",
      "2617 [D loss: 0.997032] [G loss: 0.987046]\n",
      "2618 [D loss: 1.009285] [G loss: 0.988958]\n",
      "2619 [D loss: 1.010779] [G loss: 1.005851]\n",
      "2620 [D loss: 0.994925] [G loss: 0.997335]\n",
      "2621 [D loss: 1.013808] [G loss: 0.992437]\n",
      "2622 [D loss: 1.021468] [G loss: 0.993944]\n",
      "2623 [D loss: 1.010710] [G loss: 1.000435]\n",
      "2624 [D loss: 1.005754] [G loss: 0.996233]\n",
      "2625 [D loss: 1.011007] [G loss: 0.982075]\n",
      "2626 [D loss: 0.993017] [G loss: 0.999428]\n",
      "2627 [D loss: 0.993427] [G loss: 0.992650]\n",
      "2628 [D loss: 1.005060] [G loss: 0.989365]\n",
      "2629 [D loss: 1.000375] [G loss: 0.985595]\n",
      "2630 [D loss: 1.002022] [G loss: 0.996600]\n",
      "2631 [D loss: 1.008130] [G loss: 0.979854]\n",
      "2632 [D loss: 0.987932] [G loss: 1.018553]\n",
      "2633 [D loss: 1.009419] [G loss: 0.993372]\n",
      "2634 [D loss: 1.002387] [G loss: 0.999454]\n",
      "2635 [D loss: 1.006092] [G loss: 0.996089]\n",
      "2636 [D loss: 1.002178] [G loss: 0.990364]\n",
      "2637 [D loss: 0.999543] [G loss: 0.994011]\n",
      "2638 [D loss: 1.002541] [G loss: 0.975310]\n",
      "2639 [D loss: 1.010932] [G loss: 0.990059]\n",
      "2640 [D loss: 1.002763] [G loss: 0.990104]\n",
      "2641 [D loss: 1.018899] [G loss: 0.972282]\n",
      "2642 [D loss: 1.012236] [G loss: 0.980822]\n",
      "2643 [D loss: 0.999137] [G loss: 0.994313]\n",
      "2644 [D loss: 1.018627] [G loss: 0.970902]\n",
      "2645 [D loss: 1.009808] [G loss: 0.996158]\n",
      "2646 [D loss: 1.010138] [G loss: 0.995154]\n",
      "2647 [D loss: 0.991142] [G loss: 1.005504]\n",
      "2648 [D loss: 1.001748] [G loss: 0.998839]\n",
      "2649 [D loss: 1.006257] [G loss: 0.996013]\n",
      "2650 [D loss: 0.997668] [G loss: 0.987312]\n",
      "2651 [D loss: 0.999050] [G loss: 1.006574]\n",
      "2652 [D loss: 1.000131] [G loss: 0.978137]\n",
      "2653 [D loss: 0.997684] [G loss: 0.989762]\n",
      "2654 [D loss: 1.014017] [G loss: 0.980023]\n",
      "2655 [D loss: 0.995614] [G loss: 0.986365]\n",
      "2656 [D loss: 1.010268] [G loss: 0.996957]\n",
      "2657 [D loss: 1.001940] [G loss: 1.002075]\n",
      "2658 [D loss: 1.003404] [G loss: 0.987890]\n",
      "2659 [D loss: 0.999402] [G loss: 0.986123]\n",
      "2660 [D loss: 1.006627] [G loss: 0.979458]\n",
      "2661 [D loss: 1.025227] [G loss: 0.982684]\n",
      "2662 [D loss: 1.010684] [G loss: 0.982672]\n",
      "2663 [D loss: 0.997686] [G loss: 0.974703]\n",
      "2664 [D loss: 1.010864] [G loss: 0.977465]\n",
      "2665 [D loss: 1.008977] [G loss: 0.987930]\n",
      "2666 [D loss: 0.989974] [G loss: 0.980551]\n",
      "2667 [D loss: 1.000210] [G loss: 0.977066]\n",
      "2668 [D loss: 1.006070] [G loss: 0.976535]\n",
      "2669 [D loss: 1.004769] [G loss: 0.986369]\n",
      "2670 [D loss: 1.010301] [G loss: 0.970360]\n",
      "2671 [D loss: 0.988865] [G loss: 0.991871]\n",
      "2672 [D loss: 0.999056] [G loss: 0.996296]\n",
      "2673 [D loss: 1.006462] [G loss: 0.996251]\n",
      "2674 [D loss: 1.017701] [G loss: 0.981756]\n",
      "2675 [D loss: 0.994714] [G loss: 0.999657]\n",
      "2676 [D loss: 0.998774] [G loss: 0.998936]\n",
      "2677 [D loss: 1.009211] [G loss: 0.987392]\n",
      "2678 [D loss: 1.013216] [G loss: 0.990494]\n",
      "2679 [D loss: 1.004359] [G loss: 0.988405]\n",
      "2680 [D loss: 1.003845] [G loss: 0.979816]\n",
      "2681 [D loss: 0.994849] [G loss: 0.995989]\n",
      "2682 [D loss: 1.005082] [G loss: 0.981724]\n",
      "2683 [D loss: 1.004963] [G loss: 0.970879]\n",
      "2684 [D loss: 0.997733] [G loss: 0.992733]\n",
      "2685 [D loss: 1.011394] [G loss: 0.976089]\n",
      "2686 [D loss: 1.002417] [G loss: 0.977335]\n",
      "2687 [D loss: 1.005671] [G loss: 0.983997]\n",
      "2688 [D loss: 0.996448] [G loss: 0.996703]\n",
      "2689 [D loss: 1.007182] [G loss: 0.998698]\n",
      "2690 [D loss: 1.011857] [G loss: 0.971024]\n",
      "2691 [D loss: 0.994367] [G loss: 1.001559]\n",
      "2692 [D loss: 1.011813] [G loss: 0.963604]\n",
      "2693 [D loss: 1.009495] [G loss: 0.993131]\n",
      "2694 [D loss: 1.011023] [G loss: 0.970495]\n",
      "2695 [D loss: 1.007059] [G loss: 0.977468]\n",
      "2696 [D loss: 1.010481] [G loss: 0.996930]\n",
      "2697 [D loss: 0.997816] [G loss: 0.992101]\n",
      "2698 [D loss: 1.002435] [G loss: 0.983889]\n",
      "2699 [D loss: 0.998068] [G loss: 1.000574]\n",
      "2700 [D loss: 1.007066] [G loss: 0.984826]\n",
      "2701 [D loss: 1.001825] [G loss: 0.985863]\n",
      "2702 [D loss: 1.000601] [G loss: 0.982560]\n",
      "2703 [D loss: 0.994241] [G loss: 0.977938]\n",
      "2704 [D loss: 1.010354] [G loss: 0.987287]\n",
      "2705 [D loss: 1.019303] [G loss: 0.999309]\n",
      "2706 [D loss: 1.004176] [G loss: 0.982569]\n",
      "2707 [D loss: 0.998377] [G loss: 0.999910]\n",
      "2708 [D loss: 0.995441] [G loss: 0.994788]\n",
      "2709 [D loss: 1.009954] [G loss: 0.984077]\n",
      "2710 [D loss: 1.017546] [G loss: 0.992702]\n",
      "2711 [D loss: 0.994957] [G loss: 0.974311]\n",
      "2712 [D loss: 1.006466] [G loss: 0.993082]\n",
      "2713 [D loss: 1.005548] [G loss: 0.987437]\n",
      "2714 [D loss: 1.001620] [G loss: 0.999391]\n",
      "2715 [D loss: 1.010690] [G loss: 0.995056]\n",
      "2716 [D loss: 0.998103] [G loss: 0.997546]\n",
      "2717 [D loss: 1.009232] [G loss: 0.988660]\n",
      "2718 [D loss: 1.003757] [G loss: 0.977100]\n",
      "2719 [D loss: 1.007486] [G loss: 0.976947]\n",
      "2720 [D loss: 1.000869] [G loss: 0.985409]\n",
      "2721 [D loss: 1.000179] [G loss: 0.992520]\n",
      "2722 [D loss: 1.004234] [G loss: 0.975331]\n",
      "2723 [D loss: 1.002236] [G loss: 0.999519]\n",
      "2724 [D loss: 1.002970] [G loss: 0.998806]\n",
      "2725 [D loss: 0.999481] [G loss: 0.975507]\n",
      "2726 [D loss: 1.003853] [G loss: 0.991482]\n",
      "2727 [D loss: 1.008430] [G loss: 0.976971]\n",
      "2728 [D loss: 1.015186] [G loss: 0.983089]\n",
      "2729 [D loss: 1.000939] [G loss: 1.001903]\n",
      "2730 [D loss: 0.999115] [G loss: 0.987858]\n",
      "2731 [D loss: 0.996671] [G loss: 0.975577]\n",
      "2732 [D loss: 1.011153] [G loss: 0.997583]\n",
      "2733 [D loss: 1.004253] [G loss: 0.993625]\n",
      "2734 [D loss: 1.014194] [G loss: 0.983838]\n",
      "2735 [D loss: 1.001646] [G loss: 0.971532]\n",
      "2736 [D loss: 1.002332] [G loss: 0.979475]\n",
      "2737 [D loss: 0.999124] [G loss: 0.988119]\n",
      "2738 [D loss: 1.006867] [G loss: 0.988744]\n",
      "2739 [D loss: 1.006643] [G loss: 0.989764]\n",
      "2740 [D loss: 1.006447] [G loss: 1.001472]\n",
      "2741 [D loss: 1.001384] [G loss: 0.990207]\n",
      "2742 [D loss: 1.009035] [G loss: 0.986116]\n",
      "2743 [D loss: 1.004249] [G loss: 1.006887]\n",
      "2744 [D loss: 1.010958] [G loss: 0.973432]\n",
      "2745 [D loss: 1.004666] [G loss: 0.997546]\n",
      "2746 [D loss: 1.003839] [G loss: 0.994394]\n",
      "2747 [D loss: 0.997719] [G loss: 0.997609]\n",
      "2748 [D loss: 0.994373] [G loss: 0.982756]\n",
      "2749 [D loss: 1.018517] [G loss: 1.000967]\n",
      "2750 [D loss: 1.011059] [G loss: 0.998579]\n",
      "2751 [D loss: 0.998165] [G loss: 1.001112]\n",
      "2752 [D loss: 1.000578] [G loss: 1.006364]\n",
      "2753 [D loss: 1.023402] [G loss: 0.987228]\n",
      "2754 [D loss: 1.013260] [G loss: 0.986236]\n",
      "2755 [D loss: 1.000487] [G loss: 0.998776]\n",
      "2756 [D loss: 1.003945] [G loss: 0.993376]\n",
      "2757 [D loss: 1.006943] [G loss: 0.978940]\n",
      "2758 [D loss: 0.999054] [G loss: 0.989216]\n",
      "2759 [D loss: 1.012246] [G loss: 0.989094]\n",
      "2760 [D loss: 1.016982] [G loss: 0.984994]\n",
      "2761 [D loss: 1.006013] [G loss: 1.003223]\n",
      "2762 [D loss: 0.993940] [G loss: 0.981756]\n",
      "2763 [D loss: 1.003491] [G loss: 0.991516]\n",
      "2764 [D loss: 1.006564] [G loss: 0.988746]\n",
      "2765 [D loss: 1.019316] [G loss: 0.983427]\n",
      "2766 [D loss: 1.005625] [G loss: 0.978045]\n",
      "2767 [D loss: 1.018628] [G loss: 0.982341]\n",
      "2768 [D loss: 1.001639] [G loss: 0.975426]\n",
      "2769 [D loss: 1.001047] [G loss: 0.975153]\n",
      "2770 [D loss: 0.997773] [G loss: 0.984018]\n",
      "2771 [D loss: 0.998654] [G loss: 0.986617]\n",
      "2772 [D loss: 1.009977] [G loss: 0.982376]\n",
      "2773 [D loss: 0.999999] [G loss: 0.993007]\n",
      "2774 [D loss: 0.993205] [G loss: 0.988964]\n",
      "2775 [D loss: 0.998890] [G loss: 0.972756]\n",
      "2776 [D loss: 0.995439] [G loss: 0.989827]\n",
      "2777 [D loss: 0.998610] [G loss: 0.981779]\n",
      "2778 [D loss: 1.006671] [G loss: 0.984673]\n",
      "2779 [D loss: 0.986605] [G loss: 0.994723]\n",
      "2780 [D loss: 1.001361] [G loss: 0.996196]\n",
      "2781 [D loss: 1.010678] [G loss: 0.971597]\n",
      "2782 [D loss: 1.015066] [G loss: 0.987319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2783 [D loss: 1.005930] [G loss: 0.968358]\n",
      "2784 [D loss: 1.003806] [G loss: 0.977932]\n",
      "2785 [D loss: 1.013231] [G loss: 0.974215]\n",
      "2786 [D loss: 1.012863] [G loss: 0.997990]\n",
      "2787 [D loss: 1.006135] [G loss: 0.989193]\n",
      "2788 [D loss: 0.997633] [G loss: 0.989475]\n",
      "2789 [D loss: 1.006923] [G loss: 0.987301]\n",
      "2790 [D loss: 1.002095] [G loss: 0.982133]\n",
      "2791 [D loss: 1.001571] [G loss: 0.977547]\n",
      "2792 [D loss: 1.007627] [G loss: 0.978109]\n",
      "2793 [D loss: 1.012635] [G loss: 0.970100]\n",
      "2794 [D loss: 1.006436] [G loss: 0.981615]\n",
      "2795 [D loss: 1.009753] [G loss: 0.979766]\n",
      "2796 [D loss: 1.012828] [G loss: 0.975094]\n",
      "2797 [D loss: 1.002839] [G loss: 0.967512]\n",
      "2798 [D loss: 1.012901] [G loss: 0.974507]\n",
      "2799 [D loss: 1.014717] [G loss: 0.983626]\n",
      "2800 [D loss: 1.009813] [G loss: 0.980731]\n",
      "2801 [D loss: 0.996854] [G loss: 0.975323]\n",
      "2802 [D loss: 1.010804] [G loss: 0.975234]\n",
      "2803 [D loss: 1.003732] [G loss: 0.986669]\n",
      "2804 [D loss: 1.003801] [G loss: 0.985891]\n",
      "2805 [D loss: 1.005022] [G loss: 0.979680]\n",
      "2806 [D loss: 0.997046] [G loss: 0.986053]\n",
      "2807 [D loss: 1.008938] [G loss: 0.969187]\n",
      "2808 [D loss: 1.014194] [G loss: 0.975217]\n",
      "2809 [D loss: 1.004890] [G loss: 0.971868]\n",
      "2810 [D loss: 1.005399] [G loss: 0.979156]\n",
      "2811 [D loss: 1.001190] [G loss: 0.995193]\n",
      "2812 [D loss: 1.000672] [G loss: 0.985447]\n",
      "2813 [D loss: 1.006300] [G loss: 0.975992]\n",
      "2814 [D loss: 1.007787] [G loss: 0.986397]\n",
      "2815 [D loss: 0.991939] [G loss: 0.993936]\n",
      "2816 [D loss: 1.005503] [G loss: 0.977532]\n",
      "2817 [D loss: 1.001413] [G loss: 0.971124]\n",
      "2818 [D loss: 0.998503] [G loss: 0.981149]\n",
      "2819 [D loss: 1.002403] [G loss: 0.985428]\n",
      "2820 [D loss: 1.004307] [G loss: 0.990570]\n",
      "2821 [D loss: 0.996738] [G loss: 0.987358]\n",
      "2822 [D loss: 1.019755] [G loss: 0.974453]\n",
      "2823 [D loss: 1.007624] [G loss: 0.985868]\n",
      "2824 [D loss: 1.009483] [G loss: 0.982385]\n",
      "2825 [D loss: 1.001440] [G loss: 0.997916]\n",
      "2826 [D loss: 1.010059] [G loss: 0.969262]\n",
      "2827 [D loss: 1.006296] [G loss: 0.994164]\n",
      "2828 [D loss: 1.012386] [G loss: 0.985015]\n",
      "2829 [D loss: 1.009583] [G loss: 0.992179]\n",
      "2830 [D loss: 1.007095] [G loss: 0.991197]\n",
      "2831 [D loss: 1.005452] [G loss: 0.993276]\n",
      "2832 [D loss: 1.010681] [G loss: 0.995876]\n",
      "2833 [D loss: 1.004864] [G loss: 0.985163]\n",
      "2834 [D loss: 1.001930] [G loss: 0.983705]\n",
      "2835 [D loss: 1.009186] [G loss: 0.983138]\n",
      "2836 [D loss: 1.001989] [G loss: 0.988057]\n",
      "2837 [D loss: 1.003219] [G loss: 0.999369]\n",
      "2838 [D loss: 1.010449] [G loss: 0.999050]\n",
      "2839 [D loss: 1.010367] [G loss: 0.998274]\n",
      "2840 [D loss: 1.000648] [G loss: 1.003895]\n",
      "2841 [D loss: 1.002685] [G loss: 1.006164]\n",
      "2842 [D loss: 1.000512] [G loss: 1.002480]\n",
      "2843 [D loss: 0.995824] [G loss: 0.988992]\n",
      "2844 [D loss: 1.011993] [G loss: 1.001620]\n",
      "2845 [D loss: 1.004746] [G loss: 0.984909]\n",
      "2846 [D loss: 1.015909] [G loss: 0.993721]\n",
      "2847 [D loss: 1.006621] [G loss: 0.993979]\n",
      "2848 [D loss: 1.001920] [G loss: 0.999995]\n",
      "2849 [D loss: 0.987864] [G loss: 0.999427]\n",
      "2850 [D loss: 1.020653] [G loss: 0.987599]\n",
      "2851 [D loss: 1.006879] [G loss: 0.995156]\n",
      "2852 [D loss: 1.006975] [G loss: 0.997822]\n",
      "2853 [D loss: 1.004773] [G loss: 0.987372]\n",
      "2854 [D loss: 0.996269] [G loss: 0.998946]\n",
      "2855 [D loss: 1.012503] [G loss: 0.986056]\n",
      "2856 [D loss: 1.002818] [G loss: 1.005880]\n",
      "2857 [D loss: 0.998258] [G loss: 1.000324]\n",
      "2858 [D loss: 1.009085] [G loss: 0.982605]\n",
      "2859 [D loss: 1.009050] [G loss: 0.989583]\n",
      "2860 [D loss: 1.007545] [G loss: 0.995143]\n",
      "2861 [D loss: 1.010875] [G loss: 0.989785]\n",
      "2862 [D loss: 1.005447] [G loss: 1.008989]\n",
      "2863 [D loss: 0.997069] [G loss: 0.997072]\n",
      "2864 [D loss: 1.009968] [G loss: 0.991123]\n",
      "2865 [D loss: 1.011836] [G loss: 0.995022]\n",
      "2866 [D loss: 1.001575] [G loss: 1.015748]\n",
      "2867 [D loss: 1.003938] [G loss: 0.996134]\n",
      "2868 [D loss: 0.999633] [G loss: 0.981424]\n",
      "2869 [D loss: 1.014336] [G loss: 0.977696]\n",
      "2870 [D loss: 1.002771] [G loss: 1.012225]\n",
      "2871 [D loss: 1.013363] [G loss: 1.006642]\n",
      "2872 [D loss: 1.002772] [G loss: 0.994083]\n",
      "2873 [D loss: 0.993082] [G loss: 0.993740]\n",
      "2874 [D loss: 1.001520] [G loss: 1.020979]\n",
      "2875 [D loss: 0.992326] [G loss: 1.000317]\n",
      "2876 [D loss: 1.002350] [G loss: 0.993227]\n",
      "2877 [D loss: 1.003890] [G loss: 0.991815]\n",
      "2878 [D loss: 1.001037] [G loss: 1.010761]\n",
      "2879 [D loss: 1.011219] [G loss: 0.997206]\n",
      "2880 [D loss: 0.995005] [G loss: 1.003822]\n",
      "2881 [D loss: 0.998626] [G loss: 1.003205]\n",
      "2882 [D loss: 1.000709] [G loss: 1.002084]\n",
      "2883 [D loss: 1.005460] [G loss: 0.997230]\n",
      "2884 [D loss: 1.001346] [G loss: 0.983121]\n",
      "2885 [D loss: 1.007450] [G loss: 0.992964]\n",
      "2886 [D loss: 1.006219] [G loss: 1.001614]\n",
      "2887 [D loss: 0.996310] [G loss: 0.988625]\n",
      "2888 [D loss: 0.998540] [G loss: 1.000918]\n",
      "2889 [D loss: 1.000321] [G loss: 0.986242]\n",
      "2890 [D loss: 1.003114] [G loss: 0.991858]\n",
      "2891 [D loss: 1.002279] [G loss: 1.001138]\n",
      "2892 [D loss: 0.988865] [G loss: 1.002121]\n",
      "2893 [D loss: 1.006230] [G loss: 0.996321]\n",
      "2894 [D loss: 1.009020] [G loss: 0.996841]\n",
      "2895 [D loss: 1.003378] [G loss: 1.000025]\n",
      "2896 [D loss: 1.004769] [G loss: 1.001188]\n",
      "2897 [D loss: 0.997532] [G loss: 0.994859]\n",
      "2898 [D loss: 1.001922] [G loss: 0.994268]\n",
      "2899 [D loss: 1.006392] [G loss: 0.988885]\n",
      "2900 [D loss: 1.005997] [G loss: 0.980298]\n",
      "2901 [D loss: 1.000800] [G loss: 0.993517]\n",
      "2902 [D loss: 0.998072] [G loss: 0.998507]\n",
      "2903 [D loss: 1.001509] [G loss: 0.997949]\n",
      "2904 [D loss: 1.007674] [G loss: 0.990307]\n",
      "2905 [D loss: 0.997995] [G loss: 0.998782]\n",
      "2906 [D loss: 0.997863] [G loss: 0.993683]\n",
      "2907 [D loss: 1.006034] [G loss: 0.971597]\n",
      "2908 [D loss: 1.008064] [G loss: 0.999952]\n",
      "2909 [D loss: 1.005842] [G loss: 0.990474]\n",
      "2910 [D loss: 1.001991] [G loss: 0.986343]\n",
      "2911 [D loss: 1.000958] [G loss: 0.984116]\n",
      "2912 [D loss: 0.994261] [G loss: 0.994072]\n",
      "2913 [D loss: 1.003557] [G loss: 0.985862]\n",
      "2914 [D loss: 0.996876] [G loss: 0.968629]\n",
      "2915 [D loss: 1.006182] [G loss: 0.979920]\n",
      "2916 [D loss: 1.002174] [G loss: 1.001397]\n",
      "2917 [D loss: 1.005463] [G loss: 0.989030]\n",
      "2918 [D loss: 0.999892] [G loss: 0.993053]\n",
      "2919 [D loss: 1.016136] [G loss: 0.991391]\n",
      "2920 [D loss: 0.996799] [G loss: 0.992577]\n",
      "2921 [D loss: 1.005736] [G loss: 0.983628]\n",
      "2922 [D loss: 1.012770] [G loss: 0.987643]\n",
      "2923 [D loss: 1.010774] [G loss: 0.986411]\n",
      "2924 [D loss: 1.001331] [G loss: 0.995530]\n",
      "2925 [D loss: 1.002194] [G loss: 0.996730]\n",
      "2926 [D loss: 1.002940] [G loss: 0.996775]\n",
      "2927 [D loss: 1.000816] [G loss: 0.976813]\n",
      "2928 [D loss: 1.009682] [G loss: 1.001706]\n",
      "2929 [D loss: 1.010697] [G loss: 0.980328]\n",
      "2930 [D loss: 0.996713] [G loss: 0.999697]\n",
      "2931 [D loss: 1.005010] [G loss: 0.983431]\n",
      "2932 [D loss: 0.993717] [G loss: 1.005168]\n",
      "2933 [D loss: 1.001278] [G loss: 0.997040]\n",
      "2934 [D loss: 1.005337] [G loss: 0.995290]\n",
      "2935 [D loss: 1.008713] [G loss: 0.962665]\n",
      "2936 [D loss: 1.020369] [G loss: 0.981519]\n",
      "2937 [D loss: 1.005153] [G loss: 0.979752]\n",
      "2938 [D loss: 1.005519] [G loss: 0.994064]\n",
      "2939 [D loss: 1.001057] [G loss: 0.979695]\n",
      "2940 [D loss: 1.001007] [G loss: 0.991066]\n",
      "2941 [D loss: 1.011433] [G loss: 0.974434]\n",
      "2942 [D loss: 1.001783] [G loss: 0.987583]\n",
      "2943 [D loss: 1.018711] [G loss: 0.979160]\n",
      "2944 [D loss: 0.991249] [G loss: 0.982217]\n",
      "2945 [D loss: 1.004820] [G loss: 0.994690]\n",
      "2946 [D loss: 1.011134] [G loss: 0.976784]\n",
      "2947 [D loss: 1.013106] [G loss: 0.983870]\n",
      "2948 [D loss: 1.006727] [G loss: 0.999998]\n",
      "2949 [D loss: 1.009793] [G loss: 0.997958]\n",
      "2950 [D loss: 1.003816] [G loss: 0.990028]\n",
      "2951 [D loss: 1.008141] [G loss: 0.978890]\n",
      "2952 [D loss: 0.997406] [G loss: 0.983699]\n",
      "2953 [D loss: 1.014780] [G loss: 0.979865]\n",
      "2954 [D loss: 1.008859] [G loss: 0.974433]\n",
      "2955 [D loss: 1.009402] [G loss: 0.982017]\n",
      "2956 [D loss: 1.006550] [G loss: 0.976032]\n",
      "2957 [D loss: 1.006774] [G loss: 0.980383]\n",
      "2958 [D loss: 1.005398] [G loss: 0.980041]\n",
      "2959 [D loss: 1.006878] [G loss: 0.993362]\n",
      "2960 [D loss: 1.000168] [G loss: 0.981387]\n",
      "2961 [D loss: 0.997745] [G loss: 0.990932]\n",
      "2962 [D loss: 1.005811] [G loss: 0.967120]\n",
      "2963 [D loss: 1.007712] [G loss: 0.989917]\n",
      "2964 [D loss: 1.002420] [G loss: 0.987574]\n",
      "2965 [D loss: 1.008921] [G loss: 0.979705]\n",
      "2966 [D loss: 1.001568] [G loss: 0.995280]\n",
      "2967 [D loss: 1.007189] [G loss: 1.000398]\n",
      "2968 [D loss: 1.013346] [G loss: 0.971357]\n",
      "2969 [D loss: 0.989569] [G loss: 0.984455]\n",
      "2970 [D loss: 1.000183] [G loss: 0.986762]\n",
      "2971 [D loss: 1.011620] [G loss: 0.985439]\n",
      "2972 [D loss: 1.007045] [G loss: 0.974262]\n",
      "2973 [D loss: 1.022902] [G loss: 0.963912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2974 [D loss: 1.010346] [G loss: 0.968533]\n",
      "2975 [D loss: 1.001339] [G loss: 0.999981]\n",
      "2976 [D loss: 1.012597] [G loss: 0.970124]\n",
      "2977 [D loss: 1.014574] [G loss: 0.983459]\n",
      "2978 [D loss: 1.007924] [G loss: 0.990004]\n",
      "2979 [D loss: 1.009402] [G loss: 0.984214]\n",
      "2980 [D loss: 1.023114] [G loss: 0.978709]\n",
      "2981 [D loss: 1.005811] [G loss: 0.989138]\n",
      "2982 [D loss: 1.009640] [G loss: 0.997329]\n",
      "2983 [D loss: 1.006438] [G loss: 0.992876]\n",
      "2984 [D loss: 1.006172] [G loss: 0.970026]\n",
      "2985 [D loss: 1.006071] [G loss: 0.985058]\n",
      "2986 [D loss: 1.001006] [G loss: 0.970694]\n",
      "2987 [D loss: 1.000653] [G loss: 1.004515]\n",
      "2988 [D loss: 1.010829] [G loss: 0.989470]\n",
      "2989 [D loss: 0.997931] [G loss: 0.993978]\n",
      "2990 [D loss: 1.005458] [G loss: 0.983522]\n",
      "2991 [D loss: 1.010157] [G loss: 0.991674]\n",
      "2992 [D loss: 1.007917] [G loss: 0.992764]\n",
      "2993 [D loss: 1.006577] [G loss: 0.984108]\n",
      "2994 [D loss: 1.011330] [G loss: 0.990636]\n",
      "2995 [D loss: 1.006755] [G loss: 1.002708]\n",
      "2996 [D loss: 0.997659] [G loss: 0.996484]\n",
      "2997 [D loss: 1.000277] [G loss: 1.000218]\n",
      "2998 [D loss: 1.005650] [G loss: 0.981640]\n",
      "2999 [D loss: 0.996765] [G loss: 0.993622]\n",
      "3000 [D loss: 1.004593] [G loss: 0.994107]\n",
      "3001 [D loss: 1.002638] [G loss: 1.014748]\n",
      "3002 [D loss: 1.007265] [G loss: 0.990976]\n",
      "3003 [D loss: 1.001503] [G loss: 0.991949]\n",
      "3004 [D loss: 1.007034] [G loss: 0.989811]\n",
      "3005 [D loss: 1.013372] [G loss: 0.991204]\n",
      "3006 [D loss: 0.996966] [G loss: 0.997753]\n",
      "3007 [D loss: 1.006614] [G loss: 0.995139]\n",
      "3008 [D loss: 1.000822] [G loss: 0.978338]\n",
      "3009 [D loss: 1.000639] [G loss: 0.984022]\n",
      "3010 [D loss: 1.018966] [G loss: 0.982515]\n",
      "3011 [D loss: 1.001822] [G loss: 0.990159]\n",
      "3012 [D loss: 1.010650] [G loss: 0.991637]\n",
      "3013 [D loss: 1.015467] [G loss: 0.980405]\n",
      "3014 [D loss: 1.011220] [G loss: 1.000767]\n",
      "3015 [D loss: 1.009000] [G loss: 0.993999]\n",
      "3016 [D loss: 1.005045] [G loss: 0.983334]\n",
      "3017 [D loss: 1.003404] [G loss: 0.997217]\n",
      "3018 [D loss: 1.015345] [G loss: 0.986886]\n",
      "3019 [D loss: 0.999979] [G loss: 0.990058]\n",
      "3020 [D loss: 1.011411] [G loss: 0.984680]\n",
      "3021 [D loss: 1.006142] [G loss: 0.981743]\n",
      "3022 [D loss: 1.003369] [G loss: 0.968107]\n",
      "3023 [D loss: 1.001093] [G loss: 0.986242]\n",
      "3024 [D loss: 1.004226] [G loss: 0.995736]\n",
      "3025 [D loss: 1.010233] [G loss: 0.994925]\n",
      "3026 [D loss: 1.018416] [G loss: 0.985475]\n",
      "3027 [D loss: 1.010587] [G loss: 1.003826]\n",
      "3028 [D loss: 0.994181] [G loss: 0.989632]\n",
      "3029 [D loss: 1.005253] [G loss: 0.999850]\n",
      "3030 [D loss: 1.004256] [G loss: 1.004951]\n",
      "3031 [D loss: 1.000788] [G loss: 1.005060]\n",
      "3032 [D loss: 1.016173] [G loss: 0.995071]\n",
      "3033 [D loss: 1.009274] [G loss: 0.983543]\n",
      "3034 [D loss: 0.999383] [G loss: 0.991288]\n",
      "3035 [D loss: 1.009064] [G loss: 1.002561]\n",
      "3036 [D loss: 1.002666] [G loss: 0.994500]\n",
      "3037 [D loss: 0.989023] [G loss: 0.990311]\n",
      "3038 [D loss: 1.004345] [G loss: 0.991246]\n",
      "3039 [D loss: 1.007873] [G loss: 1.006844]\n",
      "3040 [D loss: 1.005647] [G loss: 0.994778]\n",
      "3041 [D loss: 1.015928] [G loss: 0.995382]\n",
      "3042 [D loss: 1.011415] [G loss: 0.987422]\n",
      "3043 [D loss: 1.002080] [G loss: 0.994142]\n",
      "3044 [D loss: 0.993454] [G loss: 0.991968]\n",
      "3045 [D loss: 1.006749] [G loss: 1.001064]\n",
      "3046 [D loss: 1.002873] [G loss: 0.987872]\n",
      "3047 [D loss: 1.002101] [G loss: 0.995206]\n",
      "3048 [D loss: 1.011709] [G loss: 1.006667]\n",
      "3049 [D loss: 1.016604] [G loss: 0.993999]\n",
      "3050 [D loss: 0.991159] [G loss: 1.001599]\n",
      "3051 [D loss: 1.007815] [G loss: 0.993106]\n",
      "3052 [D loss: 1.010473] [G loss: 0.986091]\n",
      "3053 [D loss: 1.004474] [G loss: 1.003283]\n",
      "3054 [D loss: 1.004035] [G loss: 0.983577]\n",
      "3055 [D loss: 1.001571] [G loss: 1.003190]\n",
      "3056 [D loss: 1.005186] [G loss: 1.006803]\n",
      "3057 [D loss: 1.012415] [G loss: 0.995162]\n",
      "3058 [D loss: 1.011037] [G loss: 0.990398]\n",
      "3059 [D loss: 1.008088] [G loss: 1.000240]\n",
      "3060 [D loss: 0.996707] [G loss: 0.997377]\n",
      "3061 [D loss: 1.007606] [G loss: 0.989516]\n",
      "3062 [D loss: 0.999342] [G loss: 0.982107]\n",
      "3063 [D loss: 1.008651] [G loss: 0.997475]\n",
      "3064 [D loss: 0.998324] [G loss: 0.986574]\n",
      "3065 [D loss: 1.007682] [G loss: 0.983369]\n",
      "3066 [D loss: 1.007219] [G loss: 0.996957]\n",
      "3067 [D loss: 0.998507] [G loss: 0.990428]\n",
      "3068 [D loss: 1.015141] [G loss: 0.996917]\n",
      "3069 [D loss: 0.993105] [G loss: 1.007426]\n",
      "3070 [D loss: 1.014873] [G loss: 0.984005]\n",
      "3071 [D loss: 1.007089] [G loss: 0.995795]\n",
      "3072 [D loss: 1.010688] [G loss: 0.992323]\n",
      "3073 [D loss: 1.003703] [G loss: 0.990106]\n",
      "3074 [D loss: 0.998897] [G loss: 0.994770]\n",
      "3075 [D loss: 1.016855] [G loss: 0.971961]\n",
      "3076 [D loss: 0.991853] [G loss: 0.987669]\n",
      "3077 [D loss: 1.003811] [G loss: 0.967728]\n",
      "3078 [D loss: 1.002600] [G loss: 0.981265]\n",
      "3079 [D loss: 1.005526] [G loss: 0.988687]\n",
      "3080 [D loss: 1.005621] [G loss: 0.961050]\n",
      "3081 [D loss: 1.004778] [G loss: 0.993876]\n",
      "3082 [D loss: 1.008612] [G loss: 0.980637]\n",
      "3083 [D loss: 1.018973] [G loss: 0.997283]\n",
      "3084 [D loss: 1.013293] [G loss: 0.975589]\n",
      "3085 [D loss: 1.014378] [G loss: 0.987424]\n",
      "3086 [D loss: 0.999315] [G loss: 0.998361]\n",
      "3087 [D loss: 1.006514] [G loss: 0.991040]\n",
      "3088 [D loss: 1.005099] [G loss: 0.996665]\n",
      "3089 [D loss: 1.008281] [G loss: 0.982135]\n",
      "3090 [D loss: 1.004903] [G loss: 0.978092]\n",
      "3091 [D loss: 0.999657] [G loss: 0.989360]\n",
      "3092 [D loss: 1.001299] [G loss: 0.993292]\n",
      "3093 [D loss: 1.009873] [G loss: 0.985919]\n",
      "3094 [D loss: 1.000104] [G loss: 0.972801]\n",
      "3095 [D loss: 0.998606] [G loss: 1.000887]\n",
      "3096 [D loss: 1.010273] [G loss: 0.987098]\n",
      "3097 [D loss: 1.003961] [G loss: 0.990064]\n",
      "3098 [D loss: 0.997076] [G loss: 0.993875]\n",
      "3099 [D loss: 1.008268] [G loss: 0.980415]\n",
      "3100 [D loss: 1.001931] [G loss: 1.001702]\n",
      "3101 [D loss: 0.992986] [G loss: 0.986453]\n",
      "3102 [D loss: 1.015849] [G loss: 0.986181]\n",
      "3103 [D loss: 0.999114] [G loss: 0.987589]\n",
      "3104 [D loss: 1.010765] [G loss: 0.990518]\n",
      "3105 [D loss: 1.011583] [G loss: 0.993146]\n",
      "3106 [D loss: 1.003626] [G loss: 1.012183]\n",
      "3107 [D loss: 1.013449] [G loss: 0.998149]\n",
      "3108 [D loss: 1.007788] [G loss: 0.993274]\n",
      "3109 [D loss: 1.010405] [G loss: 0.991493]\n",
      "3110 [D loss: 1.013359] [G loss: 0.977485]\n",
      "3111 [D loss: 1.010186] [G loss: 0.980739]\n",
      "3112 [D loss: 1.013539] [G loss: 0.974595]\n",
      "3113 [D loss: 1.010328] [G loss: 0.999016]\n",
      "3114 [D loss: 1.009891] [G loss: 0.990978]\n",
      "3115 [D loss: 0.990536] [G loss: 0.987475]\n",
      "3116 [D loss: 0.999281] [G loss: 0.991537]\n",
      "3117 [D loss: 1.005646] [G loss: 1.009594]\n",
      "3118 [D loss: 0.995918] [G loss: 0.987050]\n",
      "3119 [D loss: 0.997726] [G loss: 1.004973]\n",
      "3120 [D loss: 1.002603] [G loss: 1.009241]\n",
      "3121 [D loss: 1.005835] [G loss: 1.000853]\n",
      "3122 [D loss: 1.006080] [G loss: 0.990385]\n",
      "3123 [D loss: 1.014767] [G loss: 1.008018]\n",
      "3124 [D loss: 1.000610] [G loss: 0.990495]\n",
      "3125 [D loss: 0.999491] [G loss: 0.997212]\n",
      "3126 [D loss: 0.993524] [G loss: 0.994520]\n",
      "3127 [D loss: 1.002319] [G loss: 0.991640]\n",
      "3128 [D loss: 1.008420] [G loss: 0.995408]\n",
      "3129 [D loss: 1.007020] [G loss: 1.004802]\n",
      "3130 [D loss: 1.012857] [G loss: 1.002740]\n",
      "3131 [D loss: 1.003149] [G loss: 1.000049]\n",
      "3132 [D loss: 1.023243] [G loss: 0.985010]\n",
      "3133 [D loss: 1.005347] [G loss: 0.993753]\n",
      "3134 [D loss: 0.995330] [G loss: 0.998358]\n",
      "3135 [D loss: 1.001603] [G loss: 0.966432]\n",
      "3136 [D loss: 0.997760] [G loss: 0.983338]\n",
      "3137 [D loss: 1.010660] [G loss: 0.965627]\n",
      "3138 [D loss: 0.993975] [G loss: 0.996772]\n",
      "3139 [D loss: 1.002273] [G loss: 1.014894]\n",
      "3140 [D loss: 1.005394] [G loss: 1.002088]\n",
      "3141 [D loss: 0.997034] [G loss: 0.988245]\n",
      "3142 [D loss: 1.012186] [G loss: 0.991084]\n",
      "3143 [D loss: 1.007787] [G loss: 1.003055]\n",
      "3144 [D loss: 1.004405] [G loss: 0.998541]\n",
      "3145 [D loss: 1.010086] [G loss: 0.995530]\n",
      "3146 [D loss: 1.005217] [G loss: 0.994254]\n",
      "3147 [D loss: 1.003971] [G loss: 1.001481]\n",
      "3148 [D loss: 0.996237] [G loss: 0.991906]\n",
      "3149 [D loss: 1.003602] [G loss: 1.001617]\n",
      "3150 [D loss: 0.991425] [G loss: 0.991864]\n",
      "3151 [D loss: 1.001516] [G loss: 0.999271]\n",
      "3152 [D loss: 0.998506] [G loss: 0.978620]\n",
      "3153 [D loss: 1.003246] [G loss: 0.986479]\n",
      "3154 [D loss: 1.014485] [G loss: 1.002694]\n",
      "3155 [D loss: 0.996344] [G loss: 0.978771]\n",
      "3156 [D loss: 1.005453] [G loss: 0.985794]\n",
      "3157 [D loss: 1.005786] [G loss: 0.979336]\n",
      "3158 [D loss: 1.010600] [G loss: 0.990683]\n",
      "3159 [D loss: 1.011146] [G loss: 0.975987]\n",
      "3160 [D loss: 1.013589] [G loss: 0.988614]\n",
      "3161 [D loss: 1.011935] [G loss: 0.997177]\n",
      "3162 [D loss: 0.999287] [G loss: 0.986771]\n",
      "3163 [D loss: 1.005076] [G loss: 0.978443]\n",
      "3164 [D loss: 1.000718] [G loss: 1.000235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3165 [D loss: 1.003366] [G loss: 0.993942]\n",
      "3166 [D loss: 1.006638] [G loss: 0.982634]\n",
      "3167 [D loss: 0.997311] [G loss: 0.991978]\n",
      "3168 [D loss: 1.010316] [G loss: 0.980192]\n",
      "3169 [D loss: 1.007343] [G loss: 0.991072]\n",
      "3170 [D loss: 1.009091] [G loss: 0.998277]\n",
      "3171 [D loss: 1.001334] [G loss: 0.974246]\n",
      "3172 [D loss: 1.009840] [G loss: 0.987052]\n",
      "3173 [D loss: 1.003747] [G loss: 1.000732]\n",
      "3174 [D loss: 1.002485] [G loss: 0.987112]\n",
      "3175 [D loss: 1.001950] [G loss: 0.979258]\n",
      "3176 [D loss: 1.012929] [G loss: 0.995143]\n",
      "3177 [D loss: 1.009669] [G loss: 0.993217]\n",
      "3178 [D loss: 1.016298] [G loss: 0.972605]\n",
      "3179 [D loss: 0.999219] [G loss: 0.989746]\n",
      "3180 [D loss: 1.005432] [G loss: 0.986651]\n",
      "3181 [D loss: 1.014511] [G loss: 0.989437]\n",
      "3182 [D loss: 1.009359] [G loss: 0.998748]\n",
      "3183 [D loss: 0.989467] [G loss: 0.976246]\n",
      "3184 [D loss: 1.000707] [G loss: 1.001982]\n",
      "3185 [D loss: 0.995786] [G loss: 0.995957]\n",
      "3186 [D loss: 1.006958] [G loss: 1.001814]\n",
      "3187 [D loss: 1.000301] [G loss: 0.988014]\n",
      "3188 [D loss: 1.005721] [G loss: 0.978583]\n",
      "3189 [D loss: 1.005395] [G loss: 0.983545]\n",
      "3190 [D loss: 1.001590] [G loss: 0.985890]\n",
      "3191 [D loss: 1.012850] [G loss: 0.983115]\n",
      "3192 [D loss: 1.008510] [G loss: 0.980319]\n",
      "3193 [D loss: 0.999501] [G loss: 0.991946]\n",
      "3194 [D loss: 1.007467] [G loss: 0.980211]\n",
      "3195 [D loss: 0.995600] [G loss: 0.990415]\n",
      "3196 [D loss: 1.001570] [G loss: 0.996282]\n",
      "3197 [D loss: 1.004938] [G loss: 0.994831]\n",
      "3198 [D loss: 0.994097] [G loss: 1.005505]\n",
      "3199 [D loss: 0.994586] [G loss: 1.001700]\n",
      "3200 [D loss: 1.005147] [G loss: 0.981441]\n",
      "3201 [D loss: 1.002254] [G loss: 0.993122]\n",
      "3202 [D loss: 1.002766] [G loss: 0.995219]\n",
      "3203 [D loss: 1.012887] [G loss: 0.986180]\n",
      "3204 [D loss: 1.006350] [G loss: 0.994538]\n",
      "3205 [D loss: 1.007436] [G loss: 1.004883]\n",
      "3206 [D loss: 1.003803] [G loss: 0.984064]\n",
      "3207 [D loss: 1.007149] [G loss: 0.992197]\n",
      "3208 [D loss: 1.004029] [G loss: 0.992674]\n",
      "3209 [D loss: 1.006918] [G loss: 0.984618]\n",
      "3210 [D loss: 1.005354] [G loss: 0.969204]\n",
      "3211 [D loss: 1.010670] [G loss: 0.989191]\n",
      "3212 [D loss: 0.997941] [G loss: 0.994207]\n",
      "3213 [D loss: 1.012582] [G loss: 0.985829]\n",
      "3214 [D loss: 0.997518] [G loss: 0.980868]\n",
      "3215 [D loss: 1.012138] [G loss: 0.985119]\n",
      "3216 [D loss: 1.014669] [G loss: 0.990421]\n",
      "3217 [D loss: 1.003801] [G loss: 0.999562]\n",
      "3218 [D loss: 1.006730] [G loss: 0.979128]\n",
      "3219 [D loss: 1.003983] [G loss: 0.997803]\n",
      "3220 [D loss: 1.006544] [G loss: 0.991009]\n",
      "3221 [D loss: 0.996861] [G loss: 0.974634]\n",
      "3222 [D loss: 1.010051] [G loss: 0.971409]\n",
      "3223 [D loss: 1.014600] [G loss: 1.004162]\n",
      "3224 [D loss: 1.003291] [G loss: 0.997211]\n",
      "3225 [D loss: 1.004013] [G loss: 0.977581]\n",
      "3226 [D loss: 1.010721] [G loss: 1.004531]\n",
      "3227 [D loss: 1.009183] [G loss: 0.994693]\n",
      "3228 [D loss: 0.996274] [G loss: 1.008068]\n",
      "3229 [D loss: 1.003264] [G loss: 1.008689]\n",
      "3230 [D loss: 1.005168] [G loss: 0.995889]\n",
      "3231 [D loss: 1.009396] [G loss: 0.995022]\n",
      "3232 [D loss: 1.003605] [G loss: 0.993327]\n",
      "3233 [D loss: 0.992305] [G loss: 0.992196]\n",
      "3234 [D loss: 1.009633] [G loss: 0.990938]\n",
      "3235 [D loss: 1.008402] [G loss: 0.991381]\n",
      "3236 [D loss: 1.004095] [G loss: 0.997048]\n",
      "3237 [D loss: 1.003367] [G loss: 0.986910]\n",
      "3238 [D loss: 0.999634] [G loss: 1.000765]\n",
      "3239 [D loss: 1.000809] [G loss: 0.998620]\n",
      "3240 [D loss: 1.017187] [G loss: 0.989972]\n",
      "3241 [D loss: 1.010813] [G loss: 0.989705]\n",
      "3242 [D loss: 1.013990] [G loss: 0.998823]\n",
      "3243 [D loss: 1.004371] [G loss: 0.986882]\n",
      "3244 [D loss: 0.998193] [G loss: 0.976804]\n",
      "3245 [D loss: 1.008396] [G loss: 0.981530]\n",
      "3246 [D loss: 1.014970] [G loss: 0.989859]\n",
      "3247 [D loss: 1.008054] [G loss: 0.983837]\n",
      "3248 [D loss: 1.001331] [G loss: 0.992481]\n",
      "3249 [D loss: 1.005088] [G loss: 0.980482]\n",
      "3250 [D loss: 1.006205] [G loss: 0.989318]\n",
      "3251 [D loss: 1.007830] [G loss: 0.979481]\n",
      "3252 [D loss: 1.018547] [G loss: 0.989412]\n",
      "3253 [D loss: 1.016772] [G loss: 0.962737]\n",
      "3254 [D loss: 1.011884] [G loss: 0.987996]\n",
      "3255 [D loss: 0.990713] [G loss: 1.009659]\n",
      "3256 [D loss: 0.994166] [G loss: 0.992869]\n",
      "3257 [D loss: 1.006969] [G loss: 0.990691]\n",
      "3258 [D loss: 0.995916] [G loss: 1.004078]\n",
      "3259 [D loss: 0.991212] [G loss: 0.980362]\n",
      "3260 [D loss: 0.996943] [G loss: 0.988212]\n",
      "3261 [D loss: 1.007011] [G loss: 0.990052]\n",
      "3262 [D loss: 1.011833] [G loss: 0.978488]\n",
      "3263 [D loss: 1.008021] [G loss: 0.995145]\n",
      "3264 [D loss: 0.991669] [G loss: 0.972407]\n",
      "3265 [D loss: 1.012591] [G loss: 0.987187]\n",
      "3266 [D loss: 1.012085] [G loss: 0.990593]\n",
      "3267 [D loss: 0.994159] [G loss: 0.981247]\n",
      "3268 [D loss: 1.001074] [G loss: 0.985959]\n",
      "3269 [D loss: 0.996093] [G loss: 0.988196]\n",
      "3270 [D loss: 1.002475] [G loss: 0.990018]\n",
      "3271 [D loss: 1.009905] [G loss: 0.996060]\n",
      "3272 [D loss: 1.007485] [G loss: 0.983103]\n",
      "3273 [D loss: 1.009156] [G loss: 0.992542]\n",
      "3274 [D loss: 1.001386] [G loss: 0.994613]\n",
      "3275 [D loss: 0.999723] [G loss: 1.003807]\n",
      "3276 [D loss: 1.000692] [G loss: 0.997586]\n",
      "3277 [D loss: 0.990151] [G loss: 0.985778]\n",
      "3278 [D loss: 1.007274] [G loss: 0.993072]\n",
      "3279 [D loss: 0.998698] [G loss: 0.986827]\n",
      "3280 [D loss: 1.004497] [G loss: 0.995437]\n",
      "3281 [D loss: 0.996674] [G loss: 0.988491]\n",
      "3282 [D loss: 0.998042] [G loss: 0.993076]\n",
      "3283 [D loss: 1.003544] [G loss: 1.001543]\n",
      "3284 [D loss: 0.996413] [G loss: 0.981443]\n",
      "3285 [D loss: 0.989578] [G loss: 0.995955]\n",
      "3286 [D loss: 1.002196] [G loss: 0.981582]\n",
      "3287 [D loss: 1.008259] [G loss: 0.985766]\n",
      "3288 [D loss: 0.995341] [G loss: 1.002004]\n",
      "3289 [D loss: 1.012899] [G loss: 0.985398]\n",
      "3290 [D loss: 1.011788] [G loss: 0.981076]\n",
      "3291 [D loss: 1.000106] [G loss: 0.995475]\n",
      "3292 [D loss: 1.011534] [G loss: 0.989186]\n",
      "3293 [D loss: 1.010687] [G loss: 0.983037]\n",
      "3294 [D loss: 1.014063] [G loss: 0.997539]\n",
      "3295 [D loss: 1.000923] [G loss: 0.993780]\n",
      "3296 [D loss: 1.005998] [G loss: 1.001889]\n",
      "3297 [D loss: 1.006471] [G loss: 0.983446]\n",
      "3298 [D loss: 1.008788] [G loss: 0.994731]\n",
      "3299 [D loss: 1.012511] [G loss: 0.997226]\n",
      "3300 [D loss: 1.016253] [G loss: 1.001484]\n",
      "3301 [D loss: 1.006720] [G loss: 0.996804]\n",
      "3302 [D loss: 1.010924] [G loss: 1.007687]\n",
      "3303 [D loss: 1.009798] [G loss: 0.984313]\n",
      "3304 [D loss: 1.010981] [G loss: 1.004641]\n",
      "3305 [D loss: 1.005790] [G loss: 0.999281]\n",
      "3306 [D loss: 1.006447] [G loss: 0.992541]\n",
      "3307 [D loss: 1.002365] [G loss: 1.000862]\n",
      "3308 [D loss: 1.006895] [G loss: 0.981988]\n",
      "3309 [D loss: 1.012092] [G loss: 0.995527]\n",
      "3310 [D loss: 1.000269] [G loss: 0.990615]\n",
      "3311 [D loss: 0.994704] [G loss: 0.994933]\n",
      "3312 [D loss: 1.004546] [G loss: 0.991452]\n",
      "3313 [D loss: 1.014585] [G loss: 0.996086]\n",
      "3314 [D loss: 1.005594] [G loss: 0.994703]\n",
      "3315 [D loss: 1.007569] [G loss: 0.993329]\n",
      "3316 [D loss: 1.009147] [G loss: 0.991010]\n",
      "3317 [D loss: 1.001779] [G loss: 1.006660]\n",
      "3318 [D loss: 1.014403] [G loss: 1.003625]\n",
      "3319 [D loss: 1.002932] [G loss: 0.995758]\n",
      "3320 [D loss: 1.009047] [G loss: 1.004707]\n",
      "3321 [D loss: 0.998902] [G loss: 1.009030]\n",
      "3322 [D loss: 1.002463] [G loss: 0.998226]\n",
      "3323 [D loss: 1.000594] [G loss: 1.008695]\n",
      "3324 [D loss: 1.021529] [G loss: 0.993605]\n",
      "3325 [D loss: 1.004018] [G loss: 1.011371]\n",
      "3326 [D loss: 1.012419] [G loss: 0.982969]\n",
      "3327 [D loss: 1.014752] [G loss: 0.996227]\n",
      "3328 [D loss: 1.002899] [G loss: 0.991822]\n",
      "3329 [D loss: 1.013496] [G loss: 1.004276]\n",
      "3330 [D loss: 1.015343] [G loss: 0.985688]\n",
      "3331 [D loss: 1.001845] [G loss: 0.997545]\n",
      "3332 [D loss: 1.004575] [G loss: 1.004752]\n",
      "3333 [D loss: 1.008682] [G loss: 0.978912]\n",
      "3334 [D loss: 1.001082] [G loss: 1.000218]\n",
      "3335 [D loss: 1.005056] [G loss: 1.014451]\n",
      "3336 [D loss: 1.007657] [G loss: 1.010167]\n",
      "3337 [D loss: 1.003732] [G loss: 1.013842]\n",
      "3338 [D loss: 1.007228] [G loss: 1.012502]\n",
      "3339 [D loss: 1.002056] [G loss: 1.000294]\n",
      "3340 [D loss: 1.000650] [G loss: 1.005216]\n",
      "3341 [D loss: 1.000542] [G loss: 1.004038]\n",
      "3342 [D loss: 1.002778] [G loss: 0.998146]\n",
      "3343 [D loss: 1.001912] [G loss: 1.006200]\n",
      "3344 [D loss: 1.002859] [G loss: 1.001328]\n",
      "3345 [D loss: 1.001496] [G loss: 0.989530]\n",
      "3346 [D loss: 0.992441] [G loss: 1.018641]\n",
      "3347 [D loss: 0.996949] [G loss: 1.014935]\n",
      "3348 [D loss: 0.999245] [G loss: 1.001191]\n",
      "3349 [D loss: 0.993283] [G loss: 1.017709]\n",
      "3350 [D loss: 0.998692] [G loss: 1.013399]\n",
      "3351 [D loss: 1.007899] [G loss: 1.007169]\n",
      "3352 [D loss: 1.011391] [G loss: 0.995829]\n",
      "3353 [D loss: 0.999657] [G loss: 0.990162]\n",
      "3354 [D loss: 0.998238] [G loss: 0.989678]\n",
      "3355 [D loss: 1.014230] [G loss: 0.980051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3356 [D loss: 0.999185] [G loss: 0.990166]\n",
      "3357 [D loss: 1.002681] [G loss: 1.004066]\n",
      "3358 [D loss: 1.010885] [G loss: 0.987540]\n",
      "3359 [D loss: 1.002610] [G loss: 0.989526]\n",
      "3360 [D loss: 1.006746] [G loss: 0.991010]\n",
      "3361 [D loss: 1.005797] [G loss: 1.001209]\n",
      "3362 [D loss: 1.007796] [G loss: 0.983150]\n",
      "3363 [D loss: 1.002673] [G loss: 0.982914]\n",
      "3364 [D loss: 1.021732] [G loss: 0.991075]\n",
      "3365 [D loss: 1.000883] [G loss: 0.996927]\n",
      "3366 [D loss: 1.008182] [G loss: 0.984092]\n",
      "3367 [D loss: 0.982598] [G loss: 0.990508]\n",
      "3368 [D loss: 1.004679] [G loss: 1.002138]\n",
      "3369 [D loss: 0.998027] [G loss: 0.988132]\n",
      "3370 [D loss: 0.997904] [G loss: 0.990872]\n",
      "3371 [D loss: 1.023639] [G loss: 0.965475]\n",
      "3372 [D loss: 1.000874] [G loss: 0.987161]\n",
      "3373 [D loss: 1.001252] [G loss: 0.994640]\n",
      "3374 [D loss: 1.010879] [G loss: 0.999714]\n",
      "3375 [D loss: 1.005584] [G loss: 0.998195]\n",
      "3376 [D loss: 1.001983] [G loss: 0.991780]\n",
      "3377 [D loss: 1.012174] [G loss: 0.972210]\n",
      "3378 [D loss: 0.995882] [G loss: 0.998148]\n",
      "3379 [D loss: 1.014302] [G loss: 0.973251]\n",
      "3380 [D loss: 1.012387] [G loss: 0.984327]\n",
      "3381 [D loss: 0.994569] [G loss: 0.990449]\n",
      "3382 [D loss: 1.002337] [G loss: 1.001453]\n",
      "3383 [D loss: 1.005607] [G loss: 0.985075]\n",
      "3384 [D loss: 1.003310] [G loss: 0.978782]\n",
      "3385 [D loss: 1.007964] [G loss: 0.970876]\n",
      "3386 [D loss: 0.997913] [G loss: 0.990417]\n",
      "3387 [D loss: 1.002930] [G loss: 0.989323]\n",
      "3388 [D loss: 1.008654] [G loss: 0.990856]\n",
      "3389 [D loss: 1.005750] [G loss: 0.981258]\n",
      "3390 [D loss: 0.994053] [G loss: 0.983116]\n",
      "3391 [D loss: 1.001489] [G loss: 0.989097]\n",
      "3392 [D loss: 1.002005] [G loss: 0.971231]\n",
      "3393 [D loss: 0.999663] [G loss: 0.997421]\n",
      "3394 [D loss: 1.001122] [G loss: 0.997738]\n",
      "3395 [D loss: 1.002682] [G loss: 0.978990]\n",
      "3396 [D loss: 1.006864] [G loss: 0.981952]\n",
      "3397 [D loss: 1.013011] [G loss: 0.972677]\n",
      "3398 [D loss: 1.006200] [G loss: 0.982402]\n",
      "3399 [D loss: 0.999803] [G loss: 0.977356]\n",
      "3400 [D loss: 1.000036] [G loss: 0.965856]\n",
      "3401 [D loss: 1.010859] [G loss: 0.974956]\n",
      "3402 [D loss: 1.011225] [G loss: 0.989042]\n",
      "3403 [D loss: 1.007214] [G loss: 0.991847]\n",
      "3404 [D loss: 1.009678] [G loss: 0.974256]\n",
      "3405 [D loss: 1.003731] [G loss: 0.977078]\n",
      "3406 [D loss: 0.999354] [G loss: 0.985924]\n",
      "3407 [D loss: 1.009232] [G loss: 0.979830]\n",
      "3408 [D loss: 1.008082] [G loss: 0.974743]\n",
      "3409 [D loss: 1.012492] [G loss: 0.982385]\n",
      "3410 [D loss: 1.009430] [G loss: 0.969392]\n",
      "3411 [D loss: 1.012797] [G loss: 0.972385]\n",
      "3412 [D loss: 1.001200] [G loss: 0.982836]\n",
      "3413 [D loss: 0.994461] [G loss: 0.981508]\n",
      "3414 [D loss: 0.998910] [G loss: 0.997438]\n",
      "3415 [D loss: 1.003904] [G loss: 0.977682]\n",
      "3416 [D loss: 1.013820] [G loss: 0.976648]\n",
      "3417 [D loss: 1.001749] [G loss: 0.987037]\n",
      "3418 [D loss: 0.992284] [G loss: 0.986508]\n",
      "3419 [D loss: 1.004184] [G loss: 0.993175]\n",
      "3420 [D loss: 1.008570] [G loss: 0.983084]\n",
      "3421 [D loss: 1.008963] [G loss: 0.988030]\n",
      "3422 [D loss: 1.022293] [G loss: 0.970092]\n",
      "3423 [D loss: 0.998450] [G loss: 0.978240]\n",
      "3424 [D loss: 1.006351] [G loss: 0.975599]\n",
      "3425 [D loss: 1.003052] [G loss: 0.979516]\n",
      "3426 [D loss: 1.010191] [G loss: 0.970679]\n",
      "3427 [D loss: 1.012082] [G loss: 0.997627]\n",
      "3428 [D loss: 1.010352] [G loss: 0.987076]\n",
      "3429 [D loss: 1.007250] [G loss: 0.979327]\n",
      "3430 [D loss: 1.008895] [G loss: 0.987119]\n",
      "3431 [D loss: 1.006795] [G loss: 0.993080]\n",
      "3432 [D loss: 1.006275] [G loss: 0.993964]\n",
      "3433 [D loss: 1.010386] [G loss: 0.995756]\n",
      "3434 [D loss: 1.006679] [G loss: 0.989666]\n",
      "3435 [D loss: 0.997234] [G loss: 0.998947]\n",
      "3436 [D loss: 1.015798] [G loss: 0.993561]\n",
      "3437 [D loss: 0.997698] [G loss: 0.981991]\n",
      "3438 [D loss: 1.005637] [G loss: 0.980655]\n",
      "3439 [D loss: 0.999080] [G loss: 0.985151]\n",
      "3440 [D loss: 1.003175] [G loss: 0.999793]\n",
      "3441 [D loss: 1.011675] [G loss: 1.008127]\n",
      "3442 [D loss: 1.019203] [G loss: 0.995492]\n",
      "3443 [D loss: 1.000294] [G loss: 0.998294]\n",
      "3444 [D loss: 1.006956] [G loss: 1.013669]\n",
      "3445 [D loss: 1.003732] [G loss: 1.019916]\n",
      "3446 [D loss: 1.004066] [G loss: 0.997809]\n",
      "3447 [D loss: 1.011322] [G loss: 0.991309]\n",
      "3448 [D loss: 1.009967] [G loss: 0.997330]\n",
      "3449 [D loss: 1.002785] [G loss: 0.981041]\n",
      "3450 [D loss: 0.997003] [G loss: 0.988496]\n",
      "3451 [D loss: 1.002401] [G loss: 1.000462]\n",
      "3452 [D loss: 1.009173] [G loss: 0.996573]\n",
      "3453 [D loss: 1.006066] [G loss: 0.993265]\n",
      "3454 [D loss: 1.011587] [G loss: 1.009554]\n",
      "3455 [D loss: 1.007423] [G loss: 1.008074]\n",
      "3456 [D loss: 1.001528] [G loss: 0.993539]\n",
      "3457 [D loss: 1.007670] [G loss: 0.996874]\n",
      "3458 [D loss: 1.011228] [G loss: 0.991401]\n",
      "3459 [D loss: 1.005429] [G loss: 0.993215]\n",
      "3460 [D loss: 1.005270] [G loss: 1.012045]\n",
      "3461 [D loss: 1.005697] [G loss: 1.000900]\n",
      "3462 [D loss: 1.000959] [G loss: 1.018424]\n",
      "3463 [D loss: 1.010146] [G loss: 0.989746]\n",
      "3464 [D loss: 1.005629] [G loss: 0.995835]\n",
      "3465 [D loss: 0.997971] [G loss: 1.001529]\n",
      "3466 [D loss: 1.000482] [G loss: 1.009248]\n",
      "3467 [D loss: 1.007696] [G loss: 0.994271]\n",
      "3468 [D loss: 1.014537] [G loss: 0.992845]\n",
      "3469 [D loss: 1.006177] [G loss: 1.011596]\n",
      "3470 [D loss: 1.007690] [G loss: 0.994420]\n",
      "3471 [D loss: 1.002562] [G loss: 1.013936]\n",
      "3472 [D loss: 1.004940] [G loss: 0.990410]\n",
      "3473 [D loss: 1.002668] [G loss: 1.000590]\n",
      "3474 [D loss: 1.012095] [G loss: 1.002033]\n",
      "3475 [D loss: 1.007779] [G loss: 1.008843]\n",
      "3476 [D loss: 1.003145] [G loss: 1.001063]\n",
      "3477 [D loss: 0.998958] [G loss: 0.996710]\n",
      "3478 [D loss: 1.000090] [G loss: 1.007891]\n",
      "3479 [D loss: 1.010620] [G loss: 0.988178]\n",
      "3480 [D loss: 1.002645] [G loss: 0.998784]\n",
      "3481 [D loss: 1.011400] [G loss: 0.993271]\n",
      "3482 [D loss: 1.003721] [G loss: 0.987344]\n",
      "3483 [D loss: 1.018733] [G loss: 1.003205]\n",
      "3484 [D loss: 1.006264] [G loss: 1.007081]\n",
      "3485 [D loss: 1.007419] [G loss: 1.003020]\n",
      "3486 [D loss: 1.009557] [G loss: 0.979995]\n",
      "3487 [D loss: 1.017467] [G loss: 0.995180]\n",
      "3488 [D loss: 1.007837] [G loss: 1.014631]\n",
      "3489 [D loss: 1.012580] [G loss: 0.983177]\n",
      "3490 [D loss: 0.992606] [G loss: 1.004015]\n",
      "3491 [D loss: 1.002896] [G loss: 1.003967]\n",
      "3492 [D loss: 1.016793] [G loss: 1.003606]\n",
      "3493 [D loss: 1.008670] [G loss: 0.987559]\n",
      "3494 [D loss: 1.005046] [G loss: 0.991680]\n",
      "3495 [D loss: 0.997713] [G loss: 1.012205]\n",
      "3496 [D loss: 1.003085] [G loss: 1.006511]\n",
      "3497 [D loss: 1.005642] [G loss: 1.004041]\n",
      "3498 [D loss: 1.003324] [G loss: 1.007745]\n",
      "3499 [D loss: 1.012415] [G loss: 0.987225]\n",
      "3500 [D loss: 0.991990] [G loss: 0.999521]\n",
      "3501 [D loss: 1.006600] [G loss: 0.997085]\n",
      "3502 [D loss: 0.998761] [G loss: 1.007939]\n",
      "3503 [D loss: 1.003054] [G loss: 1.005236]\n",
      "3504 [D loss: 1.006592] [G loss: 0.994456]\n",
      "3505 [D loss: 1.000651] [G loss: 0.998242]\n",
      "3506 [D loss: 1.010352] [G loss: 1.020653]\n",
      "3507 [D loss: 1.010119] [G loss: 1.000262]\n",
      "3508 [D loss: 1.006984] [G loss: 0.997656]\n",
      "3509 [D loss: 1.003601] [G loss: 0.986745]\n",
      "3510 [D loss: 1.002973] [G loss: 0.994468]\n",
      "3511 [D loss: 0.998847] [G loss: 0.996727]\n",
      "3512 [D loss: 1.010927] [G loss: 0.990061]\n",
      "3513 [D loss: 0.997182] [G loss: 0.995068]\n",
      "3514 [D loss: 1.014485] [G loss: 0.987095]\n",
      "3515 [D loss: 0.997510] [G loss: 0.996944]\n",
      "3516 [D loss: 1.009737] [G loss: 0.998487]\n",
      "3517 [D loss: 1.016125] [G loss: 0.981887]\n",
      "3518 [D loss: 1.006310] [G loss: 0.999021]\n",
      "3519 [D loss: 0.997529] [G loss: 1.003509]\n",
      "3520 [D loss: 1.001330] [G loss: 0.998094]\n",
      "3521 [D loss: 1.005521] [G loss: 0.999322]\n",
      "3522 [D loss: 1.009487] [G loss: 0.995019]\n",
      "3523 [D loss: 1.002605] [G loss: 0.994592]\n",
      "3524 [D loss: 1.006788] [G loss: 0.979588]\n",
      "3525 [D loss: 1.000269] [G loss: 0.991335]\n",
      "3526 [D loss: 1.005945] [G loss: 0.981824]\n",
      "3527 [D loss: 1.005850] [G loss: 0.987218]\n",
      "3528 [D loss: 0.998158] [G loss: 0.986567]\n",
      "3529 [D loss: 1.001810] [G loss: 0.991675]\n",
      "3530 [D loss: 1.011634] [G loss: 0.974983]\n",
      "3531 [D loss: 1.007372] [G loss: 0.983990]\n",
      "3532 [D loss: 1.005927] [G loss: 0.989647]\n",
      "3533 [D loss: 1.008714] [G loss: 0.988773]\n",
      "3534 [D loss: 1.007783] [G loss: 0.991921]\n",
      "3535 [D loss: 1.001510] [G loss: 0.983349]\n",
      "3536 [D loss: 1.003532] [G loss: 0.993498]\n",
      "3537 [D loss: 0.998171] [G loss: 0.973095]\n",
      "3538 [D loss: 0.999721] [G loss: 0.988194]\n",
      "3539 [D loss: 0.999396] [G loss: 0.979311]\n",
      "3540 [D loss: 1.005434] [G loss: 0.998674]\n",
      "3541 [D loss: 1.008195] [G loss: 0.995470]\n",
      "3542 [D loss: 1.006446] [G loss: 0.994225]\n",
      "3543 [D loss: 0.999632] [G loss: 0.994246]\n",
      "3544 [D loss: 1.001155] [G loss: 0.990639]\n",
      "3545 [D loss: 1.007367] [G loss: 0.991824]\n",
      "3546 [D loss: 1.006788] [G loss: 1.000889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547 [D loss: 1.002312] [G loss: 0.995781]\n",
      "3548 [D loss: 1.003864] [G loss: 0.984019]\n",
      "3549 [D loss: 1.017065] [G loss: 0.991751]\n",
      "3550 [D loss: 1.006613] [G loss: 0.976344]\n",
      "3551 [D loss: 1.000587] [G loss: 0.990770]\n",
      "3552 [D loss: 1.011870] [G loss: 0.990975]\n",
      "3553 [D loss: 1.007017] [G loss: 0.994779]\n",
      "3554 [D loss: 0.996035] [G loss: 0.980771]\n",
      "3555 [D loss: 1.013081] [G loss: 0.986236]\n",
      "3556 [D loss: 1.012264] [G loss: 0.968182]\n",
      "3557 [D loss: 1.000302] [G loss: 0.998013]\n",
      "3558 [D loss: 1.006342] [G loss: 0.977299]\n",
      "3559 [D loss: 1.013086] [G loss: 0.997602]\n",
      "3560 [D loss: 1.003014] [G loss: 0.995081]\n",
      "3561 [D loss: 0.994304] [G loss: 0.991298]\n",
      "3562 [D loss: 1.001919] [G loss: 0.995277]\n",
      "3563 [D loss: 1.001392] [G loss: 1.000338]\n",
      "3564 [D loss: 1.005503] [G loss: 0.986447]\n",
      "3565 [D loss: 1.012928] [G loss: 0.977523]\n",
      "3566 [D loss: 0.993091] [G loss: 1.000470]\n",
      "3567 [D loss: 1.006682] [G loss: 0.967712]\n",
      "3568 [D loss: 1.012220] [G loss: 0.990506]\n",
      "3569 [D loss: 1.010213] [G loss: 0.995729]\n",
      "3570 [D loss: 1.003741] [G loss: 0.975495]\n",
      "3571 [D loss: 1.014489] [G loss: 0.989533]\n",
      "3572 [D loss: 1.001616] [G loss: 0.979534]\n",
      "3573 [D loss: 1.009236] [G loss: 0.976167]\n",
      "3574 [D loss: 1.008510] [G loss: 0.988332]\n",
      "3575 [D loss: 0.996342] [G loss: 0.985888]\n",
      "3576 [D loss: 1.014615] [G loss: 0.977831]\n",
      "3577 [D loss: 1.002777] [G loss: 0.989869]\n",
      "3578 [D loss: 1.012125] [G loss: 0.993092]\n",
      "3579 [D loss: 1.002935] [G loss: 0.994874]\n",
      "3580 [D loss: 1.018924] [G loss: 0.996791]\n",
      "3581 [D loss: 1.009325] [G loss: 0.974013]\n",
      "3582 [D loss: 1.000296] [G loss: 0.989112]\n",
      "3583 [D loss: 1.002136] [G loss: 1.002519]\n",
      "3584 [D loss: 1.003906] [G loss: 0.985631]\n",
      "3585 [D loss: 1.009429] [G loss: 0.981142]\n",
      "3586 [D loss: 1.007052] [G loss: 0.976416]\n",
      "3587 [D loss: 1.006646] [G loss: 0.995646]\n",
      "3588 [D loss: 1.011827] [G loss: 0.980016]\n",
      "3589 [D loss: 1.010836] [G loss: 0.984972]\n",
      "3590 [D loss: 1.001527] [G loss: 1.003094]\n",
      "3591 [D loss: 1.019080] [G loss: 0.980192]\n",
      "3592 [D loss: 1.002049] [G loss: 0.986441]\n",
      "3593 [D loss: 1.001780] [G loss: 0.995631]\n",
      "3594 [D loss: 1.006661] [G loss: 0.986089]\n",
      "3595 [D loss: 1.018885] [G loss: 0.987920]\n",
      "3596 [D loss: 1.016125] [G loss: 1.008936]\n",
      "3597 [D loss: 1.002617] [G loss: 1.004163]\n",
      "3598 [D loss: 1.015045] [G loss: 0.994009]\n",
      "3599 [D loss: 1.007061] [G loss: 0.991066]\n",
      "3600 [D loss: 1.012655] [G loss: 0.980294]\n",
      "3601 [D loss: 1.008023] [G loss: 0.993988]\n",
      "3602 [D loss: 1.002272] [G loss: 0.996853]\n",
      "3603 [D loss: 1.005193] [G loss: 0.999507]\n",
      "3604 [D loss: 1.002748] [G loss: 0.985272]\n",
      "3605 [D loss: 1.019513] [G loss: 0.992667]\n",
      "3606 [D loss: 1.003920] [G loss: 1.006154]\n",
      "3607 [D loss: 1.009152] [G loss: 0.983204]\n",
      "3608 [D loss: 1.002314] [G loss: 0.983353]\n",
      "3609 [D loss: 1.007219] [G loss: 0.992654]\n",
      "3610 [D loss: 1.006308] [G loss: 1.002175]\n",
      "3611 [D loss: 1.005845] [G loss: 1.008243]\n",
      "3612 [D loss: 1.007594] [G loss: 1.005925]\n",
      "3613 [D loss: 0.996721] [G loss: 0.999487]\n",
      "3614 [D loss: 1.007089] [G loss: 0.978659]\n",
      "3615 [D loss: 1.022015] [G loss: 0.978810]\n",
      "3616 [D loss: 1.004064] [G loss: 1.009729]\n",
      "3617 [D loss: 1.000700] [G loss: 1.006787]\n",
      "3618 [D loss: 1.005299] [G loss: 1.000731]\n",
      "3619 [D loss: 0.987238] [G loss: 0.989524]\n",
      "3620 [D loss: 1.012377] [G loss: 1.006196]\n",
      "3621 [D loss: 1.014069] [G loss: 1.002028]\n",
      "3622 [D loss: 1.002392] [G loss: 0.989534]\n",
      "3623 [D loss: 1.001377] [G loss: 0.996055]\n",
      "3624 [D loss: 0.998725] [G loss: 0.989145]\n",
      "3625 [D loss: 1.009062] [G loss: 1.004682]\n",
      "3626 [D loss: 1.006048] [G loss: 1.000435]\n",
      "3627 [D loss: 1.013602] [G loss: 0.983169]\n",
      "3628 [D loss: 0.997760] [G loss: 0.998507]\n",
      "3629 [D loss: 0.999890] [G loss: 0.992513]\n",
      "3630 [D loss: 0.996361] [G loss: 0.991918]\n",
      "3631 [D loss: 0.995634] [G loss: 1.012030]\n",
      "3632 [D loss: 0.994736] [G loss: 0.994887]\n",
      "3633 [D loss: 1.008970] [G loss: 0.986464]\n",
      "3634 [D loss: 1.012349] [G loss: 0.976531]\n",
      "3635 [D loss: 1.007225] [G loss: 0.981862]\n",
      "3636 [D loss: 1.014778] [G loss: 0.984095]\n",
      "3637 [D loss: 1.008732] [G loss: 0.984495]\n",
      "3638 [D loss: 1.004984] [G loss: 0.978391]\n",
      "3639 [D loss: 1.003954] [G loss: 1.004376]\n",
      "3640 [D loss: 1.001785] [G loss: 0.986080]\n",
      "3641 [D loss: 1.000547] [G loss: 0.992969]\n",
      "3642 [D loss: 1.015779] [G loss: 0.982933]\n",
      "3643 [D loss: 1.013005] [G loss: 0.976896]\n",
      "3644 [D loss: 0.997343] [G loss: 0.983544]\n",
      "3645 [D loss: 1.009070] [G loss: 0.980196]\n",
      "3646 [D loss: 1.006258] [G loss: 0.970075]\n",
      "3647 [D loss: 1.015799] [G loss: 0.989579]\n",
      "3648 [D loss: 1.008634] [G loss: 0.988966]\n",
      "3649 [D loss: 1.003057] [G loss: 0.981234]\n",
      "3650 [D loss: 1.015272] [G loss: 0.977167]\n",
      "3651 [D loss: 1.001109] [G loss: 0.980861]\n",
      "3652 [D loss: 1.016082] [G loss: 0.972300]\n",
      "3653 [D loss: 1.003422] [G loss: 0.976066]\n",
      "3654 [D loss: 1.007140] [G loss: 0.981224]\n",
      "3655 [D loss: 1.007610] [G loss: 0.977465]\n",
      "3656 [D loss: 1.007743] [G loss: 0.994542]\n",
      "3657 [D loss: 1.003406] [G loss: 0.994903]\n",
      "3658 [D loss: 1.015040] [G loss: 0.985761]\n",
      "3659 [D loss: 0.996943] [G loss: 0.988654]\n",
      "3660 [D loss: 1.005616] [G loss: 0.993874]\n",
      "3661 [D loss: 1.004450] [G loss: 0.971447]\n",
      "3662 [D loss: 1.008047] [G loss: 0.973992]\n",
      "3663 [D loss: 1.006988] [G loss: 0.998963]\n",
      "3664 [D loss: 1.010080] [G loss: 0.967447]\n",
      "3665 [D loss: 1.006731] [G loss: 0.957016]\n",
      "3666 [D loss: 1.006362] [G loss: 0.993124]\n",
      "3667 [D loss: 1.002528] [G loss: 0.985192]\n",
      "3668 [D loss: 0.996837] [G loss: 0.980544]\n",
      "3669 [D loss: 0.995581] [G loss: 0.996348]\n",
      "3670 [D loss: 1.003583] [G loss: 0.981415]\n",
      "3671 [D loss: 1.000989] [G loss: 0.958663]\n",
      "3672 [D loss: 1.003478] [G loss: 0.984248]\n",
      "3673 [D loss: 1.014425] [G loss: 0.968489]\n",
      "3674 [D loss: 0.994728] [G loss: 0.991736]\n",
      "3675 [D loss: 0.993334] [G loss: 0.988314]\n",
      "3676 [D loss: 1.000449] [G loss: 0.977527]\n",
      "3677 [D loss: 1.001119] [G loss: 0.985773]\n",
      "3678 [D loss: 1.003296] [G loss: 0.975705]\n",
      "3679 [D loss: 1.006832] [G loss: 0.958838]\n",
      "3680 [D loss: 1.003380] [G loss: 0.979876]\n",
      "3681 [D loss: 0.997451] [G loss: 0.975388]\n",
      "3682 [D loss: 1.004724] [G loss: 0.986181]\n",
      "3683 [D loss: 0.992218] [G loss: 0.982379]\n",
      "3684 [D loss: 1.007160] [G loss: 0.980096]\n",
      "3685 [D loss: 1.002336] [G loss: 0.966690]\n",
      "3686 [D loss: 0.999590] [G loss: 1.000291]\n",
      "3687 [D loss: 1.008448] [G loss: 0.990572]\n",
      "3688 [D loss: 1.006407] [G loss: 0.994135]\n",
      "3689 [D loss: 1.001185] [G loss: 0.988830]\n",
      "3690 [D loss: 0.998677] [G loss: 0.990201]\n",
      "3691 [D loss: 0.993615] [G loss: 0.986933]\n",
      "3692 [D loss: 1.015499] [G loss: 0.985578]\n",
      "3693 [D loss: 0.990862] [G loss: 0.999592]\n",
      "3694 [D loss: 1.004989] [G loss: 0.982536]\n",
      "3695 [D loss: 1.009833] [G loss: 0.990789]\n",
      "3696 [D loss: 1.006291] [G loss: 1.005151]\n",
      "3697 [D loss: 1.019900] [G loss: 0.990399]\n",
      "3698 [D loss: 1.016664] [G loss: 0.992394]\n",
      "3699 [D loss: 1.009026] [G loss: 0.974738]\n",
      "3700 [D loss: 1.000563] [G loss: 1.003065]\n",
      "3701 [D loss: 1.008102] [G loss: 0.992428]\n",
      "3702 [D loss: 1.000795] [G loss: 0.971971]\n",
      "3703 [D loss: 1.005969] [G loss: 0.988583]\n",
      "3704 [D loss: 0.999603] [G loss: 0.996864]\n",
      "3705 [D loss: 1.007019] [G loss: 0.979061]\n",
      "3706 [D loss: 1.006630] [G loss: 0.991885]\n",
      "3707 [D loss: 1.004231] [G loss: 0.982399]\n",
      "3708 [D loss: 1.014023] [G loss: 0.995501]\n",
      "3709 [D loss: 0.996301] [G loss: 0.991681]\n",
      "3710 [D loss: 0.999696] [G loss: 1.008101]\n",
      "3711 [D loss: 1.004602] [G loss: 0.999889]\n",
      "3712 [D loss: 1.007233] [G loss: 0.995254]\n",
      "3713 [D loss: 1.013363] [G loss: 0.989076]\n",
      "3714 [D loss: 1.003029] [G loss: 0.993594]\n",
      "3715 [D loss: 1.006465] [G loss: 0.995802]\n",
      "3716 [D loss: 0.996756] [G loss: 0.991543]\n",
      "3717 [D loss: 0.998407] [G loss: 0.997632]\n",
      "3718 [D loss: 1.007468] [G loss: 0.994432]\n",
      "3719 [D loss: 0.997830] [G loss: 1.012297]\n",
      "3720 [D loss: 1.015138] [G loss: 0.964296]\n",
      "3721 [D loss: 1.003308] [G loss: 0.987609]\n",
      "3722 [D loss: 1.015054] [G loss: 0.977679]\n",
      "3723 [D loss: 1.013805] [G loss: 0.996584]\n",
      "3724 [D loss: 1.000680] [G loss: 0.995944]\n",
      "3725 [D loss: 1.010263] [G loss: 0.989308]\n",
      "3726 [D loss: 0.992999] [G loss: 0.988285]\n",
      "3727 [D loss: 1.003159] [G loss: 1.004811]\n",
      "3728 [D loss: 1.003658] [G loss: 0.987946]\n",
      "3729 [D loss: 1.005030] [G loss: 0.992553]\n",
      "3730 [D loss: 1.009219] [G loss: 0.989462]\n",
      "3731 [D loss: 1.000429] [G loss: 0.998047]\n",
      "3732 [D loss: 1.009917] [G loss: 0.996268]\n",
      "3733 [D loss: 0.996998] [G loss: 1.002114]\n",
      "3734 [D loss: 0.990183] [G loss: 0.997750]\n",
      "3735 [D loss: 1.022085] [G loss: 0.986828]\n",
      "3736 [D loss: 1.003776] [G loss: 1.002444]\n",
      "3737 [D loss: 1.008371] [G loss: 0.986965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3738 [D loss: 1.008362] [G loss: 0.986457]\n",
      "3739 [D loss: 0.998690] [G loss: 0.994555]\n",
      "3740 [D loss: 0.994306] [G loss: 0.993863]\n",
      "3741 [D loss: 1.013360] [G loss: 1.011479]\n",
      "3742 [D loss: 1.005444] [G loss: 0.998178]\n",
      "3743 [D loss: 1.008448] [G loss: 0.969545]\n",
      "3744 [D loss: 0.998504] [G loss: 0.989209]\n",
      "3745 [D loss: 1.007743] [G loss: 0.981592]\n",
      "3746 [D loss: 0.994943] [G loss: 1.003189]\n",
      "3747 [D loss: 0.991008] [G loss: 1.017210]\n",
      "3748 [D loss: 0.999226] [G loss: 0.994292]\n",
      "3749 [D loss: 1.009278] [G loss: 0.985037]\n",
      "3750 [D loss: 1.010111] [G loss: 0.982242]\n",
      "3751 [D loss: 0.999522] [G loss: 0.990928]\n",
      "3752 [D loss: 0.992948] [G loss: 0.992796]\n",
      "3753 [D loss: 1.006547] [G loss: 1.005248]\n",
      "3754 [D loss: 0.991203] [G loss: 0.986283]\n",
      "3755 [D loss: 0.996247] [G loss: 0.998580]\n",
      "3756 [D loss: 0.996098] [G loss: 0.982422]\n",
      "3757 [D loss: 1.005462] [G loss: 1.013242]\n",
      "3758 [D loss: 0.999616] [G loss: 0.976604]\n",
      "3759 [D loss: 1.004854] [G loss: 1.001478]\n",
      "3760 [D loss: 1.009683] [G loss: 1.002642]\n",
      "3761 [D loss: 1.002059] [G loss: 0.998071]\n",
      "3762 [D loss: 1.000792] [G loss: 0.998787]\n",
      "3763 [D loss: 1.011673] [G loss: 0.990148]\n",
      "3764 [D loss: 1.010067] [G loss: 0.988300]\n",
      "3765 [D loss: 1.011445] [G loss: 0.996683]\n",
      "3766 [D loss: 1.019939] [G loss: 0.995714]\n",
      "3767 [D loss: 1.008765] [G loss: 0.995496]\n",
      "3768 [D loss: 1.009354] [G loss: 0.984754]\n",
      "3769 [D loss: 1.012176] [G loss: 1.010270]\n",
      "3770 [D loss: 1.014543] [G loss: 0.985987]\n",
      "3771 [D loss: 1.005517] [G loss: 1.003546]\n",
      "3772 [D loss: 1.003581] [G loss: 0.997847]\n",
      "3773 [D loss: 1.015788] [G loss: 0.993689]\n",
      "3774 [D loss: 1.005158] [G loss: 1.002662]\n",
      "3775 [D loss: 0.999840] [G loss: 0.997628]\n",
      "3776 [D loss: 0.995191] [G loss: 1.002098]\n",
      "3777 [D loss: 1.014794] [G loss: 0.991975]\n",
      "3778 [D loss: 1.003499] [G loss: 0.992157]\n",
      "3779 [D loss: 1.005321] [G loss: 1.018449]\n",
      "3780 [D loss: 1.007968] [G loss: 0.985190]\n",
      "3781 [D loss: 1.008250] [G loss: 0.990481]\n",
      "3782 [D loss: 1.021084] [G loss: 0.995944]\n",
      "3783 [D loss: 1.001492] [G loss: 1.014453]\n",
      "3784 [D loss: 1.012034] [G loss: 0.994597]\n",
      "3785 [D loss: 1.004115] [G loss: 1.001087]\n",
      "3786 [D loss: 1.007657] [G loss: 1.009362]\n",
      "3787 [D loss: 1.012304] [G loss: 1.008872]\n",
      "3788 [D loss: 0.990518] [G loss: 1.023720]\n",
      "3789 [D loss: 1.008166] [G loss: 0.994513]\n",
      "3790 [D loss: 1.010768] [G loss: 0.991808]\n",
      "3791 [D loss: 1.009077] [G loss: 0.993804]\n",
      "3792 [D loss: 1.009925] [G loss: 0.993670]\n",
      "3793 [D loss: 1.006209] [G loss: 0.995874]\n",
      "3794 [D loss: 0.999253] [G loss: 0.999769]\n",
      "3795 [D loss: 1.015537] [G loss: 0.987554]\n",
      "3796 [D loss: 0.996659] [G loss: 1.001927]\n",
      "3797 [D loss: 1.013614] [G loss: 0.983141]\n",
      "3798 [D loss: 1.000776] [G loss: 1.000372]\n",
      "3799 [D loss: 1.004911] [G loss: 0.989521]\n",
      "3800 [D loss: 1.000855] [G loss: 1.011217]\n",
      "3801 [D loss: 1.003857] [G loss: 0.984755]\n",
      "3802 [D loss: 1.001428] [G loss: 0.999540]\n",
      "3803 [D loss: 1.004987] [G loss: 0.995532]\n",
      "3804 [D loss: 1.008345] [G loss: 1.010760]\n",
      "3805 [D loss: 1.004874] [G loss: 0.990860]\n",
      "3806 [D loss: 0.996238] [G loss: 0.987669]\n",
      "3807 [D loss: 1.014193] [G loss: 0.992963]\n",
      "3808 [D loss: 1.005284] [G loss: 0.990901]\n",
      "3809 [D loss: 1.003174] [G loss: 0.983120]\n",
      "3810 [D loss: 1.002112] [G loss: 1.005770]\n",
      "3811 [D loss: 1.012583] [G loss: 0.975926]\n",
      "3812 [D loss: 0.995444] [G loss: 0.995894]\n",
      "3813 [D loss: 0.997324] [G loss: 0.985281]\n",
      "3814 [D loss: 1.007147] [G loss: 0.987848]\n",
      "3815 [D loss: 1.014454] [G loss: 0.985563]\n",
      "3816 [D loss: 1.007386] [G loss: 0.983719]\n",
      "3817 [D loss: 1.017410] [G loss: 0.989158]\n",
      "3818 [D loss: 0.997275] [G loss: 0.986833]\n",
      "3819 [D loss: 0.996817] [G loss: 0.988869]\n",
      "3820 [D loss: 1.008292] [G loss: 0.984667]\n",
      "3821 [D loss: 1.011275] [G loss: 0.995198]\n",
      "3822 [D loss: 1.015390] [G loss: 0.987090]\n",
      "3823 [D loss: 0.998232] [G loss: 0.990592]\n",
      "3824 [D loss: 1.002021] [G loss: 0.993840]\n",
      "3825 [D loss: 1.017541] [G loss: 0.980673]\n",
      "3826 [D loss: 0.998879] [G loss: 0.990774]\n",
      "3827 [D loss: 1.001526] [G loss: 1.009724]\n",
      "3828 [D loss: 1.010738] [G loss: 0.978504]\n",
      "3829 [D loss: 0.997727] [G loss: 0.992945]\n",
      "3830 [D loss: 1.006760] [G loss: 0.986990]\n",
      "3831 [D loss: 1.004444] [G loss: 0.992363]\n",
      "3832 [D loss: 1.006550] [G loss: 0.983728]\n",
      "3833 [D loss: 1.001495] [G loss: 0.990994]\n",
      "3834 [D loss: 0.999561] [G loss: 0.985841]\n",
      "3835 [D loss: 1.009039] [G loss: 1.007501]\n",
      "3836 [D loss: 1.010244] [G loss: 0.991782]\n",
      "3837 [D loss: 1.007097] [G loss: 0.992844]\n",
      "3838 [D loss: 1.012058] [G loss: 0.977944]\n",
      "3839 [D loss: 1.005134] [G loss: 0.984133]\n",
      "3840 [D loss: 0.996748] [G loss: 1.006318]\n",
      "3841 [D loss: 1.010940] [G loss: 0.986291]\n",
      "3842 [D loss: 1.009610] [G loss: 0.995260]\n",
      "3843 [D loss: 1.001618] [G loss: 0.993562]\n",
      "3844 [D loss: 1.008699] [G loss: 0.986057]\n",
      "3845 [D loss: 1.004011] [G loss: 1.015712]\n",
      "3846 [D loss: 1.012950] [G loss: 1.006978]\n",
      "3847 [D loss: 1.011792] [G loss: 1.008622]\n",
      "3848 [D loss: 1.000265] [G loss: 1.011045]\n",
      "3849 [D loss: 1.009603] [G loss: 1.001953]\n",
      "3850 [D loss: 1.007918] [G loss: 0.997935]\n",
      "3851 [D loss: 0.996328] [G loss: 0.997938]\n",
      "3852 [D loss: 1.009993] [G loss: 0.982066]\n",
      "3853 [D loss: 0.998777] [G loss: 1.001114]\n",
      "3854 [D loss: 0.996929] [G loss: 0.988093]\n",
      "3855 [D loss: 1.003202] [G loss: 0.988964]\n",
      "3856 [D loss: 1.000169] [G loss: 1.003458]\n",
      "3857 [D loss: 0.999651] [G loss: 0.999591]\n",
      "3858 [D loss: 1.002371] [G loss: 1.009310]\n",
      "3859 [D loss: 1.004418] [G loss: 1.015363]\n",
      "3860 [D loss: 1.010117] [G loss: 1.001812]\n",
      "3861 [D loss: 1.002714] [G loss: 1.001513]\n",
      "3862 [D loss: 1.014236] [G loss: 0.997386]\n",
      "3863 [D loss: 1.004042] [G loss: 0.994341]\n",
      "3864 [D loss: 1.005960] [G loss: 0.988387]\n",
      "3865 [D loss: 1.003501] [G loss: 0.999062]\n",
      "3866 [D loss: 1.015894] [G loss: 0.978496]\n",
      "3867 [D loss: 0.995870] [G loss: 1.006602]\n",
      "3868 [D loss: 1.007105] [G loss: 0.994853]\n",
      "3869 [D loss: 0.991801] [G loss: 0.991338]\n",
      "3870 [D loss: 1.001315] [G loss: 0.988037]\n",
      "3871 [D loss: 1.010493] [G loss: 0.985508]\n",
      "3872 [D loss: 1.015310] [G loss: 1.007226]\n",
      "3873 [D loss: 1.014508] [G loss: 0.991112]\n",
      "3874 [D loss: 1.006968] [G loss: 0.979019]\n",
      "3875 [D loss: 0.996356] [G loss: 0.991310]\n",
      "3876 [D loss: 1.016776] [G loss: 0.982870]\n",
      "3877 [D loss: 1.005158] [G loss: 0.997253]\n",
      "3878 [D loss: 1.004064] [G loss: 0.997527]\n",
      "3879 [D loss: 1.007396] [G loss: 0.979254]\n",
      "3880 [D loss: 1.012065] [G loss: 0.982224]\n",
      "3881 [D loss: 0.993562] [G loss: 0.984643]\n",
      "3882 [D loss: 1.018846] [G loss: 0.991291]\n",
      "3883 [D loss: 1.015593] [G loss: 0.998917]\n",
      "3884 [D loss: 1.007556] [G loss: 0.968782]\n",
      "3885 [D loss: 1.007109] [G loss: 0.992491]\n",
      "3886 [D loss: 1.002219] [G loss: 0.988840]\n",
      "3887 [D loss: 1.011196] [G loss: 0.993442]\n",
      "3888 [D loss: 0.998650] [G loss: 0.992975]\n",
      "3889 [D loss: 0.995492] [G loss: 1.000770]\n",
      "3890 [D loss: 1.004553] [G loss: 0.998064]\n",
      "3891 [D loss: 0.992596] [G loss: 0.995928]\n",
      "3892 [D loss: 0.992894] [G loss: 1.012455]\n",
      "3893 [D loss: 1.008713] [G loss: 0.984186]\n",
      "3894 [D loss: 1.004177] [G loss: 0.990288]\n",
      "3895 [D loss: 1.004222] [G loss: 0.981611]\n",
      "3896 [D loss: 0.994383] [G loss: 1.003535]\n",
      "3897 [D loss: 1.011565] [G loss: 0.984906]\n",
      "3898 [D loss: 1.007867] [G loss: 0.991440]\n",
      "3899 [D loss: 0.998278] [G loss: 1.004621]\n",
      "3900 [D loss: 0.989347] [G loss: 0.991157]\n",
      "3901 [D loss: 1.011257] [G loss: 0.980979]\n",
      "3902 [D loss: 1.008288] [G loss: 0.995869]\n",
      "3903 [D loss: 1.010133] [G loss: 0.983640]\n",
      "3904 [D loss: 1.006138] [G loss: 1.001503]\n",
      "3905 [D loss: 1.002010] [G loss: 0.992220]\n",
      "3906 [D loss: 1.003313] [G loss: 0.994429]\n",
      "3907 [D loss: 1.002132] [G loss: 0.998512]\n",
      "3908 [D loss: 1.007959] [G loss: 0.982915]\n",
      "3909 [D loss: 0.993844] [G loss: 0.997142]\n",
      "3910 [D loss: 1.001383] [G loss: 0.983821]\n",
      "3911 [D loss: 0.989852] [G loss: 1.020278]\n",
      "3912 [D loss: 1.002020] [G loss: 0.989514]\n",
      "3913 [D loss: 1.016345] [G loss: 0.979928]\n",
      "3914 [D loss: 1.014109] [G loss: 0.985438]\n",
      "3915 [D loss: 1.001501] [G loss: 0.979178]\n",
      "3916 [D loss: 1.005612] [G loss: 0.999489]\n",
      "3917 [D loss: 1.011688] [G loss: 0.982636]\n",
      "3918 [D loss: 1.004483] [G loss: 0.983734]\n",
      "3919 [D loss: 0.999966] [G loss: 0.986760]\n",
      "3920 [D loss: 1.016253] [G loss: 0.976419]\n",
      "3921 [D loss: 1.007117] [G loss: 0.990739]\n",
      "3922 [D loss: 1.003924] [G loss: 0.987474]\n",
      "3923 [D loss: 1.001949] [G loss: 0.985236]\n",
      "3924 [D loss: 1.002626] [G loss: 1.001537]\n",
      "3925 [D loss: 1.013736] [G loss: 0.990073]\n",
      "3926 [D loss: 0.987011] [G loss: 0.997735]\n",
      "3927 [D loss: 0.998529] [G loss: 0.993356]\n",
      "3928 [D loss: 1.011087] [G loss: 0.980043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3929 [D loss: 0.992533] [G loss: 0.994016]\n",
      "3930 [D loss: 1.006131] [G loss: 0.999877]\n",
      "3931 [D loss: 1.002193] [G loss: 0.995838]\n",
      "3932 [D loss: 1.010473] [G loss: 0.993145]\n",
      "3933 [D loss: 0.998043] [G loss: 0.994425]\n",
      "3934 [D loss: 1.012852] [G loss: 0.991471]\n",
      "3935 [D loss: 0.998516] [G loss: 0.992780]\n",
      "3936 [D loss: 0.997369] [G loss: 0.998520]\n",
      "3937 [D loss: 1.011492] [G loss: 0.988490]\n",
      "3938 [D loss: 1.007536] [G loss: 0.985897]\n",
      "3939 [D loss: 1.021298] [G loss: 0.995089]\n",
      "3940 [D loss: 1.003610] [G loss: 1.003313]\n",
      "3941 [D loss: 0.994540] [G loss: 0.990026]\n",
      "3942 [D loss: 1.000548] [G loss: 0.981027]\n",
      "3943 [D loss: 1.002067] [G loss: 0.997380]\n",
      "3944 [D loss: 1.018454] [G loss: 0.990466]\n",
      "3945 [D loss: 0.990400] [G loss: 0.992322]\n",
      "3946 [D loss: 1.003822] [G loss: 0.972166]\n",
      "3947 [D loss: 1.012974] [G loss: 0.981604]\n",
      "3948 [D loss: 1.010978] [G loss: 0.993247]\n",
      "3949 [D loss: 0.999720] [G loss: 0.981008]\n",
      "3950 [D loss: 1.003758] [G loss: 1.004651]\n",
      "3951 [D loss: 1.000872] [G loss: 0.993378]\n",
      "3952 [D loss: 1.000070] [G loss: 1.000029]\n",
      "3953 [D loss: 1.014113] [G loss: 0.992804]\n",
      "3954 [D loss: 1.004747] [G loss: 0.992787]\n",
      "3955 [D loss: 1.015529] [G loss: 0.987587]\n",
      "3956 [D loss: 1.014646] [G loss: 0.985201]\n",
      "3957 [D loss: 1.012328] [G loss: 0.983713]\n",
      "3958 [D loss: 1.026120] [G loss: 0.996914]\n",
      "3959 [D loss: 1.014345] [G loss: 0.983943]\n",
      "3960 [D loss: 1.009778] [G loss: 0.994306]\n",
      "3961 [D loss: 0.992344] [G loss: 0.990263]\n",
      "3962 [D loss: 1.006087] [G loss: 0.991577]\n",
      "3963 [D loss: 1.006484] [G loss: 1.001071]\n",
      "3964 [D loss: 1.009781] [G loss: 0.977595]\n",
      "3965 [D loss: 1.002005] [G loss: 1.000361]\n",
      "3966 [D loss: 1.009089] [G loss: 0.997895]\n",
      "3967 [D loss: 0.998135] [G loss: 0.981482]\n",
      "3968 [D loss: 1.001641] [G loss: 0.990916]\n",
      "3969 [D loss: 1.018068] [G loss: 0.989922]\n",
      "3970 [D loss: 1.002929] [G loss: 0.988738]\n",
      "3971 [D loss: 0.992715] [G loss: 1.001216]\n",
      "3972 [D loss: 0.994350] [G loss: 0.985083]\n",
      "3973 [D loss: 1.001648] [G loss: 0.994908]\n",
      "3974 [D loss: 0.996469] [G loss: 0.990472]\n",
      "3975 [D loss: 1.009716] [G loss: 1.002536]\n",
      "3976 [D loss: 1.003084] [G loss: 1.000270]\n",
      "3977 [D loss: 1.008848] [G loss: 1.002348]\n",
      "3978 [D loss: 1.012138] [G loss: 0.983075]\n",
      "3979 [D loss: 0.995416] [G loss: 0.985196]\n",
      "3980 [D loss: 1.003525] [G loss: 0.997625]\n",
      "3981 [D loss: 1.005682] [G loss: 0.983230]\n",
      "3982 [D loss: 1.005095] [G loss: 0.983989]\n",
      "3983 [D loss: 0.995468] [G loss: 0.992741]\n",
      "3984 [D loss: 0.998627] [G loss: 0.995008]\n",
      "3985 [D loss: 0.993680] [G loss: 0.995784]\n",
      "3986 [D loss: 1.017460] [G loss: 0.972449]\n",
      "3987 [D loss: 1.007135] [G loss: 0.974552]\n",
      "3988 [D loss: 0.998844] [G loss: 0.975173]\n",
      "3989 [D loss: 1.017363] [G loss: 0.975346]\n",
      "3990 [D loss: 1.012739] [G loss: 0.999180]\n",
      "3991 [D loss: 1.004192] [G loss: 0.979036]\n",
      "3992 [D loss: 0.999198] [G loss: 0.984563]\n",
      "3993 [D loss: 1.005267] [G loss: 1.011708]\n",
      "3994 [D loss: 0.994322] [G loss: 0.981717]\n",
      "3995 [D loss: 1.001565] [G loss: 0.992785]\n",
      "3996 [D loss: 1.004158] [G loss: 1.007288]\n",
      "3997 [D loss: 1.001457] [G loss: 0.989025]\n",
      "3998 [D loss: 1.009833] [G loss: 1.008236]\n",
      "3999 [D loss: 0.993550] [G loss: 1.003779]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

