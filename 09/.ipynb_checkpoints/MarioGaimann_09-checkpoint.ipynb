{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import numpy.random as rd\n",
    "\n",
    "from sklearn.datasets import fetch_openml # MNIST data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Cropping2D\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1 - Clustering\n",
    "\n",
    "### A1 - a - Performance of different linkages on samples\n",
    "\n",
    "Linkage information:\n",
    "\n",
    " - ward minimizes the variance of the clusters being merged.\n",
    " - average uses the average of the distances of each observation of the two sets.\n",
    " - complete or maximum linkage uses the maximum distances between all observations of the two sets.\n",
    " - single uses the minimum of the distances between all observations of the two sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 - AutoEncoder\n",
    "\n",
    "### A2 - a - Generate 2D images showing polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' WRONG INTERPRETATION OF THE TASK\n",
    "    CORRECT INTERPRETATION BELOW\n",
    "\n",
    "def generate_poly(degree, x, polycoeff_range):\n",
    "    polycoeffs = rd.uniform(coeff_range[0], coeff_range[1], degree+1)\n",
    "    \n",
    "    y = np.zeros(x.size)\n",
    "    \n",
    "    degree = 0\n",
    "    for p in polycoeffs:\n",
    "        y += p * (x**degree)\n",
    "        degree += 1\n",
    "    \n",
    "    return y\n",
    "\n",
    "# define initial parameters\n",
    "coeff_range=[-1,1]\n",
    "x = np.linspace(-20,20, 41)\n",
    "\n",
    "\n",
    "# generate polynomials and corresponding pixel images\n",
    "for degree in range(3):\n",
    "    y = generate_poly(degree, x, coeff_range)\n",
    "\n",
    "    # plot image\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.xlim([x[0], x[-1]])\n",
    "    plt.ylim([x[0], x[-1]])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    plt.savefig(\"fig.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # pixelate image\n",
    "    basewidth = 40\n",
    "    img=Image.open(\"fig.png\")\n",
    "    wpercent = (basewidth/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    \n",
    "    ax.axis('off')  \n",
    "    img.save(\"fig.png\")\n",
    "    img=mpimg.imread(\"fig.png\")\n",
    "    ax.axis('off')\n",
    "    imgplot=plt.imshow(img)\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "def generate_pixelated_image(xdata, ydata):\n",
    "    # generate pixel image\n",
    "    length = xdata.size\n",
    "    image = np.zeros(shape=(length, length))\n",
    "\n",
    "    # treat every column\n",
    "    for col in range(length):\n",
    "        # check whether yval belongs to a certain pixel\n",
    "        for row in range(length):\n",
    "            \n",
    "            # access yval for that column\n",
    "            yval = ydata[row]\n",
    "            \n",
    "            # check which pixel it corresponds to\n",
    "            if yval >= xdata[row] and yval < xdata[row]+1:\n",
    "\n",
    "                image[col][row] = 1\n",
    "                \n",
    "    return image  \n",
    "'''    \n",
    "    \n",
    "# define points\n",
    "x=np.linspace(-1,1,40)\n",
    "xx,yy=np.meshgrid(x,x)\n",
    "\n",
    "       \n",
    "# define polynomial generation\n",
    "def generate_poly(deg, xx=xx, yy=yy):\n",
    "    x_coeffs=np.random.uniform(-5.,5., deg+1)\n",
    "    y_coeffs=np.random.uniform(-5.,5., deg+1)\n",
    "    \n",
    "    img=np.zeros((40,40))\n",
    "    \n",
    "    for d in range(deg+1):\n",
    "        img+=x_coeffs[d]*xx**d + y_coeffs[d]*yy**d \n",
    "        \n",
    "    return img\n",
    "\n",
    "\n",
    "# sample images\n",
    "samples=[]\n",
    "n_samples = 5000\n",
    "\n",
    "for i in range(n_samples):\n",
    "    deg=int(np.random.uniform(1,2))\n",
    "    samples.append(generate_poly(deg))\n",
    "samples=np.array(samples)\n",
    "\n",
    "plt.imshow(samples[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2 - b - Build two autoencoder architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build autoencoders\n",
    "\n",
    "# input parameter\n",
    "input_size = 1600\n",
    "hidden_size1 = 400\n",
    "hidden_size2 = 200\n",
    "hidden_size3 = 40\n",
    "code_size = 10\n",
    "\n",
    "# single hidden dense layer\n",
    "input_img_s = Input(shape=(input_size,))\n",
    "code_s = Dense(code_size, activation='relu')(input_img_s)\n",
    "output_img_s = Dense(input_size, activation='sigmoid')(code_s)\n",
    "autenc_small = Model(input_img_s, output_img_s)\n",
    "\n",
    "# several hidden dense layers\n",
    "input_img_l = Input(shape=(input_size,))\n",
    "hidden_1 = Dense(hidden_size1, activation='relu')(input_img_l)\n",
    "hidden_2 = Dense(hidden_size2, activation='relu')(hidden_1)\n",
    "hidden_3 = Dense(hidden_size3, activation='relu')(hidden_2)\n",
    "code_l = Dense(code_size, activation='relu')(hidden_3)\n",
    "hidden_3r = Dense(hidden_size3, activation='relu')(code_l)\n",
    "hidden_2r = Dense(hidden_size2, activation='relu')(hidden_3r)\n",
    "hidden_1r = Dense(hidden_size1, activation='relu')(hidden_2r)\n",
    "output_img_l = Dense(input_size, activation='sigmoid')(hidden_1r)\n",
    "autenc_large = Model(input_img_l, output_img_l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 10)                16010     \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 1600)              17600     \n",
      "=================================================================\n",
      "Total params: 33,610\n",
      "Trainable params: 33,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 3s 515us/step - loss: -7.7700\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 1s 148us/step - loss: -36.8380\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 1s 163us/step - loss: -44.8854\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: -45.4822\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 1s 146us/step - loss: -45.5852\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 1s 134us/step - loss: -45.7048\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: -45.8987\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: -46.2472\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: -46.7554\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 1s 151us/step - loss: -47.3246\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 1s 134us/step - loss: -47.8866\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: -48.5103\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: -49.3416\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 1s 132us/step - loss: -50.3242\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: -51.2115\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: -51.9135\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 1s 134us/step - loss: -52.4218\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 1s 147us/step - loss: -52.8115\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 1s 150us/step - loss: -53.1100\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 1s 153us/step - loss: -53.3242\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 400)               640400    \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 40)                8040      \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 40)                440       \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 200)               8200      \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 400)               80400     \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 1600)              641600    \n",
      "=================================================================\n",
      "Total params: 1,459,690\n",
      "Trainable params: 1,459,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 4s 838us/step - loss: -19.8434\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 1s 280us/step - loss: -26.4547\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: -40.0449\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 1s 291us/step - loss: -45.6102\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: -46.0707\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 1s 278us/step - loss: -47.1194\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: -47.4997\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 1s 298us/step - loss: -47.7008\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: -47.8475\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: -48.1521\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: -50.7874\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 1s 292us/step - loss: -53.1853\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 1s 287us/step - loss: -53.7196\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 2s 332us/step - loss: -53.7886\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 2s 316us/step - loss: -53.8133\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 2s 307us/step - loss: -53.8314\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 1s 277us/step - loss: -53.8508\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 1s 278us/step - loss: -53.8700\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 1s 281us/step - loss: -53.8795\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 1s 278us/step - loss: -53.8882\n"
     ]
    }
   ],
   "source": [
    "samples=samples.reshape((len(samples), np.prod(samples.shape[1:])))\n",
    "\n",
    "# compile and fit\n",
    "autencs = [autenc_small, autenc_large]\n",
    "for autenc in autencs:\n",
    "    autenc.summary()\n",
    "    autenc.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    autenc.fit(samples, samples, epochs=20, batch_size=256, shuffle=True)\n",
    "    # output and input should be same!\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: surprisingly, the loss is for both architectures about the same. This holds for various sizes of the coding layer.\n",
    "\n",
    "\n",
    "### A2 - c - Visualize the results of latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-3d65947e3ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Specify the layer to want to visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mlayer_to_visualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-3d65947e3ca6>\u001b[0m in \u001b[0;36mlayer_to_visualize\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_convout1_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvout1_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# The [0] is to disable the training phase flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "def layer_to_visualize(layer):\n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "\n",
    "    _convout1_f = K.function(inputs, [layer.output])\n",
    "    def convout1_f(X):\n",
    "        # The [0] is to disable the training phase flag\n",
    "        return _convout1_f([0] + [X])\n",
    "\n",
    "    convolutions = convout1_f(img_to_visualize)\n",
    "    convolutions = np.squeeze(convolutions)\n",
    "\n",
    "    print ('Shape of conv:', convolutions.shape)\n",
    "\n",
    "    n = convolutions.shape[0]\n",
    "    n = int(np.ceil(np.sqrt(n)))\n",
    "\n",
    "    # Visualization of each filter of the layer\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    for i in range(len(convolutions)):\n",
    "        ax = fig.add_subplot(n,n,i+1)\n",
    "        ax.imshow(convolutions[i], cmap='gray')\n",
    "\n",
    "# Specify the layer to want to visualize\n",
    "layer_to_visualize(code_s)\n",
    "layer_to_visualize(code_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3 - KL - divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X = \\{x_1, x_2\\}$. \n",
    "Let $P(x_1) = 0.2$ and $P(x_2) = 0.8$ as well as \n",
    "$Q(x_1) = 0.5$ and $Q(x_2) = 0.5$. Then,\n",
    "\n",
    "$D_{KL}(P||Q) = \\sum_{x\\in X} P(x) \\log(\\frac{P(x)}{Q(x)}) = 0.2 \\cdot \\log(0.2/0.5) + 0.8 \\cdot \\log(0.8/0.5) \\approx 0.193 $ and\n",
    "\n",
    "$D_{KL}(Q||P) = \\sum_{x\\in X} Q(x) \\log(\\frac{Q(x)}{P(x)}) = 0.5 \\cdot \\log(0.5/0.2) + 0.5 \\cdot \\log(0.5/0.8) \\approx 0.223 $.\n",
    "\n",
    "Hence $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$ in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
